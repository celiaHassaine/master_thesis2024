{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9106346",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import chardet\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "import mysql.connector\n",
    "from mysql.connector import FieldType\n",
    "import numpy as np\n",
    "import unidecode\n",
    "from mysql.connector.cursor import MySQLCursor\n",
    "import sys\n",
    "from IPython.display import clear_output\n",
    "import re\n",
    "\n",
    "conn_datassistant = {\n",
    "    'host': \"localhost\",\n",
    "    'user': \"root\",\n",
    "    'password': \"CX29Mhky\",\n",
    "    'database': \"databaseassistant\"\n",
    "}\n",
    "\n",
    "conn_new_db = {\n",
    "    'host': \"localhost\",\n",
    "    'user': \"root\",\n",
    "    'password': \"CX29Mhky\",\n",
    "    'database': \"new_db\"\n",
    "}\n",
    "\n",
    "\n",
    "engine = create_engine(\"mysql+mysqlconnector://root:CX29Mhky@localhost/new_db\")\n",
    "\n",
    "path_irm = \"./data/zirm_pc/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb64923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# structure col2check:\n",
    "#       col2check = {table_name:{col_in_excel: col_in_db, col_in_excel: col_in_db ...\n",
    "\n",
    "def substring_ucl(col, ucl2=True):\n",
    "    print(ucl2)\n",
    "    if ucl2:\n",
    "        print(ucl2)\n",
    "        display(col.map(lambda x: x.split('UCL2_')[-1].split(\"CUL2_\")[-1].split(\"ULC2_\")[-1].split(\"*\")[0] if not pd.isna(x) else x))\n",
    "        return col.map(lambda x: x.split('UCL2_')[-1].split(\"CUL2_\")[-1].split(\"ULC2_\")[-1].split(\"*\")[0] if not pd.isna(x) else x)\n",
    "    else:\n",
    "        return col.map(lambda x: str(x).split('-')[-1] if not pd.isna(x) else x)\n",
    "    \n",
    "def make_upper(col):\n",
    "    return col.str.upper()\n",
    "\n",
    "def sexe(col, diag=[]):\n",
    "    c = col.copy()\n",
    "    if len(diag) > 0:\n",
    "        c.replace(diag[0], diag[1], inplace=True)\n",
    "    else:\n",
    "        c.replace(\"H\", \"M\", inplace=True)\n",
    "    return c\n",
    "\n",
    "def adjust_dates(group):\n",
    "    if len(group) == 1:\n",
    "        group[\"adjusted_date\"] = group[\"date_pet_tau_info\"]\n",
    "        return group\n",
    "    else:\n",
    "        sorted_group = group.sort_values('date_pet_tau_info')\n",
    "        sorted_group['adjusted_date'] = sorted_group['date_pet_tau_info'].min()\n",
    "        sorted_group.loc[sorted_group['timepoint'] == 2, 'adjusted_date'] = sorted_group['date_pet_tau_info'].max()\n",
    "        return sorted_group\n",
    "    \n",
    "def make_lower(col):\n",
    "    return col.str.lower()\n",
    "\n",
    "def change_type(col, types):\n",
    "    return col.astype(types)\n",
    "\n",
    "def makeapoeint(col):\n",
    "    c = col.copy()\n",
    "    c.replace('[a-zA-Z]', \"\", inplace=True, regex=True)\n",
    "    display(c)\n",
    "    return c\n",
    "\n",
    "def convert_coma(col):\n",
    "    c = col.copy()\n",
    "    c.replace('[a-zA-Z]', np.nan, inplace=True, regex=True)\n",
    "    c.replace('',np.nan,regex = True, inplace=True)\n",
    "    c.replace(\",\", \".\", regex=True, inplace=True)\n",
    "    return c\n",
    "\n",
    "def get_comments(col, df, basic_col):\n",
    "    return df[basic_col].copy().map(lambda x: '_'.join(str(x).split('_')[1:]) if len(str(x).split('_')) >1 else None)\n",
    "                                      \n",
    "def make_binary(col):\n",
    "    x = col.map(lambda x: str(x).split('_')[0]).copy()\n",
    "    x.replace('0', False, inplace=True)\n",
    "    x.replace('1', True, inplace=True)\n",
    "    return x\n",
    "                                      \n",
    "    \n",
    "def avoid_accent(col):\n",
    "    for i in list(col):\n",
    "        try:\n",
    "            if not pd.isna(i):\n",
    "                if i != unidecode.unidecode(i):\n",
    "                    print(i)\n",
    "                    print(unidecode.unidecode(i))\n",
    "        except:\n",
    "            print(i)\n",
    "            print(pd.isna(i))\n",
    "            sys.exit()\n",
    "    return col.map(lambda x: unidecode.unidecode(x) if not pd.isna(x) else x)\n",
    "\n",
    "def update_table(table, filename, df_db, df, col2check, col2id, file_update):\n",
    "    display(df)\n",
    "    df2add = pd.DataFrame(columns=df_db.columns)\n",
    "    up = \"\"\n",
    "    for index, row in df.iterrows():\n",
    "        r = []\n",
    "        for ids in col2id:\n",
    "            condition = \"\\nWHERE \"\n",
    "            for ind, tup in enumerate(ids):\n",
    "                if ind == 0:\n",
    "                    r = df_db.loc[df_db[tup[1]] == row[tup[0]]]\n",
    "                    condition += tup[1]+\" = \\\"\"+str(row[tup[0]])+\"\\\"\"\n",
    "                else:\n",
    "                    r = r.loc[r[tup[1]] == row[tup[0]]]\n",
    "                    condition += \" and \"+tup[1] +\" = \\\"\"+str(row[tup[0]])+\"\\\"\"\n",
    "\n",
    "            condition += \";\\n\\n\"\n",
    "            if len(r) == 1:\n",
    "                break\n",
    "            if len(r) > 1:\n",
    "                print(\"error\")\n",
    "                sys.exit()\n",
    "        if len(r) == 1:\n",
    "            #display(r)\n",
    "            up2add = \"UPDATE patient \\nSET \"#address = %s WHERE address = %s\"\n",
    "            avant = \"\\n# BEFORE: \"\n",
    "            col2change = \"\"\n",
    "            for h in col2check:\n",
    "                indb = r[col2check[h]][r.index[0]]\n",
    "                inexcel = row[h]\n",
    "                if pd.isna(indb):\n",
    "                    if not pd.isna(inexcel):\n",
    "                        col2change += str(col2check[h])+\" = '\"+str(inexcel)+\"', \"\n",
    "                        avant += str(col2check[h])+\" = '\"+str(indb)+\"', \"\n",
    "                elif not pd.isna(inexcel):\n",
    "                    if indb != inexcel:\n",
    "                        col2change += str(col2check[h])+\" = '\"+str(inexcel)+\"', \"\n",
    "                        avant += str(col2check[h])+\" = '\"+str(indb)+\"', \"\n",
    "            if len(col2change) > 0:       \n",
    "                col2change = col2change[:-2]\n",
    "                up2add += col2change + avant + condition\n",
    "                up += up2add\n",
    "        elif len(r)==0 and not row[list(col2check.keys())].isnull().values.all():\n",
    "            d = {}\n",
    "            for h in col2check:\n",
    "                d[col2check[h]] = [row[h]]\n",
    "                \n",
    "            df2add = pd.concat([df2add, pd.DataFrame(d)])\n",
    "    if len(up) > 0:\n",
    "        with open(\"sql_code/update_\"+file_update+\"_from_\"+filename+\".txt\", 'w') as f:\n",
    "            f.write(up)\n",
    "    if len(df2add) > 0:\n",
    "        display(df2add)\n",
    "        if input(\"ajouter les valeur manquantes à la db?\") == \"y\":\n",
    "            df2add.drop_duplicates().to_sql(name=\"patient\", con=engine, if_exists=\"append\", index=False)\n",
    "            print(\"les données ont bien été ajoutées!\")\n",
    "\n",
    "def add_info_from_excel(table, col2id, col2check, col_with_date, col2modify_db, col2modify_excel, path_to_excel, file_update=None, skip_rows=None):\n",
    "    \"\"\"\n",
    "    function to automatically add excels by checking if the patient exist, if it needs update on its information and if it needs to be added, \n",
    "    all that on the right tables\n",
    "\n",
    "    Arguments:\n",
    "    col2id  --  \n",
    "    col2check  --\n",
    "    col_with_date  -- \n",
    "    path_to_excel  --  \n",
    "\n",
    "    return -- nothing\n",
    "    \"\"\"\n",
    "    if file_update is None:\n",
    "        file_update = table\n",
    "    sql = \"select * from \"+table\n",
    "    df_db = pd.read_sql(sql, con=mysql.connector.connect(**conn_new_db), coerce_float=True)  \n",
    "    df_excel = pd.read_csv(path_to_excel, delimiter=\";\", skiprows=skip_rows) \n",
    "    for c in df_excel.columns:\n",
    "        if pd.api.types.is_string_dtype(df_excel[c].dtype):\n",
    "            df_excel[c] = df_excel[c].str.strip()\n",
    "    \n",
    "    # step 1: modify column types\n",
    "    for i in col2modify_db:\n",
    "        if isinstance(i[1], str):\n",
    "            df_db[i[0]] = i[1]\n",
    "        elif len(i) == 2:\n",
    "            df_db[i[0]] = i[1](df_db[i[0]])\n",
    "        elif len(i) == 3:\n",
    "            df_db[i[0]] = i[1](df_db[i[0]], i[2])\n",
    "\n",
    "    \n",
    "    for i in col_with_date:\n",
    "        df_excel[i] = pd.to_datetime(df_excel[i], format=\"%d/%m/%Y\").dt.date\n",
    "        \"\"\"\n",
    "        for r in range(len(df_excel)):\n",
    "            print(\"avant\")\n",
    "            print(df_excel.loc[r, i])\n",
    "            df_excel.loc[r, i] = pd.to_datetime(df_excel.loc[r, i], format=\"%d/%m/%Y\").date\"\"\"\n",
    "\n",
    "    \n",
    "    for i in col2modify_excel:\n",
    "        if i[1] == get_comments:\n",
    "            df_excel[i[0]] = get_comments(i[0], df_excel, i[2])\n",
    "        elif isinstance(i[1], str):\n",
    "            df_excel[i[0]] = i[1]\n",
    "        elif len(i) == 2:\n",
    "            df_excel[i[0]] = i[1](df_excel[i[0]])\n",
    "        elif len(i) == 3:\n",
    "            df_excel[i[0]] = i[1](df_excel[i[0]], i[2])\n",
    "\n",
    "    if table == \"patient\":\n",
    "        update_table(table, (path_to_excel.split(\"\\\\\")[-1]).split(\".\")[0], df_db, df_excel, col2check, col2id, file_update)\n",
    "    else:\n",
    "        col2id = col2id[0]\n",
    "        df_pat = pd.read_sql(\"select * from patient\", con=mysql.connector.connect(**conn_new_db), coerce_float=False) \n",
    "        ids = None\n",
    "        \n",
    "        # 1. we take the id column assuming it is FIRST in the colid\n",
    "        todrop = [col2id[0][1], col2id[1][0]]\n",
    "        df_excel.rename(columns={col2id[0][0]: col2id[0][1]}, inplace=True)\n",
    "        ids = col2id[0][1]\n",
    "        df_excel.dropna(subset=todrop, how='any', inplace=True)\n",
    "        \n",
    "        # 2. we modify the column of the db to match type\n",
    "        df_pat[ids] = df_pat[ids].astype(df_excel.dtypes.to_dict()[ids])\n",
    "        \n",
    "        # 3. on merge la db de excel avec celle de patient pour avoir id_patient\n",
    "        df_excel = pd.merge(df_excel, df_pat[[ids, \"id_patient\"]].copy(), on=ids, how=\"left\")\n",
    "        del col2check[col2id[0][0]]\n",
    "        \n",
    "        # we change type of column of excel to match the db\n",
    "        df_excel.rename(columns=col2check, inplace=True)\n",
    "        c = list(col2check.values())\n",
    "        c.append(\"id_patient\")\n",
    "        #for i in set(df_db.columns).difference(set(df_excel.columns)):\n",
    "        #    df_excel[i] = np.nan\n",
    "        df_excel.dropna(subset=\"id_patient\", inplace=True)\n",
    "        df_excel = df_excel.astype(df_db[list(c)].dtypes.to_dict())\n",
    "        nonan = [col2check[h] for h in col2check if h != \"\" and not col2check[h] in [x[1] for x in col2id]]\n",
    "        df_excel.dropna(subset=nonan, inplace=True)\n",
    "        \n",
    "        # on va scroll ligne par ligne dans l'excel\n",
    "        up = \"\"\n",
    "        for index, row in df_excel.iterrows():\n",
    "            r = df_db.loc[df_db[\"id_patient\"] == row[\"id_patient\"]]\n",
    "            for i in col2id:\n",
    "                if \"date\" in i[1]:\n",
    "                    r = r.loc[r[i[1]] == row[i[1]]]\n",
    "            condition = \"\\nWHERE \"\n",
    "            condition += \" id_patient = \\\"\"+str(row[\"id_patient\"])+\"\\\"\"\n",
    "            condition += \" and \"+col2id[1][1] +\" = \\\"\"+str(row[col2id[1][1]])+\"\\\"\"\n",
    "            \n",
    "            # 5. si la ligne existe, on verifie si les données sont coherentes ou meme presente\n",
    "            if row[list(col2check.values())].isnull().values.all() or pd.isna(row[\"id_patient\"]):\n",
    "                df_excel.drop(index, inplace=True)\n",
    "            elif len(r) == 1:\n",
    "                up2add = \"UPDATE \"+table+\" \\nSET \"#address = %s WHERE address = %s\"\n",
    "                avant = \"\\n# BEFORE: \"\n",
    "                col2change = \"\"\n",
    "                for h in col2check:\n",
    "                    if \"date\" not in h and h != \"\":\n",
    "                        indb = r[col2check[h]][r.index[0]]\n",
    "                        inexcel = row[col2check[h]]\n",
    "                        if pd.isna(indb):\n",
    "                            if not pd.isna(inexcel):\n",
    "                                col2change += str(col2check[h])+\" = '\"+str(inexcel)+\"', \"\n",
    "                                avant += str(col2check[h])+\" = '\"+str(indb)+\"', \"\n",
    "                        elif not pd.isna(inexcel):\n",
    "                            if indb != inexcel:\n",
    "                                col2change += str(col2check[h])+\" = '\"+str(inexcel)+\"', \"\n",
    "                                avant += str(col2check[h])+\" = '\"+str(indb)+\"', \"\n",
    "                if len(col2change) > 0:       \n",
    "                    col2change = col2change[:-2]\n",
    "                    up2add += col2change + avant + condition\n",
    "                    up += up2add + \";\\n\\n\"\n",
    "                df_excel.drop(index, inplace=True)\n",
    "            if len(r) > 1:\n",
    "                print(\"\")\n",
    "                print(\"error\")\n",
    "                sys.exit()\n",
    "        if len(up) > 0:\n",
    "            with open(\"sql_code/update_\"+file_update+\"_from_\"+(path_to_excel.split(\"\\\\\")[-1]).split(\".\")[0]+\".txt\", 'w') as f:\n",
    "                f.write(up)\n",
    "        df_excel.drop_duplicates(subset=[\"id_patient\", col2id[1][1]], keep='first', inplace=True)\n",
    "        if len(df_excel) > 0:\n",
    "            print(\"we will add:\\nin: \"+table)\n",
    "            colonne = list(col2check.values())\n",
    "            colonne.append(\"id_patient\")\n",
    "            display(df_excel[colonne])\n",
    "            if input(\"ajouter les valeur manquantes de \"+table+\" à la db?\") == \"y\":\n",
    "                df_excel[colonne].to_sql(name=table, con=engine, if_exists=\"append\", index=False)  \n",
    "                print(\"les données ont bien été ajoutées!\")\n",
    "                \n",
    "# l[3]+l[4] ne peut pas aller plus loin car id incoherent\n",
    "# select * from patient where MEDEX=\"AS4104B\" or PET_TAU=89 ATTENTION DEUX \n",
    "#      PERSONNES DIFF MAIS VEUT DONNER 89 A CE MEDEX ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa71421f",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "####################################### TAU DATA NEURO #################################################################\n",
    "########################################################################################################################\n",
    "\n",
    "print(\"doing patient\")\n",
    "add_info_from_excel(\n",
    "    table=\"patient\",\n",
    "    col2id=[[(\"MEDEX\", \"MEDEX\")], [(\"ID\", \"PET_TAU\")]],\n",
    "    col2check={\"MEDEX\": \"MEDEX\", \"ID\": \"PET_TAU\", \"DOB\": \"DOB\"},\n",
    "    col_with_date=[\"DOB\"],\n",
    "    col2modify_db=[[\"PET_TAU\", change_type, \"Int64\"]],\n",
    "    col2modify_excel=[[\"MEDEX\", make_upper], [\"ID\", change_type, \"Int64\"]],\n",
    "    path_to_excel='data\\csv\\csv\\TAU_DATA_NEUROPSY_T0.csv',\n",
    "    skip_rows=[1]\n",
    ")\n",
    "\n",
    "input(\"UPDATE THE NEW PATIENTS?\")\n",
    "\n",
    "# NEURO PSY TABLES manque: timepoint, [gremots_names, gremots_verbs] pour naming, [tics] pour mmse\n",
    "for timepoint in range(0,3):\n",
    "    print(\"timepoint: \"+str(timepoint))\n",
    "    # fcsrt + z\n",
    "    add_info_from_excel(\n",
    "        table=\"neuro_fcsrt\",\n",
    "        col2id=[[(\"ID\", \"PET_TAU\"), (\"Date_FCSRT\", \"date_fcsrt\")]],\n",
    "        col2check={\"ID\": \"PET_TAU\", \"Date_FCSRT\": \"date_fcsrt\", \"FCSRT_Immediate_cued_recall_Score\": \"immediate_cued_recall_score\", \n",
    "                   \"FCSRT_FR1_Score\": \"fr1_score\", \"FCSRT_FR2_Score\": \"fr2_score\", \"FCSRT_FR3_Score\": \"fr3_score\", \n",
    "                   \"FCSRT_Delayed_FR_Score\": \"delayed_fr_score\", \"FCSRT_TR1_Score\": \"tr1_score\", \"FCSRT_TR2_Score\": \"tr2_score\", \n",
    "                   \"FCSRT_TR3_Score\": \"tr3_score\", \"FCSRT_Delayed_TR_Score\": \"delayed_tr_score\", \"FCSRT_Recognition_Score\": \"recognition_score\", \n",
    "                   \"FCSRT_SUM_FR_Score\": \"sum_fr_score\", \"FCSRT_SUM_TR_Score\": \"sum_tr_score\", \"\":\"timepoint\"},\n",
    "        col_with_date=[\"Date_FCSRT\"],\n",
    "        col2modify_db=[[\"immediate_cued_recall_score\", change_type, \"float\"], [\"fr1_score\", change_type, \"float\"], \n",
    "                       [\"fr2_score\", change_type, \"float\"], [\"fr3_score\", change_type, \"float\"], [\"delayed_fr_score\", change_type, \"float\"], \n",
    "                       [\"tr1_score\", change_type, \"float\"], [\"tr2_score\", change_type, \"float\"], [\"tr3_score\", change_type, \"float\"], \n",
    "                       [\"delayed_tr_score\", change_type, \"float\"], [\"recognition_score\", change_type, \"float\"], [\"sum_fr_score\", change_type, \"float\"], \n",
    "                       [\"sum_tr_score\", change_type, \"float\"], [\"timepoint\", change_type, \"int\"]],\n",
    "\n",
    "        col2modify_excel=[[\"ID\", change_type, \"Int64\"], [\"FCSRT_Immediate_cued_recall_Score\", convert_coma], \n",
    "                          [\"FCSRT_FR1_Score\", convert_coma], [\"FCSRT_FR2_Score\", convert_coma], [\"FCSRT_FR3_Score\", convert_coma], \n",
    "                          [\"FCSRT_Delayed_FR_Score\", convert_coma], [\"FCSRT_TR1_Score\", convert_coma], [\"FCSRT_TR2_Score\", convert_coma], \n",
    "                          [\"FCSRT_TR3_Score\", convert_coma], [\"FCSRT_Delayed_TR_Score\", convert_coma], [\"FCSRT_Recognition_Score\", convert_coma], \n",
    "                          [\"FCSRT_SUM_FR_Score\", convert_coma], [\"FCSRT_SUM_TR_Score\", convert_coma], [\"timepoint\", str(timepoint)]],\n",
    "        path_to_excel='data\\csv\\csv\\TAU_DATA_NEUROPSY_T'+str(timepoint)+'.csv'\n",
    "    )\n",
    "\n",
    "    add_info_from_excel(\n",
    "        table=\"neuro_fcsrt_z\",\n",
    "        col2id=[[(\"ID\", \"PET_TAU\"), (\"Date_FCSRT\", \"date_fcsrt_z\")]],\n",
    "        col2check={\"ID\": \"PET_TAU\", \"Date_FCSRT\": \"date_fcsrt_z\", \"Z_FCSRT_SUM_FR\": \"sum_fr\", \n",
    "                   \"Z_FCSRT_SUM_TR\": \"sum_tr\", \"Z_FCSRT_Delayed_FR\": \"delayed_fr\", \"\": \"parameter\", \"\": \"timepoint\"},\n",
    "        col_with_date=[\"Date_FCSRT\"],\n",
    "        col2modify_db=[[\"sum_fr\", change_type, \"float\"], [\"sum_tr\", change_type, \"float\"], \n",
    "                       [\"delayed_fr\", change_type, \"float\"]],\n",
    "        col2modify_excel=[[\"ID\", change_type, \"Int64\"], [\"Z_FCSRT_SUM_FR\", convert_coma], \n",
    "                          [\"Z_FCSRT_SUM_TR\", convert_coma], [\"Z_FCSRT_Delayed_FR\", convert_coma], [\"parameter\", \"2\"], [\"timepoint\", str(timepoint)]],\n",
    "        path_to_excel='data\\csv\\csv\\TAU_DATA_NEUROPSY_T'+str(timepoint)+'.csv'\n",
    "    )\n",
    "\n",
    "        # luria\n",
    "    add_info_from_excel(\n",
    "        table=\"neuro_luria\",\n",
    "        col2id=[[(\"ID\", \"PET_TAU\"), (\"Date_Luria\", \"date_luria\")]],\n",
    "        col2check={\"ID\": \"PET_TAU\", \"Date_Luria\": \"date_luria\", \"Luria_Score\": \"score\", \"\": \"timepoint\"},\n",
    "        col_with_date=[\"Date_Luria\"],\n",
    "        col2modify_db=[[\"score\", change_type, \"float\"]],\n",
    "        col2modify_excel=[[\"ID\", change_type, \"Int64\"], [\"Luria_Score\", convert_coma], [\"timepoint\", str(timepoint)]],\n",
    "        path_to_excel='data\\csv\\csv\\TAU_DATA_NEUROPSY_T'+str(timepoint)+'.csv'\n",
    "    )\n",
    "\n",
    "    add_info_from_excel(\n",
    "        table=\"neuro_luria_z\",\n",
    "        col2id=[[(\"ID\", \"PET_TAU\"), (\"Date_Luria\", \"date_luria_z\")]],\n",
    "        col2check={\"ID\": \"PET_TAU\", \"Date_Luria\": \"date_luria_z\", \"Z_Luria\": \"z\", \n",
    "                   \"\": \"parameter\", \"\":\"timepoint\"},\n",
    "        col_with_date=[\"Date_Luria\"],\n",
    "        col2modify_db=[[\"z\", change_type, \"float\"]],\n",
    "        col2modify_excel=[[\"ID\", change_type, \"Int64\"], [\"Z_Luria\", convert_coma], [\"parameter\", \"2\"], \n",
    "                          [\"timepoint\", str(timepoint)]],\n",
    "        path_to_excel='data\\csv\\csv\\TAU_DATA_NEUROPSY_T'+str(timepoint)+'.csv'\n",
    "    )\n",
    "\n",
    "        # trail_making_test\n",
    "    add_info_from_excel(\n",
    "        table=\"neuro_trail_making_test\",\n",
    "        col2id=[[(\"ID\", \"PET_TAU\"), (\"Date_TMT\", \"date_trail_making_test\")]],\n",
    "        col2check={\"ID\": \"PET_TAU\", \"Date_TMT\": \"date_trail_making_test\", \"TMT_A_Time\": \"tmt_a_time\", \n",
    "                   \"TMT_B_Time\": \"tmt_b_time\", \"TMT_A_Errors\": \"tmt_a_errors\", \"TMT_B_Errors\": \"tmt_b_errors\", \n",
    "                   \"TMT_B_Perseverations\": \"tmt_b_perseverations\", \"TMT_B-A_Time\": \"tmt_b_a_time\", \n",
    "                   \"TMT_B-A_Errors\": \"tmt_b_a_errors\", \"\": \"timepoint\"},\n",
    "        col_with_date=[\"Date_TMT\"],\n",
    "        col2modify_db=[[\"tmt_a_time\", change_type, \"float\"], [\"tmt_b_time\", change_type, \"float\"], \n",
    "                       [\"tmt_a_errors\", change_type, \"float\"], [\"tmt_b_errors\", change_type, \"float\"], \n",
    "                       [\"tmt_b_perseverations\", change_type, \"float\"], [\"tmt_b_a_time\", change_type, \"float\"], \n",
    "                       [\"tmt_b_a_errors\", change_type, \"float\"]],\n",
    "        col2modify_excel=[[\"ID\", change_type, \"Int64\"], [\"TMT_A_Time\", convert_coma], \n",
    "                          [\"TMT_B_Time\", convert_coma], [\"TMT_A_Errors\", convert_coma], [\"TMT_B_Errors\", convert_coma], \n",
    "                          [\"TMT_B_Perseverations\", convert_coma], [\"TMT_B-A_Time\", convert_coma], \n",
    "                          [\"TMT_B-A_Errors\", convert_coma], [\"timepoint\", str(timepoint)]],\n",
    "        path_to_excel='data\\csv\\csv\\TAU_DATA_NEUROPSY_T'+str(timepoint)+'.csv'\n",
    "    )\n",
    "\n",
    "    add_info_from_excel(\n",
    "        table=\"neuro_trail_making_test_z\",\n",
    "        col2id=[[(\"ID\", \"PET_TAU\"), (\"Date_TMT\", \"date_trail_making_test_z\")]],\n",
    "        col2check={\"ID\": \"PET_TAU\", \"Date_TMT\": \"date_trail_making_test_z\", \n",
    "                   \"Z_TMT_B-A_Time\": \"z_tmt_b_a_time\", \"Z_TMT_B-A_Errors\": \"z_tmt_b_a_errors\", \"\": \"parameter\",\n",
    "                   \"\":\"timepoint\"},\n",
    "        col_with_date=[\"Date_TMT\"],\n",
    "        col2modify_db=[[\"z_tmt_b_a_errors\", change_type, \"float\"], [\"z_tmt_b_a_time\", change_type, \"float\"]],\n",
    "        col2modify_excel=[[\"ID\", change_type, \"Int64\"], [\"Z_TMT_B-A_Time\", convert_coma], \n",
    "                          [\"Z_TMT_B-A_Errors\", convert_coma], [\"parameter\", \"2\"], [\"timepoint\", str(timepoint)]],\n",
    "        path_to_excel='data\\csv\\csv\\TAU_DATA_NEUROPSY_T'+str(timepoint)+'.csv'\n",
    "    )\n",
    "\n",
    "        # fluency\n",
    "    add_info_from_excel(\n",
    "        table=\"neuro_fluency\",\n",
    "        col2id=[[(\"ID\", \"PET_TAU\"), (\"Date_Fluency\", \"date_fluency\")]],\n",
    "        col2check={\"ID\": \"PET_TAU\", \"Date_Fluency\": \"date_fluency\", \n",
    "                   \"Fluency_Animals\": \"animals\", \"Fluency_P\": \"p\", \"\": \"timepoint\"},\n",
    "        col_with_date=[\"Date_Fluency\"],\n",
    "        col2modify_db=[[\"animals\", change_type, \"float\"], [\"p\", change_type, \"float\"]],\n",
    "        col2modify_excel=[[\"ID\", change_type, \"Int64\"], [\"Fluency_Animals\", convert_coma], \n",
    "                          [\"Fluency_P\", convert_coma], [\"timepoint\", str(timepoint)]],\n",
    "        path_to_excel='data\\csv\\csv\\TAU_DATA_NEUROPSY_T'+str(timepoint)+'.csv'\n",
    "    )\n",
    "\n",
    "    add_info_from_excel(\n",
    "        table=\"neuro_fluency_z\",\n",
    "        col2id=[[(\"ID\", \"PET_TAU\"), (\"Date_Fluency\", \"date_fluency_z\")]],\n",
    "        col2check={\"ID\": \"PET_TAU\", \"Date_Fluency\": \"date_fluency_z\", \"Z_Fluency_Animals\": \"animals\", \n",
    "                   \"Z_Fluency_P\": \"p\", \"\": \"parameter\", \"\":\"timepoint\"},\n",
    "        col_with_date=[\"Date_Fluency\"],\n",
    "        col2modify_db=[[\"p\", change_type, \"float\"], [\"animals\", change_type, \"float\"]],\n",
    "        col2modify_excel=[[\"ID\", change_type, \"Int64\"], [\"Z_Fluency_Animals\", convert_coma], \n",
    "                          [\"Z_Fluency_P\", convert_coma], [\"parameter\", \"2\"], [\"timepoint\", str(timepoint)]],\n",
    "        path_to_excel='data\\csv\\csv\\TAU_DATA_NEUROPSY_T'+str(timepoint)+'.csv'\n",
    "    )\n",
    "\n",
    "        # cerad figure\n",
    "    add_info_from_excel(\n",
    "        table=\"neuro_cerad_figure_copy\",\n",
    "        col2id=[[(\"ID\", \"PET_TAU\"), (\"Date_CERAD_Figure_Copy\", \"date_cerad_figure_copy\")]],\n",
    "        col2check={\"ID\": \"PET_TAU\", \"Date_CERAD_Figure_Copy\": \"date_cerad_figure_copy\", \n",
    "                   \"CERAD_Figure_Copy_Score\": \"score\", \"\": \"timepoint\"},\n",
    "        col_with_date=[\"Date_CERAD_Figure_Copy\"],\n",
    "        col2modify_db=[[\"score\", change_type, \"float\"]],\n",
    "        col2modify_excel=[[\"ID\", change_type, \"Int64\"], [\"CERAD_Figure_Copy_Score\", convert_coma], \n",
    "                          [\"timepoint\", str(timepoint)]],\n",
    "        path_to_excel='data\\csv\\csv\\TAU_DATA_NEUROPSY_T'+str(timepoint)+'.csv'\n",
    "    )\n",
    "\n",
    "    add_info_from_excel(\n",
    "        table=\"neuro_cerad_figure_copy_z\",\n",
    "        col2id=[[(\"ID\", \"PET_TAU\"), (\"Date_CERAD_Figure_Copy\", \"date_cerad_figure_copy_z\")]],\n",
    "        col2check={\"ID\": \"PET_TAU\", \"Date_CERAD_Figure_Copy\": \"date_cerad_figure_copy_z\", \n",
    "                   \"Z_CERAD_Figure_Copy\": \"z\", \"\": \"parameter\", \"\":\"timepoint\"},\n",
    "        col_with_date=[\"Date_CERAD_Figure_Copy\"],\n",
    "        col2modify_db=[[\"z\", change_type, \"float\"]],\n",
    "        col2modify_excel=[[\"ID\", change_type, \"Int64\"], [\"Z_CERAD_Figure_Copy\", convert_coma], \n",
    "                          [\"parameter\", \"2\"], [\"timepoint\", str(timepoint)]],\n",
    "        path_to_excel='data\\csv\\csv\\TAU_DATA_NEUROPSY_T'+str(timepoint)+'.csv'\n",
    "    )\n",
    "\n",
    "        # clock\n",
    "    add_info_from_excel(\n",
    "        table=\"neuro_clock\",\n",
    "        col2id=[[(\"ID\", \"PET_TAU\"), (\"Date_Clock_Drawing\", \"date_clock\")]],\n",
    "        col2check={\"ID\": \"PET_TAU\", \"Date_Clock_Drawing\": \"date_clock\", \"Clock_Drawing\": \"drawing\", \n",
    "                   \"Clock_Copy\": \"copy\", \"\": \"timepoint\"},\n",
    "        col_with_date=[\"Date_Clock_Drawing\"],\n",
    "        col2modify_db=[[\"drawing\", change_type, \"float\"], [\"copy\", change_type, \"float\"]],\n",
    "        col2modify_excel=[[\"ID\", change_type, \"Int64\"], [\"Clock_Drawing\", convert_coma], \n",
    "                          [\"Clock_Copy\", convert_coma], [\"timepoint\", str(timepoint)]],\n",
    "        path_to_excel='data\\csv\\csv\\TAU_DATA_NEUROPSY_T'+str(timepoint)+'.csv'\n",
    "    )\n",
    "\n",
    "    add_info_from_excel(\n",
    "        table=\"neuro_clock_z\",\n",
    "        col2id=[[(\"ID\", \"PET_TAU\"), (\"Date_Clock_Drawing\", \"date_clock_z\")]],\n",
    "        col2check={\"ID\": \"PET_TAU\", \"Date_Clock_Drawing\": \"date_clock_z\", \"Z_Clock_Drawing\": \"drawing\", \n",
    "                   \"Z_Clock_Copy\": \"copy\", \"\": \"parameter\", \"\":\"timepoint\"},\n",
    "        col_with_date=[\"Date_Clock_Drawing\"],\n",
    "        col2modify_db=[[\"drawing\", change_type, \"float\"], [\"copy\", change_type, \"float\"]],\n",
    "        col2modify_excel=[[\"ID\", change_type, \"Int64\"], [\"Z_Clock_Copy\", convert_coma], \n",
    "                          [\"Z_Clock_Drawing\", convert_coma], [\"parameter\", \"2\"], [\"timepoint\", str(timepoint)]],\n",
    "        path_to_excel='data\\csv\\csv\\TAU_DATA_NEUROPSY_T'+str(timepoint)+'.csv'\n",
    "    )\n",
    "\n",
    "        # MMSE\n",
    "    add_info_from_excel(\n",
    "        table=\"neuro_mmse\",\n",
    "        col2id=[[(\"ID\", \"PET_TAU\"), (\"Date_MMSE\", \"date_mmse\")]],\n",
    "        col2check={\"ID\": \"PET_TAU\", \"Date_MMSE\": \"date_mmse\", \"MMSE\": \"mmse\", \"\": \"timepoint\", \n",
    "                  \"\": \"tics\"},\n",
    "        col_with_date=[\"Date_MMSE\"],\n",
    "        col2modify_db=[[\"mmse\", change_type, \"float\"]],\n",
    "        col2modify_excel=[[\"ID\", change_type, \"Int64\"], [\"MMSE\", convert_coma], \n",
    "                          [\"timepoint\", str(timepoint)], [\"tics\", str(np.nan)]],\n",
    "        path_to_excel='data\\csv\\csv\\TAU_DATA_NEUROPSY_T'+str(timepoint)+'.csv'\n",
    "    )\n",
    "\n",
    "        # naming\n",
    "    add_info_from_excel(\n",
    "        table=\"neuro_naming\",\n",
    "        col2id=[[(\"ID\", \"PET_TAU\"), (\"Date_Naming_LEXIS_64\", \"date_naming\")]],\n",
    "        col2check={\"ID\": \"PET_TAU\", \"Date_Naming_LEXIS_64\": \"date_naming\", \n",
    "                   \"Naming_Lexis_64\": \"lexis_64\", \"\": \"timepoint\", \"Naming_GREMOTS_Names\": \"gremots_names\", \n",
    "                   \"Naming_GREMOTS_Verbs\": \"gremots_verbs\"},\n",
    "        col_with_date=[\"Date_Naming_LEXIS_64\"],\n",
    "        col2modify_db=[[\"lexis_64\", change_type, \"float\"]],\n",
    "        col2modify_excel=[[\"ID\", change_type, \"Int64\"], [\"Naming_Lexis_64\", convert_coma], \n",
    "                          [\"Naming_GREMOTS_Names\", convert_coma], [\"Naming_GREMOTS_Verbs\", convert_coma], \n",
    "                          [\"timepoint\", str(timepoint)]],\n",
    "        path_to_excel='data\\csv\\csv\\TAU_DATA_NEUROPSY_T'+str(timepoint)+'.csv'\n",
    "    )\n",
    "\n",
    "    add_info_from_excel(\n",
    "        table=\"neuro_naming_lexis_z\",\n",
    "        col2id=[[(\"ID\", \"PET_TAU\"), (\"Date_Naming_LEXIS_64\", \"date_naming_lexis_z\")]],\n",
    "        col2check={\"ID\": \"PET_TAU\", \"Date_Naming_LEXIS_64\": \"date_naming_lexis_z\", \n",
    "                   \"Z_Naming_Lexis_64\": \"z\", \"\": \"parameter\", \"\":\"timepoint\"},\n",
    "        col_with_date=[\"Date_Naming_LEXIS_64\"],\n",
    "        col2modify_db=[[\"z\", change_type, \"float\"]],\n",
    "        col2modify_excel=[[\"ID\", change_type, \"Int64\"], [\"Z_Naming_Lexis_64\", convert_coma], \n",
    "                          [\"parameter\", \"2\"], [\"timepoint\", str(timepoint)]],\n",
    "        path_to_excel='data\\csv\\csv\\TAU_DATA_NEUROPSY_T'+str(timepoint)+'.csv'\n",
    "    )\n",
    "\n",
    "        # composite\n",
    "    add_info_from_excel(\n",
    "        table=\"neuro_composite\",\n",
    "        col2id=[[(\"ID\", \"PET_TAU\"), (\"Date_Composites\", \"date_composite\")]],\n",
    "        col2check={\"ID\": \"PET_TAU\", \"Date_Composites\": \"date_composite\", \"MEMORY_Composite\": \"memory\", \n",
    "                   \"EXECUTIVE_Composite\": \"executive\", \"LANGUAGE_Composite\": \"language\", \n",
    "                   \"VISUOSPATIAL_Composite\": \"visuospatial\", \"GLOBAL_COGNITIVE_Composite\": \"global_cognitive\", \n",
    "                   \"\": \"timepoint\"},\n",
    "        col_with_date=[\"Date_Composites\"],\n",
    "        col2modify_db=[[\"memory\", change_type, \"float\"], [\"executive\", change_type, \"float\"], \n",
    "                       [\"language\", change_type, \"float\"], [\"visuospatial\", change_type, \"float\"], \n",
    "                       [\"global_cognitive\", change_type, \"float\"]],\n",
    "        col2modify_excel=[[\"ID\", change_type, \"Int64\"], [\"MEMORY_Composite\", convert_coma], \n",
    "                          [\"EXECUTIVE_Composite\", convert_coma], [\"LANGUAGE_Composite\", convert_coma], \n",
    "                          [\"VISUOSPATIAL_Composite\", convert_coma], [\"GLOBAL_COGNITIVE_Composite\", convert_coma], \n",
    "                          [\"timepoint\", str(timepoint)]],\n",
    "        path_to_excel='data\\csv\\csv\\TAU_DATA_NEUROPSY_T'+str(timepoint)+'.csv'\n",
    "    )\n",
    "\n",
    "    print(\"shape\")\n",
    "        # shape\n",
    "    add_info_from_excel(\n",
    "        table=\"neuro_shapes\",\n",
    "        col2id=[[(\"ID\", \"PET_TAU\"), (\"Date_Shapes\", \"date_shapes\")]],\n",
    "        col2check={\"ID\": \"PET_TAU\", \"Date_Shapes\": \"date_shapes\", \"Shapes_Learning_Score\": \"learning_score\", \n",
    "                   \"Shapes_Forgetting_Score\": \"forgetting_score\", \"\": \"timepoint\"},\n",
    "        col_with_date=[\"Date_Shapes\"],\n",
    "        col2modify_db=[[\"forgetting_score\", change_type, \"float\"], [\"learning_score\", change_type, \"float\"]],\n",
    "        col2modify_excel=[[\"ID\", change_type, \"Int64\"], [\"Shapes_Learning_Score\", convert_coma], \n",
    "                          [\"Shapes_Forgetting_Score\", convert_coma], [\"timepoint\", str(timepoint)]],\n",
    "        path_to_excel='data\\csv\\csv\\TAU_DATA_NEUROPSY_T'+str(timepoint)+'.csv'\n",
    "    )\n",
    "\n",
    "    print(\"code\")\n",
    "        # code\n",
    "    add_info_from_excel(\n",
    "        table=\"neuro_code\",\n",
    "        col2id=[[(\"ID\", \"PET_TAU\"), (\"Date_Code\", \"date_code\")]],\n",
    "        col2check={\"ID\": \"PET_TAU\", \"Date_Code\": \"date_code\", \"Code_Score\": \"code\", \n",
    "                   \"\": \"timepoint\"},\n",
    "        col_with_date=[\"Date_Code\"],\n",
    "        col2modify_db=[[\"code\", change_type, \"float\"]],\n",
    "        col2modify_excel=[[\"ID\", change_type, \"Int64\"], [\"Code_Score\", convert_coma], [\"timepoint\", str(timepoint)]],\n",
    "        path_to_excel='data\\csv\\csv\\TAU_DATA_NEUROPSY_T'+str(timepoint)+'.csv'\n",
    "    )\n",
    "\n",
    "    print(\"logical memory\")\n",
    "\n",
    "        # logical memory\n",
    "    add_info_from_excel(\n",
    "        table=\"neuro_logical_memory\",\n",
    "        col2id=[[(\"ID\", \"PET_TAU\"), (\"Date_Logical_Memory\", \"date_logical_memory\")]],\n",
    "        col2check={\"ID\": \"PET_TAU\", \"Date_Logical_Memory\": \"date_logical_memory\", \n",
    "                   \"Logical_Memory_Items_Immediate_Recall_A&B\": \"items_immediate_recall_ab\", \n",
    "                   \"Logical_Memory_Themes_Immediate_Recall_A&B\": \"themes_immediate_recall_ab\", \n",
    "                   \"Logical_Memory_Items_Delayed_Recall_A\": \"items_delayed_recall_a\", \n",
    "                   \"Logical_Memory_Items_Delayed_Recall_A&B\": \"items_delayed_recall_ab\", \n",
    "                   \"Logical_Memory_Themes_Delayed_Recall_A&B\": \"themes_delayed_recall_ab\", \n",
    "                   \"\": \"timepoint\"},\n",
    "        col_with_date=[\"Date_Logical_Memory\"],\n",
    "        col2modify_db=[[\"items_immediate_recall_ab\", change_type, \"float\"], [\"themes_immediate_recall_ab\", change_type, \"float\"], \n",
    "                       [\"items_delayed_recall_a\", change_type, \"float\"], [\"items_delayed_recall_ab\", change_type, \"float\"], \n",
    "                       [\"themes_delayed_recall_ab\", change_type, \"float\"]],\n",
    "        col2modify_excel=[[\"ID\", change_type, \"Int64\"], [\"Logical_Memory_Items_Immediate_Recall_A&B\", convert_coma], \n",
    "                          [\"Logical_Memory_Themes_Immediate_Recall_A&B\", convert_coma], \n",
    "                          [\"Logical_Memory_Items_Delayed_Recall_A\", convert_coma], \n",
    "                          [\"Logical_Memory_Items_Delayed_Recall_A&B\", convert_coma], [\"Logical_Memory_Themes_Delayed_Recall_A&B\", convert_coma], \n",
    "                          [\"timepoint\", str(timepoint)]],\n",
    "        path_to_excel='data\\csv\\csv\\TAU_DATA_NEUROPSY_T'+str(timepoint)+'.csv'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cae7d08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# ATTENTION BM A PLUSIEURS FEUILLES\n",
    "# AUSSI Récap pourcentage atteinte Multiseuil\n",
    "# TAU_PET_Datasheet (1)\n",
    "l = [\"participants_AppleGame\", \"participants_MRI\", \"info_participant_anonyme\", \"Liste UCL AD\", \n",
    "     \"Resumé analyses SIMOA plasma\", \"TAU_Dg neuro complet_FINAL\", \"TAU_PET_Datasheet\", \"BM_DATA_NEUROPSY\", \"braak\", \"APOE\"]\n",
    "\n",
    "# update simoa plasma but need to equal numbers within number of digits\n",
    "# analyse lcr not the same in db and in TAU Dg neuro\n",
    "# doit encore faire 7 - 8 - 9\n",
    "current = l[7]\n",
    "allin = False\n",
    "if current == \"participants_AppleGame\" or allin:\n",
    "    add_info_from_excel(\n",
    "        table=\"patient\",\n",
    "        col2id=[[(\"Numéro UCL\", \"UCL2\")], [(\"Numéro Tau-PET\", \"PET_TAU\")], [(\"Numéro administratif\", \"MEDEX\")], [(\"Nom\", \"last_name\"), (\"Prénom\", \"first_name\"), (\"Date de naissance\", \"DOB\")]],\n",
    "        col2check={\"Numéro UCL\": \"UCL2\", \"Numéro Tau-PET\": \"PET_TAU\", \"Numéro administratif\": \"MEDEX\", \"APOE\": \"APOE\", \n",
    "                 \"Nom\": \"last_name\", \"Prénom\": \"first_name\", \"Date de naissance\": \"DOB\", \"sexe\": \"sexe\", \"contact\": \"mail\", \n",
    "                 \"telephone\" :\"phone\", \"recontacter_autres_Etude\": \"recontact_post_study\"},\n",
    "        col_with_date=[\"Date de naissance\"],\n",
    "        col2modify_db=[[\"last_name\", avoid_accent], [\"first_name\", avoid_accent],[\"APOE\", change_type, \"Int64\"], [\"UCL2\", change_type, \"Int64\"], [\"PET_TAU\", change_type, \"Int64\"], [\"BM\", change_type, \"Int64\"]],\n",
    "        col2modify_excel=[[\"Nom\", avoid_accent], [\"sexe\", make_upper], [\"sexe\", sexe], [\"Numéro UCL\", substring_ucl], [\"Nom\", make_upper], [\"Prénom\", make_lower], [\"Prénom\", avoid_accent], [\"Numéro UCL\", change_type, \"Int64\"], [\"Numéro Tau-PET\", change_type, \"Int64\"], [\"APOE\", change_type, \"Int64\"]],\n",
    "        path_to_excel='data\\csv\\csv\\participants_AppleGame.csv'\n",
    "    )\n",
    "    input(\"UPDATE THE NEW PATIENTS!\")\n",
    "if current == \"participants_MRI\" or allin:\n",
    "    # preprocess: faut ajouter une colonne (UCL_AD) et y mettre l id de la ligne 11 \"UCL-AD-xx\" en enlevant le prefix et laisser uniquement le nombre\n",
    "    # enlever le x dans la colonne des dates de ponction lombaire\n",
    "    # PATIENT\n",
    "    add_info_from_excel(\n",
    "        table=\"patient\",\n",
    "        col2id=[[(\"Num_UCL2\", \"UCL2\")], [(\"Num_PETtau\", \"PET_TAU\")], [(\"UCL_AD\", \"UCL_AD\")]],\n",
    "        col2check={\"Num_UCL2\": \"UCL2\", \"Num_PETtau\": \"PET_TAU\", \"UCL_AD\": \"UCL_AD\", \"Date_naissance\": \"DOB\", \"Sex\": \"sexe\"},\n",
    "        col_with_date=[\"Date_naissance\"],\n",
    "        col2modify_db=[[\"UCL2\", change_type, \"Int64\"] , [\"PET_TAU\", change_type, \"Int64\"], [\"UCL_AD\", change_type, \"Int64\"]],\n",
    "        col2modify_excel=[[\"Num_UCL2\", substring_ucl], [\"Sex\", make_upper], [\"Sex\", sexe], [\"Num_UCL2\", change_type, \"Int64\"] , [\"Num_PETtau\", change_type, \"Int64\"], [\"UCL_AD\", change_type, \"Int64\"]],\n",
    "        path_to_excel='data\\csv\\csv\\participants_MRI.csv'\n",
    "    )\n",
    "\n",
    "    \n",
    "    input(\"UPDATE THE NEW PATIENTS!\")\n",
    "    # PONCTION LOMBAIRE\n",
    "    add_info_from_excel(\n",
    "        table=\"analyse_lcr\",\n",
    "        col2id=[[(\"Num_UCL2\", \"UCL2\"), (\"Date_PL\", \"date_lcr\")]],\n",
    "        col2check={\"Num_UCL2\": \"UCL2\", \"Date_PL\": \"date_lcr\", \"PL_Ab142\": \"ab42\", \"PL_tau\": \"tau_total\", \"PL_ptau\": \"p_tau\"},\n",
    "        col_with_date=[\"Date_PL\"],\n",
    "        col2modify_db=[],\n",
    "        col2modify_excel=[[\"Num_UCL2\", substring_ucl], [\"Num_UCL2\", change_type, \"Int64\"]],\n",
    "        path_to_excel='data\\csv\\csv\\participants_MRI.csv',\n",
    "        file_update=\"analyse_lcr_ucl2\"\n",
    "    )\n",
    "    \n",
    "    add_info_from_excel(\n",
    "        table=\"analyse_lcr\",\n",
    "        file_update=\"analyse_lcr_uclad\",\n",
    "        col2id=[[(\"UCL_AD\", \"UCL_AD\"), (\"Date_PL\", \"date_lcr\")]],\n",
    "        col2check={\"UCL_AD\": \"UCL_AD\", \"Date_PL\": \"date_lcr\", \"PL_Ab142\": \"ab42\", \"PL_tau\": \"tau_total\", \"PL_ptau\": \"p_tau\"},\n",
    "        col_with_date=[\"Date_PL\"],\n",
    "        col2modify_db=[],\n",
    "        col2modify_excel=[[\"UCL_AD\", change_type, \"Int64\"]],\n",
    "        path_to_excel='data\\csv\\csv\\participants_MRI.csv'\n",
    "    )\n",
    "    \n",
    "    # PET AMY\n",
    "    add_info_from_excel(\n",
    "        table=\"pet_amy_info\",\n",
    "        col2id=[[(\"Num_UCL2\", \"UCL2\"), (\"Date_PET_amy\", \"date_pet_amy_info\")]],\n",
    "        col2check={\"Num_UCL2\": \"UCL2\", \"Date_PET_amy\": \"date_pet_amy_info\", \"centilloide_amy\": \"centilloide\", \"\": \"traceur\"},\n",
    "        col_with_date=[\"Date_PET_amy\"],\n",
    "        col2modify_db=[],\n",
    "        col2modify_excel=[[\"Num_UCL2\", substring_ucl], [\"Num_UCL2\", change_type, \"Int64\"], [\"traceur\", \"flutémétamol\"], [\"centilloide_amy\", convert_coma]],\n",
    "        path_to_excel='data\\csv\\csv\\participants_MRI.csv'\n",
    "    )\n",
    "    \n",
    "    add_info_from_excel(\n",
    "        table=\"pet_amy_info\",\n",
    "        file_update=\"pet_amy_info_uclad\",\n",
    "        col2id=[[(\"UCL_AD\", \"UCL_AD\"), (\"Date_PET_amy\", \"date_pet_amy_info\")]],\n",
    "        col2check={\"UCL_AD\": \"UCL_AD\", \"Date_PET_amy\": \"date_pet_amy_info\", \"centilloide_amy\": \"centilloide\", \"\": \"traceur\"},\n",
    "        col_with_date=[\"Date_PET_amy\"],\n",
    "        col2modify_db=[],\n",
    "        col2modify_excel=[[\"UCL_AD\", change_type, \"Int64\"], [\"traceur\", \"flutémétamol\"], [\"centilloide_amy\", convert_coma]],\n",
    "        path_to_excel='data\\csv\\csv\\participants_MRI.csv'\n",
    "    )\n",
    "if current == \"info_participant_anonyme\" or allin:\n",
    "    # preprocess: faut ajouter une colonne (UCL_AD) et y mettre l id de la ligne 11 \"UCL-AD-xx\" en enlevant le prefix et laisser uniquement le nombre\n",
    "    # enlever le x dans la colonne des dates de ponction lombaire, mettre des 0 devant le stop de smokingquand il manque\n",
    "    # ligne 54 et 101, date ta, mettre dans bon format\n",
    "    # PATIENT\n",
    "    print(\"doing patient\")\n",
    "    add_info_from_excel(\n",
    "        table=\"patient\",\n",
    "        col2id=[[(\"Identifiant\", \"UCL2\")]],\n",
    "        col2check={\"Identifiant\": \"UCL2\", \"Date_naissance\": \"DOB\", \"sexe\": \"sexe\", \"APOE\": \"APOE\", 'PaysDeNaissance': \"origin\"},\n",
    "        col_with_date=[\"Date_naissance\"],\n",
    "        col2modify_db=[[\"UCL2\", change_type, \"Int64\"], [\"APOE\", change_type, \"Int64\"]],\n",
    "        col2modify_excel=[[\"Identifiant\", substring_ucl], [\"sexe\", make_upper], [\"sexe\", sexe], [\"Identifiant\", change_type, \"Int64\"], [\"APOE\", change_type, \"Int64\"]],\n",
    "        path_to_excel='data\\csv\\csv\\info_participant_anonyme.csv'\n",
    "    )\n",
    "    \n",
    "    input(\"UPDATE THE NEW PATIENTS!\")\n",
    "    # FEATURING\n",
    "    print(\"doing features_patients\")\n",
    "    add_info_from_excel(\n",
    "        table=\"features_patients\",\n",
    "        col2id=[[(\"Identifiant\", \"UCL2\"), (\"DateAppleGame\", \"date_feat\")]],\n",
    "        col2check={\"Identifiant\": \"UCL2\", \"DateAppleGame\": \"date_feat\", \"AnneeEtude\": \"anneeetude_nbr\", \"Lateralité\": \"lateralite\", \n",
    "                    \"HeureSommeilParNuit\": \"hoursleepnight\", \"Poids\": \"weight\", \"Taille\" :\"height\", \n",
    "                    \"HeureSportParSemaine\": \"hoursportweek\", \"MemoireMoinsBonne\": \"worsememory\",\"Tabac\": \"smoking\", \n",
    "                    \"RISKcalculateur\": \"RISKcalc\", \"\": \"smoking_comments\"},\n",
    "        col_with_date=[\"DateAppleGame\"],\n",
    "        col2modify_db=[[\"anneeetude_nbr\", change_type, \"float\"], [\"hoursleepnight\", change_type, \"float\"], \n",
    "                       [\"weight\", change_type, \"float\"], [\"height\", change_type, \"float\"],\n",
    "                      [\"hoursportweek\", change_type, \"float\"], [\"RISKcalc\", change_type, \"float\"]],\n",
    "        col2modify_excel=[[\"Identifiant\", substring_ucl], [\"Identifiant\", change_type, \"Int64\"], \n",
    "                          [\"AnneeEtude\", convert_coma], [\"smoking_comments\", \"\"], \n",
    "                          [\"smoking_comments\", get_comments, \"Tabac\"], [\"Tabac\", make_binary], \n",
    "                          [\"AnneeEtude\", convert_coma], [\"HeureSommeilParNuit\", convert_coma], \n",
    "                          [\"Poids\", convert_coma], [\"Taille\", convert_coma], \n",
    "                          [\"HeureSportParSemaine\", convert_coma], [\"RISKcalculateur\", convert_coma]],\n",
    "        path_to_excel='data\\csv\\csv\\info_participant_anonyme.csv'\n",
    "    )\n",
    "    \n",
    "    # TENSION ARTERIELLE\n",
    "    print(\"doing tensionarterielle\")\n",
    "    add_info_from_excel(\n",
    "        table=\"tensionarterielle\",\n",
    "        col2id=[[(\"Identifiant\", \"UCL2\"), (\"Date_TA\", \"date_ta\")]],\n",
    "        col2check={\"TAs\": \"tas\", \"TAd\": \"tad\", \"Date_TA\": \"date_ta\", \"Identifiant\": \"UCL2\"},\n",
    "        col_with_date=[\"Date_TA\"],\n",
    "        col2modify_db=[[\"tas\", change_type, \"Int64\"],[\"tad\", change_type, \"Int64\"]],\n",
    "        col2modify_excel=[[\"Identifiant\", substring_ucl], [\"Identifiant\", change_type, \"Int64\"]],\n",
    "        path_to_excel='data\\csv\\csv\\info_participant_anonyme.csv'\n",
    "    )\n",
    "    \n",
    "    # CHOLESTEROL\n",
    "    print(\"doing cholesterol\")\n",
    "    add_info_from_excel(\n",
    "        table=\"cholesterol\",\n",
    "        col2id=[[(\"Identifiant\", \"UCL2\"), (\"date_pds\", \"date_cho\")]],\n",
    "        col2check={\"Identifiant\": \"UCL2\", \"date_pds\": \"date_cho\", 'cholesterol_tot (mg/dL)': \"cholesterol_tot\", \n",
    "                  'cholesterol_LDL (mg/dL)': \"cholesterol_ldl\", 'cholesterol_HDL (mg/dL)': \"cholesterol_hdl\", \n",
    "                  'triglycérides (mg/dL)': \"triglycerides\"},\n",
    "        col_with_date=[\"date_pds\"],\n",
    "        col2modify_db=[[\"cholesterol_tot\", change_type, \"float\"], [\"cholesterol_ldl\", change_type, \"float\"], \n",
    "                       [\"cholesterol_hdl\", change_type, \"float\"], [\"triglycerides\", change_type, \"float\"]],\n",
    "        col2modify_excel=[[\"Identifiant\", substring_ucl], [\"Identifiant\", change_type, \"Int64\"], \n",
    "                          [\"cholesterol_HDL (mg/dL)\", convert_coma]],\n",
    "        path_to_excel='data\\csv\\csv\\info_participant_anonyme.csv'\n",
    "    )\n",
    "    \n",
    "if current == \"Liste UCL AD\" or allin:\n",
    "    # PATIENT\n",
    "    print(\"doing patient\")\n",
    "    add_info_from_excel(\n",
    "        table=\"patient\",\n",
    "        col2id=[[(\"Colonne1\", \"UCL_AD\")], [(\"NA\", \"MEDEX\")], [(\"Numero TAU PET\", \"PET_TAU\")]],\n",
    "        col2check={\"Colonne1\": \"UCL_AD\", \"NA\": \"MEDEX\", \"Nom\": \"last_name\", \"Sexe\": \"sexe\", \"Numero TAU PET\": \"PET_TAU\"},\n",
    "        col_with_date=[],\n",
    "        col2modify_db=[[\"UCL_AD\", change_type, \"Int64\"], [\"PET_TAU\", change_type, \"Int64\"]],\n",
    "        col2modify_excel=[[\"Colonne1\", substring_ucl, False], [\"Sexe\", make_upper], [\"Sexe\", sexe], [\"Colonne1\", change_type, \"Int64\"], \n",
    "                          [\"Nom\", make_upper], [\"NA\", make_upper], [\"Numero TAU PET\", convert_coma], [\"Numero TAU PET\", change_type, \"Int64\"]],\n",
    "        path_to_excel='data\\csv\\csv\\Liste UCL AD.csv'\n",
    "    )\n",
    "\n",
    "    \n",
    "    input(\"UPDATE THE NEW PATIENTS!\")\n",
    "    # PONCTION LOMBAIRE\n",
    "    \"\"\"add_info_from_excel(\n",
    "        table=\"analyse_lcr\",\n",
    "        col2id=[[(\"Colonne1\", \"UCL_AD\"), (\"Date prélèvement\", \"date_lcr\")]],\n",
    "        col2check={\"Colonne1\": \"UCL_AD\", \"Aβ42 (> 432 pg/ml)\": \"ab42\", \"t-tau (< 381 pg/ml)\": \"tau_total\", \n",
    "                 \"p-tau 181 (< 61 pg/ml)\": \"p_tau\", \"Date prélèvement\": \"date_lcr\"},\n",
    "        col_with_date=[\"Date prélèvement\"],\n",
    "        col2modify_db=[],\n",
    "        col2modify_excel=[[\"Colonne1\", substring_ucl, False], [\"Colonne1\", change_type, \"Int64\"], \n",
    "                          [\"Aβ42 (> 432 pg/ml)\", change_type, \"float\"], [\"Aβ42 (> 432 pg/ml)\", change_type, \"float\"], \n",
    "                         [\"ab42\", \"t-tau (< 381 pg/ml)\", change_type, \"float\"], [\"p-tau 181 (< 61 pg/ml)\", change_type, \"float\"]],\n",
    "        path_to_excel='data\\csv\\csv\\Liste UCL AD.csv',\n",
    "        file_update=\"analyse_lcr\"\n",
    "    )\"\"\" # tout est vide\n",
    "    \n",
    "if current == \"Resumé analyses SIMOA plasma\" or allin:\n",
    "    # PATIENT\n",
    "    print(\"doing patient\")\n",
    "    add_info_from_excel(\n",
    "        table=\"patient\",\n",
    "        col2id=[[(\"Participant\", \"UCL_AD\")], [(\"UCL2\", \"UCL2\")], [(\"Numéro PET TAU / autre étude \", \"PET_TAU\")]],\n",
    "        col2check={\"Participant\": \"UCL_AD\", \"UCL2\": \"UCL2\", \"Numéro PET TAU / autre étude \": \"PET_TAU\"},\n",
    "        col_with_date=[],\n",
    "        col2modify_db=[[\"UCL_AD\", change_type, \"Int64\"], [\"PET_TAU\", change_type, \"Int64\"], [\"UCL2\", change_type, \"Int64\"]],\n",
    "        col2modify_excel=[[\"Participant\", substring_ucl, False], [\"Participant\", change_type, \"Int64\"], \n",
    "                          [\"UCL2\", substring_ucl], [\"UCL2\", change_type, \"Int64\"], \n",
    "                          [\"Numéro PET TAU / autre étude \", convert_coma], [\"Numéro PET TAU / autre étude \", change_type, \"Int64\"]],\n",
    "        path_to_excel='data\\csv\\csv\\Resumé analyses SIMOA plasma_UCL-AD à jour.csv'\n",
    "    )\n",
    "    \n",
    "    add_info_from_excel(\n",
    "        table=\"patient\",\n",
    "        col2id=[[(\"Participant\", \"UCL2\")]],\n",
    "        col2check={\"Participant\": \"UCL2\", \"DOB\": \"DOB\"},\n",
    "        col_with_date=[\"DOB\"],\n",
    "        col2modify_db=[[\"UCL2\", change_type, \"Int64\"]],\n",
    "        col2modify_excel=[[\"Participant\", substring_ucl], [\"Participant\", change_type, \"Int64\"]],\n",
    "        path_to_excel='data\\csv\\csv\\Resumé analyses SIMOA plasma_UCL2 à jour.csv'\n",
    "    )\n",
    "    \n",
    "    input(\"UPDATE THE NEW PATIENTS!\")\n",
    "    # SIMOAPLASMA\n",
    "    print(\"doing SIMOAPLASMA\")\n",
    "    add_info_from_excel(\n",
    "        table=\"simoa_plasma\",\n",
    "        col2id=[[(\"Participant\", \"UCL_AD\"), (\"Date PDS\", \"date_pds\"), (\"Date SIMOA\", \"date_simoa\")]],\n",
    "        col2check={\"Participant\": \"UCL_AD\", \"Date PDS\": \"date_pds\", \"Ab40\": \"ab40\", \"Ab42\": \"ab42\", \n",
    "                \"T-tau\": \"t_tau\", \"Date SIMOA\": \"date_simoa\"},\n",
    "        col_with_date=[\"Date PDS\", \"Date SIMOA\"],\n",
    "        col2modify_db=[[\"ab40\", change_type, \"float\"], [\"t_tau\", change_type, \"float\"], [\"ab42\", change_type, \"float\"]],\n",
    "        col2modify_excel=[[\"Participant\", substring_ucl, False], [\"Participant\", change_type, \"Int64\"], \n",
    "                          [\"Ab40\", convert_coma], [\"Ab42\", convert_coma], [\"T-tau\", convert_coma]],\n",
    "        path_to_excel='data\\csv\\csv\\Resumé analyses SIMOA plasma_UCL-AD à jour.csv'\n",
    "    )\n",
    "    \n",
    "    input(\"press to continue\")\n",
    "    add_info_from_excel(\n",
    "        table=\"simoa_plasma\",\n",
    "        col2id=[[(\"Participant\", \"UCL2\"), (\"Date PDS\", \"date_pds\"), (\"Date SIMOA\", \"date_simoa\")]],\n",
    "        col2check={\"Participant\": \"UCL2\", \"Date PDS\": \"date_pds\", \"Ab40\": \"ab40\", \"Ab42\": \"ab42\", \n",
    "                \"T-tau\": \"t_tau\", \"Date SIMOA\": \"date_simoa\"},\n",
    "        col_with_date=[\"Date PDS\", \"Date SIMOA\"],\n",
    "        col2modify_db=[[\"ab40\", change_type, \"float\"], [\"t_tau\", change_type, \"float\"], [\"ab42\", change_type, \"float\"]],\n",
    "        col2modify_excel=[[\"Participant\", substring_ucl], [\"Participant\", change_type, \"float\"], \n",
    "                          [\"Ab40\", convert_coma], [\"Ab42\", convert_coma], [\"T-tau\", convert_coma]],\n",
    "        path_to_excel='data\\csv\\csv\\Resumé analyses SIMOA plasma_UCL2 à jour.csv'\n",
    "    )\n",
    "    \n",
    "if current == \"TAU_Dg neuro complet_FINAL\" or allin:\n",
    "    # PATIENT\n",
    "    print(\"doing patient\")\n",
    "    add_info_from_excel(\n",
    "        table=\"patient\",\n",
    "        col2id=[[(\"MEDEX\", \"MEDEX\")], [(\"ID\", \"PET_TAU\")]],\n",
    "        col2check={\"MEDEX\": \"MEDEX\", \"DOB\": \"DOB\", \"Gender\": \"sexe\", \"ID\": \"PET_TAU\"},\n",
    "        col_with_date=[\"DOB\"],\n",
    "        col2modify_db=[[\"PET_TAU\", change_type, \"Int64\"]],\n",
    "        col2modify_excel=[[\"MEDEX\", make_upper], [\"ID\", change_type, \"Int64\"], [\"Gender\", make_upper], [\"Gender\", sexe], ],\n",
    "        path_to_excel='data\\csv\\csv\\TAU_Dg neuro complet_FINAL_JANV_2024.csv'\n",
    "    )\n",
    "    \n",
    "    input(\"UPDATE THE NEW PATIENTS!\")\n",
    "    \n",
    "    # FEATURE\n",
    "    print(\"doing feature\")\n",
    "    #TODO: remetter les bons labels pour diagnostics\n",
    "    #ATTENTION j ai effacé l id 89 car incoherence\n",
    "    add_info_from_excel(\n",
    "        table=\"features_patients\",\n",
    "        col2id=[[(\"MEDEX\", \"MEDEX\"), (\"Date_signature_consentement\", \"date_feat\")]],\n",
    "        col2check={\"MEDEX\": \"MEDEX\", \"Education_NSC\": \"anneeetude_rank\", \n",
    "                   \"Date_signature_consentement\": \"date_feat\", \"Classif_automatique_LQ\": \"diagnostic\"},\n",
    "        col_with_date=[\"Date_signature_consentement\"],\n",
    "        col2modify_db=[],\n",
    "        col2modify_excel=[[\"MEDEX\", make_upper], [\"Education_NSC\", change_type, \"Int64\"], \n",
    "                            [\"Classif_automatique_LQ\", sexe, [\"Preclinical AD\", \"amy+ cn\"]], \n",
    "                            [\"Classif_automatique_LQ\", sexe, [\"Amyloid- CN\", \"amy- cn\"]],\n",
    "                            [\"Classif_automatique_LQ\", sexe, [\"Non-AD MCI\", \"amy- mci\"]], \n",
    "                            [\"Classif_automatique_LQ\", sexe, [\"Prodromal AD\", \"amy+ mci\"]], \n",
    "                            [\"Classif_automatique_LQ\", sexe, [\"AD dementia\", \"amy+ dem\"]], \n",
    "                            [\"Classif_automatique_LQ\", sexe, [\"Non-AD dementia\", \"amy- dem\"]], \n",
    "                            [\"Classif_automatique_LQ\", sexe, [\"#N/A\", np.nan]]],\n",
    "        path_to_excel='data\\csv\\csv\\TAU_Dg neuro complet_FINAL_JANV_2024.csv'\n",
    "    )\n",
    "    \n",
    "    # PL\n",
    "    print(\"doing PL\")\n",
    "    add_info_from_excel(\n",
    "        table=\"analyse_lcr\",\n",
    "        col2id=[[(\"MEDEX\", \"MEDEX\"), (\"Date_PL\", \"date_lcr\")]],\n",
    "        col2check={\"MEDEX\": \"MEDEX\", \"Date_PL\": \"date_lcr\", \"Protéine P-Tau (pg/mL) Seuil = 61\": \"p_tau\", \n",
    "                \"Protéine Tau (pg/mL) Seuil = 381\": \"tau_total\", \"Beta-amyloïde 1-42 (pg/mL) Seuil = 437\": \"ab42\"},\n",
    "        col_with_date=[\"Date_PL\"],\n",
    "        col2modify_db=[[\"p_tau\", change_type, \"float\"], [\"tau_total\", change_type, \"float\"], [\"ab42\", change_type, \"float\"]],\n",
    "        col2modify_excel=[[\"MEDEX\", make_upper], [\"Protéine P-Tau (pg/mL) Seuil = 61\", convert_coma], \n",
    "                          [\"Protéine Tau (pg/mL) Seuil = 381\", convert_coma], [\"Beta-amyloïde 1-42 (pg/mL) Seuil = 437\", convert_coma]],\n",
    "        \n",
    "        path_to_excel='data\\csv\\csv\\TAU_Dg neuro complet_FINAL_JANV_2024.csv'\n",
    "    )\n",
    "    \n",
    "    # PET_AMY\n",
    "    # A VERIFIER\n",
    "    \"\"\"print(\"doing pet_amy_info\")\n",
    "    add_info_from_excel(\n",
    "        table=\"pet_amy_info\",\n",
    "        col2id=[[(\"MEDEX\", \"MEDEX\"), (\"Date PETFLUT\", \"date_pet_amy_info\")]],\n",
    "        col2check={\"MEDEX\": \"MEDEX\", \"Date PETFLUT\": \"date_pet_amy_info\", \n",
    "                   \"PET Flutemetamol centiloid\": \"centilloide\", \"\": \"traceur\"},\n",
    "        col_with_date=[\"Date PETFLUT\"],\n",
    "        col2modify_db=[[\"centilloide\", change_type, \"float\"]],\n",
    "        col2modify_excel=[[\"MEDEX\", make_upper], [\"traceur\", \"flutémétamol\"], [\"centilloide_amy\", convert_coma]],\n",
    "        path_to_excel='data\\csv\\csv\\TAU_Dg neuro complet_FINAL_JANV_2024.csv'\n",
    "    )\"\"\"\n",
    "    \n",
    "    # PET_FDG\n",
    "    # A VERIFIER que les colonnes sont bien pour le bon pet\n",
    "    \"\"\"print(\"doing PET_FDG\")\n",
    "    add_info_from_excel(\n",
    "        table=\"pet_fdg_info\",\n",
    "        col2id=[[(\"MEDEX\", \"MEDEX\"), (\"Date FDG\", \"date_pet_fdg_info\")]],\n",
    "        col2check={\"MEDEX\": \"MEDEX\", \"Date FDG\": \"date_pet_fdg_info\", \"FDG T-Sum score (limite 11090)\": \"tsum_score\", \"\": \"ad_score\"},\n",
    "        col_with_date=[\"Date FDG\"],\n",
    "        col2modify_db=[[\"tsum_score\", change_type, \"float\"]],\n",
    "        col2modify_excel=[[\"MEDEX\", make_upper], [\"ad_score\", \"0\"], [\"FDG T-Sum score (limite 11090)\", convert_coma]],\n",
    "        path_to_excel='data\\csv\\csv\\TAU_Dg neuro complet_FINAL_JANV_2024.csv'\n",
    "    )\"\"\"\n",
    "    \n",
    "    # tension arterielle\n",
    "    print(\"doing tension arterielle\")\n",
    "    add_info_from_excel(\n",
    "        table=\"tensionarterielle\",\n",
    "        col2id=[[(\"MEDEX\", \"MEDEX\"), (\"Date_TA\", \"date_ta\")]],\n",
    "        col2check={\"MEDEX\": \"MEDEX\", \"Date_TA\": \"date_ta\", \"TAs\": \"tas\", \"TAd\": \"tad\"},\n",
    "        col_with_date=[\"Date_TA\"],\n",
    "        col2modify_db=[[\"tas\", change_type, \"Int64\"], [\"tad\", change_type, \"Int64\"]],\n",
    "        col2modify_excel=[[\"MEDEX\", make_upper], [\"TAs\", convert_coma], [\"TAd\", convert_coma]],\n",
    "        path_to_excel='data\\csv\\csv\\TAU_Dg neuro complet_FINAL_JANV_2024.csv'\n",
    "    )\n",
    "    \n",
    "    # cholesterol\n",
    "    print(\"doing cholesterol\")\n",
    "    add_info_from_excel(\n",
    "        table=\"cholesterol\",\n",
    "        col2id=[[(\"MEDEX\", \"MEDEX\"), (\"date_cholesterol\", \"date_cho\")]],\n",
    "        col2check={\"MEDEX\": \"MEDEX\", \"date_cholesterol\": \"date_cho\", \"cholesterol_tot\": \"cholesterol_tot\", \n",
    "                  \"LDL\": \"cholesterol_ldl\", \"HDL\": \"cholesterol_hdl\", \"\": \"triglycerides\"}, # TRYGLYCERIDE???????\n",
    "        col_with_date=[\"date_cholesterol\"],\n",
    "        col2modify_db=[[\"cholesterol_tot\", change_type, \"float\"], [\"cholesterol_ldl\", change_type, \"float\"], \n",
    "                       [\"cholesterol_hdl\", change_type, \"float\"], [\"triglycerides\", change_type, \"float\"]],\n",
    "        col2modify_excel=[[\"MEDEX\", make_upper], [\"triglycerides\", \"0\"], [\"LDL\", convert_coma], [\"HDL\", convert_coma],\n",
    "                          [\"cholesterol_tot\", convert_coma]],\n",
    "        path_to_excel='data\\csv\\csv\\TAU_Dg neuro complet_FINAL_JANV_2024.csv'\n",
    "    )\n",
    "    \n",
    "elif current == \"TAU_PET_Datasheet\" or allin:\n",
    "    # PATIENT\n",
    "    print(\"doing patient\")\n",
    "    add_info_from_excel(\n",
    "        table=\"patient\",\n",
    "        col2id=[[(\"MEDEX\", \"MEDEX\")]],\n",
    "        col2check={\"NAME\": \"last_name\", \"SURNAME\": \"first_name\", \"MEDEX\": \"MEDEX\", \"GSM\": \"phone\", \n",
    "                 \"ACCOMPAGNANT\": \"attendant\", \"Mail\": \"mail\", \"DOB\": \"DOB\", \"SEX\": \"sexe\"},\n",
    "        col_with_date=[\"DOB\"],\n",
    "        col2modify_db=[],\n",
    "        col2modify_excel=[[\"MEDEX\", make_upper], [\"SEX\", make_upper], [\"SEX\", sexe], [\"SURNAME\", make_lower], [\"NAME\", make_upper]],\n",
    "        path_to_excel='data\\csv\\csv\\TAU_PET_Datasheet.csv'\n",
    "    )\n",
    "    \n",
    "if current == \"APOE\" or allin:\n",
    "    # preprocess: faut ajouter une colonne (UCL_AD) et y mettre l id de la ligne 11 \"UCL-AD-xx\" en enlevant le prefix et laisser uniquement le nombre\n",
    "    # enlever le x dans la colonne des dates de ponction lombaire, mettre des 0 devant le stop de smokingquand il manque\n",
    "    # ligne 54 et 101, date ta, mettre dans bon format\n",
    "    # PATIENT\n",
    "    print(\"doing patient\")\n",
    "    add_info_from_excel(\n",
    "        table=\"patient\",\n",
    "        col2id=[[(\"ID\", \"PET_TAU\")]],\n",
    "        col2check={\"ID\": \"PET_TAU\", \"APOE_genotype\": \"APOE\"},\n",
    "        col_with_date=[],\n",
    "        col2modify_db=[[\"PET_TAU\", change_type, \"Int64\"], [\"APOE\", change_type, \"Int64\"]],\n",
    "        col2modify_excel=[[\"ID\", change_type, \"Int64\"], [\"APOE_genotype\", makeapoeint],\n",
    "                          [\"APOE_genotype\", change_type, \"Int64\"]],\n",
    "        path_to_excel='data\\csv\\csv\\Template_Variables_Collaborators_Level_2_UCLouvain.csv'\n",
    "    )\n",
    "    \n",
    "    input(\"UPDATE THE NEW PATIENTS!\")\n",
    "if current == \"BM_DATA_NEUROPSY\" or allin:\n",
    "    # PATIENT\n",
    "    print(\"doing patient\")\n",
    "    add_info_from_excel(\n",
    "        table=\"patient\",\n",
    "        col2id=[[(\"MEDEX\", \"MEDEX\")], [(\"Subject_number_BM\", \"BM\")]],\n",
    "        col2check={\"MEDEX\": \"MEDEX\", \"Subject_number_BM\": \"BM\", \"DOB\": \"DOB\"},\n",
    "        col_with_date=[\"DOB\"],\n",
    "        col2modify_db=[[\"BM\", change_type, \"Int64\"]],\n",
    "        col2modify_excel=[[\"MEDEX\", make_upper], [\"Subject_number_BM\", change_type, \"Int64\"]],\n",
    "        path_to_excel='data\\csv\\csv\\BM_DATA_NEUROPSY_Parameters_Z_scores.csv',\n",
    "        skip_rows=[1]\n",
    "    )\n",
    "\n",
    "    add_info_from_excel(\n",
    "        table=\"patient\",\n",
    "        col2id=[[(\"Subject_number_BM\", \"BM\")]],\n",
    "        col2check={\"Subject_number_BM\": \"BM\", \"DOB\": \"DOB\", \"APOE_detail\": \"APOE\"},\n",
    "        col_with_date=[\"DOB\"],\n",
    "        col2modify_db=[[\"BM\", change_type, \"Int64\"]],\n",
    "        col2modify_excel=[[\"Subject_number_BM\", change_type, \"Int64\"], [\"APOE_detail\", convert_coma]],\n",
    "        path_to_excel='data\\csv\\csv\\BM_DATA_NEUROPSY_T0.csv',\n",
    "        skip_rows=[1]\n",
    "    )\n",
    "    \n",
    "    input(\"UPDATE THE NEW PATIENTS!\")\n",
    "\n",
    "    # FEATURE\n",
    "    add_info_from_excel(\n",
    "        table=\"features_patients\",\n",
    "        col2id=[[(\"Subject_number_BM\", \"BM\"), (\"Date_Cognitive_Assessment\", \"date_feat\")]],\n",
    "        col2check={\"Subject_number_BM\": \"BM\", \"Date_Cognitive_Assessment\": \"date_feat\", \"Education_NSC\": \"anneeetude_rank\", \n",
    "                   \"Education_Years\": \"anneeetude_nbr\", \"Height\": \"height\", \"Weight\": \"weight\"},\n",
    "        col_with_date=[\"Date_Cognitive_Assessment\"],\n",
    "        col2modify_db=[[\"anneeetude_rank\", change_type, \"Int64\"], [\"anneeetude_nbr\", change_type, \"float\"], \n",
    "                       [\"height\", change_type, \"float\"], [\"weight\", change_type, \"float\"]],\n",
    "        col2modify_excel=[[\"Subject_number_BM\", change_type, \"Int64\"], [\"APOE_detail\", convert_coma], [\"Education_NSC\", convert_coma], \n",
    "                          [\"Education_Years\", convert_coma], [\"Height\", convert_coma], [\"Weight\", convert_coma]],\n",
    "        path_to_excel='data\\csv\\csv\\BM_DATA_NEUROPSY_T0.csv',\n",
    "        skip_rows=[1]\n",
    "    )\n",
    "\n",
    "    # NEURO PSY TABLES manque: timepoint, [gremots_names, gremots_verbs] pour naming, [tics] pour mmse\n",
    "    for timepoint in range(0,6):\n",
    "        print(\"timepoint: \"+str(timepoint))\n",
    "        # fcsrt + z\n",
    "        add_info_from_excel(\n",
    "            table=\"neuro_fcsrt\",\n",
    "            col2id=[[(\"Subject_number_BM\", \"BM\"), (\"Date_Cognitive_Assessment\", \"date_fcsrt\")]],\n",
    "            col2check={\"Subject_number_BM\": \"BM\", \"Date_Cognitive_Assessment\": \"date_fcsrt\", \"FCSRT_Immediate_cued_recall_Score\": \"immediate_cued_recall_score\", \n",
    "                       \"FCSRT_FR1_Score\": \"fr1_score\", \"FCSRT_FR2_Score\": \"fr2_score\", \"FCSRT_FR3_Score\": \"fr3_score\", \n",
    "                       \"FCSRT_Delayed_FR_Score\": \"delayed_fr_score\", \"FCSRT_TR1_Score\": \"tr1_score\", \"FCSRT_TR2_Score\": \"tr2_score\", \n",
    "                       \"FCSRT_TR3_Score\": \"tr3_score\", \"FCSRT_Delayed_TR_Score\": \"delayed_tr_score\", \"FCSRT_Recognition_Score\": \"recognition_score\", \n",
    "                       \"FCSRT_SUM_FR_Score\": \"sum_fr_score\", \"FCSRT_SUM_TR_Score\": \"sum_tr_score\", \"\":\"timepoint\"},\n",
    "            col_with_date=[\"Date_Cognitive_Assessment\"],\n",
    "            col2modify_db=[[\"immediate_cued_recall_score\", change_type, \"float\"], [\"fr1_score\", change_type, \"float\"], \n",
    "                           [\"fr2_score\", change_type, \"float\"], [\"fr3_score\", change_type, \"float\"], [\"delayed_fr_score\", change_type, \"float\"], \n",
    "                           [\"tr1_score\", change_type, \"float\"], [\"tr2_score\", change_type, \"float\"], [\"tr3_score\", change_type, \"float\"], \n",
    "                           [\"delayed_tr_score\", change_type, \"float\"], [\"recognition_score\", change_type, \"float\"], [\"sum_fr_score\", change_type, \"float\"], \n",
    "                           [\"sum_tr_score\", change_type, \"float\"], [\"timepoint\", change_type, \"int\"]],\n",
    "\n",
    "            col2modify_excel=[[\"Subject_number_BM\", change_type, \"Int64\"], [\"FCSRT_Immediate_cued_recall_Score\", convert_coma], \n",
    "                              [\"FCSRT_FR1_Score\", convert_coma], [\"FCSRT_FR2_Score\", convert_coma], [\"FCSRT_FR3_Score\", convert_coma], \n",
    "                              [\"FCSRT_Delayed_FR_Score\", convert_coma], [\"FCSRT_TR1_Score\", convert_coma], [\"FCSRT_TR2_Score\", convert_coma], \n",
    "                              [\"FCSRT_TR3_Score\", convert_coma], [\"FCSRT_Delayed_TR_Score\", convert_coma], [\"FCSRT_Recognition_Score\", convert_coma], \n",
    "                              [\"FCSRT_SUM_FR_Score\", convert_coma], [\"FCSRT_SUM_TR_Score\", convert_coma], [\"timepoint\", str(timepoint)]],\n",
    "            path_to_excel='data\\csv\\csv\\BM_DATA_NEUROPSY_T'+str(timepoint)+'.csv',\n",
    "            skip_rows=[1]\n",
    "        )\n",
    "\n",
    "        add_info_from_excel(\n",
    "            table=\"neuro_fcsrt_z\",\n",
    "            col2id=[[(\"Subject_number_BM\", \"BM\"), (\"Date_Cognitive_Assessment\", \"date_fcsrt_z\")]],\n",
    "            col2check={\"Subject_number_BM\": \"BM\", \"Date_Cognitive_Assessment\": \"date_fcsrt_z\", \"Z_FCSRT_SUM_FR\": \"sum_fr\", \n",
    "                       \"Z_FCSRT_SUM_TR\": \"sum_tr\", \"Z_FCSRT_Delayed_FR\": \"delayed_fr\", \"\": \"parameter\", \"\": \"timepoint\"},\n",
    "            col_with_date=[\"Date_Cognitive_Assessment\"],\n",
    "            col2modify_db=[[\"sum_fr\", change_type, \"float\"], [\"sum_tr\", change_type, \"float\"], \n",
    "                           [\"delayed_fr\", change_type, \"float\"]],\n",
    "            col2modify_excel=[[\"Subject_number_BM\", change_type, \"Int64\"], [\"Z_FCSRT_SUM_FR\", convert_coma], \n",
    "                              [\"Z_FCSRT_SUM_TR\", convert_coma], [\"Z_FCSRT_Delayed_FR\", convert_coma], [\"parameter\", \"2\"], [\"timepoint\", str(timepoint)]],\n",
    "            path_to_excel='data\\csv\\csv\\BM_DATA_NEUROPSY_T'+str(timepoint)+'.csv',\n",
    "            skip_rows=[1]\n",
    "        )\n",
    "\n",
    "            # luria\n",
    "        add_info_from_excel(\n",
    "            table=\"neuro_luria\",\n",
    "            col2id=[[(\"Subject_number_BM\", \"BM\"), (\"Date_Cognitive_Assessment\", \"date_luria\")]],\n",
    "            col2check={\"Subject_number_BM\": \"BM\", \"Date_Cognitive_Assessment\": \"date_luria\", \"Luria_Score\": \"score\", \"\": \"timepoint\"},\n",
    "            col_with_date=[\"Date_Cognitive_Assessment\"],\n",
    "            col2modify_db=[[\"score\", change_type, \"float\"]],\n",
    "            col2modify_excel=[[\"Subject_number_BM\", change_type, \"Int64\"], [\"Luria_Score\", convert_coma], [\"timepoint\", str(timepoint)]],\n",
    "            path_to_excel='data\\csv\\csv\\BM_DATA_NEUROPSY_T'+str(timepoint)+'.csv',\n",
    "            skip_rows=[1]\n",
    "        )\n",
    "\n",
    "        add_info_from_excel(\n",
    "            table=\"neuro_luria_z\",\n",
    "            col2id=[[(\"Subject_number_BM\", \"BM\"), (\"Date_Cognitive_Assessment\", \"date_luria_z\")]],\n",
    "            col2check={\"Subject_number_BM\": \"BM\", \"Date_Cognitive_Assessment\": \"date_luria_z\", \"Z_Luria\": \"z\", \n",
    "                       \"\": \"parameter\", \"\":\"timepoint\"},\n",
    "            col_with_date=[\"Date_Cognitive_Assessment\"],\n",
    "            col2modify_db=[[\"z\", change_type, \"float\"]],\n",
    "            col2modify_excel=[[\"Subject_number_BM\", change_type, \"Int64\"], [\"Z_Luria\", convert_coma], [\"parameter\", \"2\"], \n",
    "                              [\"timepoint\", str(timepoint)]],\n",
    "            path_to_excel='data\\csv\\csv\\BM_DATA_NEUROPSY_T'+str(timepoint)+'.csv',\n",
    "            skip_rows=[1]\n",
    "        )\n",
    "\n",
    "            # trail_making_test\n",
    "        add_info_from_excel(\n",
    "            table=\"neuro_trail_making_test\",\n",
    "            col2id=[[(\"Subject_number_BM\", \"BM\"), (\"Date_Cognitive_Assessment\", \"date_trail_making_test\")]],\n",
    "            col2check={\"Subject_number_BM\": \"BM\", \"Date_Cognitive_Assessment\": \"date_trail_making_test\", \"TMT_A_Time\": \"tmt_a_time\", \n",
    "                       \"TMT_B_Time\": \"tmt_b_time\", \"TMT_A_Errors\": \"tmt_a_errors\", \"TMT_B_Errors\": \"tmt_b_errors\", \n",
    "                       \"TMT_B_Perseverations\": \"tmt_b_perseverations\", \"TMT_B-A_Time\": \"tmt_b_a_time\", \n",
    "                       \"TMT_B-A_Errors\": \"tmt_b_a_errors\", \"\": \"timepoint\"},\n",
    "            col_with_date=[\"Date_Cognitive_Assessment\"],\n",
    "            col2modify_db=[[\"tmt_a_time\", change_type, \"float\"], [\"tmt_b_time\", change_type, \"float\"], \n",
    "                           [\"tmt_a_errors\", change_type, \"float\"], [\"tmt_b_errors\", change_type, \"float\"], \n",
    "                           [\"tmt_b_perseverations\", change_type, \"float\"], [\"tmt_b_a_time\", change_type, \"float\"], \n",
    "                           [\"tmt_b_a_errors\", change_type, \"float\"]],\n",
    "            col2modify_excel=[[\"Subject_number_BM\", change_type, \"Int64\"], [\"TMT_A_Time\", convert_coma], \n",
    "                              [\"TMT_B_Time\", convert_coma], [\"TMT_A_Errors\", convert_coma], [\"TMT_B_Errors\", convert_coma], \n",
    "                              [\"TMT_B_Perseverations\", convert_coma], [\"TMT_B-A_Time\", convert_coma], \n",
    "                              [\"TMT_B-A_Errors\", convert_coma], [\"timepoint\", str(timepoint)]],\n",
    "            path_to_excel='data\\csv\\csv\\BM_DATA_NEUROPSY_T'+str(timepoint)+'.csv',\n",
    "            skip_rows=[1]\n",
    "        )\n",
    "\n",
    "        add_info_from_excel(\n",
    "            table=\"neuro_trail_making_test_z\",\n",
    "            col2id=[[(\"Subject_number_BM\", \"BM\"), (\"Date_Cognitive_Assessment\", \"date_trail_making_test_z\")]],\n",
    "            col2check={\"Subject_number_BM\": \"BM\", \"Date_Cognitive_Assessment\": \"date_trail_making_test_z\", \n",
    "                       \"Z_TMT_B-A_Time\": \"z_tmt_b_a_time\", \"Z_TMT_B-A_Errors\": \"z_tmt_b_a_errors\", \"\": \"parameter\",\n",
    "                       \"\":\"timepoint\"},\n",
    "            col_with_date=[\"Date_Cognitive_Assessment\"],\n",
    "            col2modify_db=[[\"z_tmt_b_a_errors\", change_type, \"float\"], [\"z_tmt_b_a_time\", change_type, \"float\"]],\n",
    "            col2modify_excel=[[\"Subject_number_BM\", change_type, \"Int64\"], [\"Z_TMT_B-A_Time\", convert_coma], \n",
    "                              [\"Z_TMT_B-A_Errors\", convert_coma], [\"parameter\", \"2\"], [\"timepoint\", str(timepoint)]],\n",
    "            path_to_excel='data\\csv\\csv\\BM_DATA_NEUROPSY_T'+str(timepoint)+'.csv',\n",
    "            skip_rows=[1]\n",
    "        )\n",
    "\n",
    "            # fluency\n",
    "        add_info_from_excel(\n",
    "            table=\"neuro_fluency\",\n",
    "            col2id=[[(\"Subject_number_BM\", \"BM\"), (\"Date_Cognitive_Assessment\", \"date_fluency\")]],\n",
    "            col2check={\"Subject_number_BM\": \"BM\", \"Date_Cognitive_Assessment\": \"date_fluency\", \n",
    "                       \"Fluency_Animals\": \"animals\", \"Fluency_P\": \"p\", \"\": \"timepoint\"},\n",
    "            col_with_date=[\"Date_Cognitive_Assessment\"],\n",
    "            col2modify_db=[[\"animals\", change_type, \"float\"], [\"p\", change_type, \"float\"]],\n",
    "            col2modify_excel=[[\"Subject_number_BM\", change_type, \"Int64\"], [\"Fluency_Animals\", convert_coma], \n",
    "                              [\"Fluency_P\", convert_coma], [\"timepoint\", str(timepoint)]],\n",
    "            path_to_excel='data\\csv\\csv\\BM_DATA_NEUROPSY_T'+str(timepoint)+'.csv',\n",
    "            skip_rows=[1]\n",
    "        )\n",
    "\n",
    "        add_info_from_excel(\n",
    "            table=\"neuro_fluency_z\",\n",
    "            col2id=[[(\"Subject_number_BM\", \"BM\"), (\"Date_Cognitive_Assessment\", \"date_fluency_z\")]],\n",
    "            col2check={\"Subject_number_BM\": \"BM\", \"Date_Cognitive_Assessment\": \"date_fluency_z\", \"Z_Fluency_Animals\": \"animals\", \n",
    "                       \"Z_Fluency_P\": \"p\", \"\": \"parameter\", \"\":\"timepoint\"},\n",
    "            col_with_date=[\"Date_Cognitive_Assessment\"],\n",
    "            col2modify_db=[[\"p\", change_type, \"float\"], [\"animals\", change_type, \"float\"]],\n",
    "            col2modify_excel=[[\"Subject_number_BM\", change_type, \"Int64\"], [\"Z_Fluency_Animals\", convert_coma], \n",
    "                              [\"Z_Fluency_P\", convert_coma], [\"parameter\", \"2\"], [\"timepoint\", str(timepoint)]],\n",
    "            path_to_excel='data\\csv\\csv\\BM_DATA_NEUROPSY_T'+str(timepoint)+'.csv',\n",
    "            skip_rows=[1]\n",
    "        )\n",
    "\n",
    "            # cerad figure\n",
    "        add_info_from_excel(\n",
    "            table=\"neuro_cerad_figure_copy\",\n",
    "            col2id=[[(\"Subject_number_BM\", \"BM\"), (\"Date_Cognitive_Assessment\", \"date_cerad_figure_copy\")]],\n",
    "            col2check={\"Subject_number_BM\": \"BM\", \"Date_Cognitive_Assessment\": \"date_cerad_figure_copy\", \n",
    "                       \"CERAD_Figure_Copy_Score\": \"score\", \"\": \"timepoint\"},\n",
    "            col_with_date=[\"Date_Cognitive_Assessment\"],\n",
    "            col2modify_db=[[\"score\", change_type, \"float\"]],\n",
    "            col2modify_excel=[[\"Subject_number_BM\", change_type, \"Int64\"], [\"CERAD_Figure_Copy_Score\", convert_coma], \n",
    "                              [\"timepoint\", str(timepoint)]],\n",
    "            path_to_excel='data\\csv\\csv\\BM_DATA_NEUROPSY_T'+str(timepoint)+'.csv',\n",
    "            skip_rows=[1]\n",
    "        )\n",
    "\n",
    "        add_info_from_excel(\n",
    "            table=\"neuro_cerad_figure_copy_z\",\n",
    "            col2id=[[(\"Subject_number_BM\", \"BM\"), (\"Date_Cognitive_Assessment\", \"date_cerad_figure_copy_z\")]],\n",
    "            col2check={\"Subject_number_BM\": \"BM\", \"Date_Cognitive_Assessment\": \"date_cerad_figure_copy_z\", \n",
    "                       \"Z_CERAD_Figure_Copy\": \"z\", \"\": \"parameter\", \"\":\"timepoint\"},\n",
    "            col_with_date=[\"Date_Cognitive_Assessment\"],\n",
    "            col2modify_db=[[\"z\", change_type, \"float\"]],\n",
    "            col2modify_excel=[[\"Subject_number_BM\", change_type, \"Int64\"], [\"Z_CERAD_Figure_Copy\", convert_coma], \n",
    "                              [\"parameter\", \"2\"], [\"timepoint\", str(timepoint)]],\n",
    "            path_to_excel='data\\csv\\csv\\BM_DATA_NEUROPSY_T'+str(timepoint)+'.csv',\n",
    "            skip_rows=[1]\n",
    "        )\n",
    "\n",
    "            # clock\n",
    "        add_info_from_excel(\n",
    "            table=\"neuro_clock\",\n",
    "            col2id=[[(\"Subject_number_BM\", \"BM\"), (\"Date_Cognitive_Assessment\", \"date_clock\")]],\n",
    "            col2check={\"Subject_number_BM\": \"BM\", \"Date_Cognitive_Assessment\": \"date_clock\", \"Clock_Drawing\": \"drawing\", \n",
    "                       \"Clock_copy\": \"copy\", \"\": \"timepoint\"},\n",
    "            col_with_date=[\"Date_Cognitive_Assessment\"],\n",
    "            col2modify_db=[[\"drawing\", change_type, \"float\"], [\"copy\", change_type, \"float\"]],\n",
    "            col2modify_excel=[[\"Subject_number_BM\", change_type, \"Int64\"], [\"Clock_Drawing\", convert_coma], \n",
    "                              [\"Clock_copy\", convert_coma], [\"timepoint\", str(timepoint)]],\n",
    "            path_to_excel='data\\csv\\csv\\BM_DATA_NEUROPSY_T'+str(timepoint)+'.csv',\n",
    "            skip_rows=[1]\n",
    "        )\n",
    "\n",
    "        add_info_from_excel(\n",
    "            table=\"neuro_clock_z\",\n",
    "            col2id=[[(\"Subject_number_BM\", \"BM\"), (\"Date_Cognitive_Assessment\", \"date_clock_z\")]],\n",
    "            col2check={\"Subject_number_BM\": \"BM\", \"Date_Cognitive_Assessment\": \"date_clock_z\", \"Z_Clock_Drawing\": \"drawing\", \n",
    "                       \"Z_Clock_Copy\": \"copy\", \"\": \"parameter\", \"\":\"timepoint\"},\n",
    "            col_with_date=[\"Date_Cognitive_Assessment\"],\n",
    "            col2modify_db=[[\"drawing\", change_type, \"float\"], [\"copy\", change_type, \"float\"]],\n",
    "            col2modify_excel=[[\"Subject_number_BM\", change_type, \"Int64\"], [\"Z_Clock_Copy\", convert_coma], \n",
    "                              [\"Z_Clock_Drawing\", convert_coma], [\"parameter\", \"2\"], [\"timepoint\", str(timepoint)]],\n",
    "            path_to_excel='data\\csv\\csv\\BM_DATA_NEUROPSY_T'+str(timepoint)+'.csv',\n",
    "            skip_rows=[1]\n",
    "        )\n",
    "\n",
    "            # MMSE\n",
    "        add_info_from_excel(\n",
    "            table=\"neuro_mmse\",\n",
    "            col2id=[[(\"Subject_number_BM\", \"BM\"), (\"Date_Cognitive_Assessment\", \"date_mmse\")]],\n",
    "            col2check={\"Subject_number_BM\": \"BM\", \"Date_Cognitive_Assessment\": \"date_mmse\", \"MMSE\": \"mmse\", \"\": \"timepoint\", \n",
    "                      \"\": \"tics\"},\n",
    "            col_with_date=[\"Date_Cognitive_Assessment\"],\n",
    "            col2modify_db=[[\"mmse\", change_type, \"float\"]],\n",
    "            col2modify_excel=[[\"Subject_number_BM\", change_type, \"Int64\"], [\"MMSE\", convert_coma], \n",
    "                              [\"timepoint\", str(timepoint)], [\"tics\", str(np.nan)]],\n",
    "            path_to_excel='data\\csv\\csv\\BM_DATA_NEUROPSY_T'+str(timepoint)+'.csv',\n",
    "            skip_rows=[1]\n",
    "        )\n",
    "\n",
    "            # naming\n",
    "        add_info_from_excel(\n",
    "            table=\"neuro_naming\",\n",
    "            col2id=[[(\"Subject_number_BM\", \"BM\"), (\"Date_Cognitive_Assessment\", \"date_naming\")]],\n",
    "            col2check={\"Subject_number_BM\": \"BM\", \"Date_Cognitive_Assessment\": \"date_naming\", \n",
    "                       \"Naming_Lexis_64\": \"lexis_64\", \"\": \"timepoint\", \"\": \"gremots_names\", \"\": \"gremots_verbs\"},\n",
    "            col_with_date=[\"Date_Cognitive_Assessment\"],\n",
    "            col2modify_db=[[\"lexis_64\", change_type, \"float\"]],\n",
    "            col2modify_excel=[[\"Subject_number_BM\", change_type, \"Int64\"], [\"Naming_Lexis_64\", convert_coma], \n",
    "                              [\"timepoint\", str(timepoint)], [\"gremots_names\", str(np.nan)], [\"gremots_verbs\", str(np.nan)]],\n",
    "            path_to_excel='data\\csv\\csv\\BM_DATA_NEUROPSY_T'+str(timepoint)+'.csv',\n",
    "            skip_rows=[1]\n",
    "        )\n",
    "\n",
    "        add_info_from_excel(\n",
    "            table=\"neuro_naming_lexis_z\",\n",
    "            col2id=[[(\"Subject_number_BM\", \"BM\"), (\"Date_Cognitive_Assessment\", \"date_naming_lexis_z\")]],\n",
    "            col2check={\"Subject_number_BM\": \"BM\", \"Date_Cognitive_Assessment\": \"date_naming_lexis_z\", \n",
    "                       \"Z_Naming_Lexis_64\": \"z\", \"\": \"parameter\", \"\":\"timepoint\"},\n",
    "            col_with_date=[\"Date_Cognitive_Assessment\"],\n",
    "            col2modify_db=[[\"z\", change_type, \"float\"]],\n",
    "            col2modify_excel=[[\"Subject_number_BM\", change_type, \"Int64\"], [\"Z_Naming_Lexis_64\", convert_coma], \n",
    "                              [\"parameter\", \"2\"], [\"timepoint\", str(timepoint)]],\n",
    "            path_to_excel='data\\csv\\csv\\BM_DATA_NEUROPSY_T'+str(timepoint)+'.csv',\n",
    "            skip_rows=[1]\n",
    "        )\n",
    "\n",
    "            # composite\n",
    "        add_info_from_excel(\n",
    "            table=\"neuro_composite\",\n",
    "            col2id=[[(\"Subject_number_BM\", \"BM\"), (\"Date_Cognitive_Assessment\", \"date_composite\")]],\n",
    "            col2check={\"Subject_number_BM\": \"BM\", \"Date_Cognitive_Assessment\": \"date_composite\", \"MEMORY_Composite\": \"memory\", \n",
    "                       \"EXECUTIVE_Composite\": \"executive\", \"LANGUAGE_Composite\": \"language\", \n",
    "                       \"VISUOSPATIAL_Composite\": \"visuospatial\", \"GLOBAL_COGNITIVE_Composite\": \"global_cognitive\", \n",
    "                       \"\": \"timepoint\"},\n",
    "            col_with_date=[\"Date_Cognitive_Assessment\"],\n",
    "            col2modify_db=[[\"memory\", change_type, \"float\"], [\"executive\", change_type, \"float\"], \n",
    "                           [\"language\", change_type, \"float\"], [\"visuospatial\", change_type, \"float\"], \n",
    "                           [\"global_cognitive\", change_type, \"float\"]],\n",
    "            col2modify_excel=[[\"Subject_number_BM\", change_type, \"Int64\"], [\"MEMORY_Composite\", convert_coma], \n",
    "                              [\"EXECUTIVE_Composite\", convert_coma], [\"LANGUAGE_Composite\", convert_coma], \n",
    "                              [\"VISUOSPATIAL_Composite\", convert_coma], [\"GLOBAL_COGNITIVE_Composite\", convert_coma], \n",
    "                              [\"timepoint\", str(timepoint)]],\n",
    "            path_to_excel='data\\csv\\csv\\BM_DATA_NEUROPSY_T'+str(timepoint)+'.csv',\n",
    "            skip_rows=[1]\n",
    "        )\n",
    "    \n",
    "if current == \"braak\" or allin:\n",
    "    keep = {\"PET_TAU\": \"\", \"timepoint\": \"\", \"Analyse visuelle TG Braak\": \"stade_braak_visuel\", \n",
    "            \"Commentaire\": \"commentaire_analyse\", \"Certitude\": \"certitude_analyse_visuelle\", \n",
    "            \"commentaire_certitude\": \"commentaire_certitude\", \"Braak 1-2\": \"atteinte_braak_1_2\", \"Braak 3-4\": \"atteinte_braak_3_4\", \n",
    "            \"Braak 5-6\": \"atteinte_braak_5_6\", \"Global\": \"atteinte_total_neocortex\", \"SUVr Braak 1-2\": \"suvr_braak_1_2\", \n",
    "            \"SUVr Braak 3-4\": \"suvr_braak_3_4\", \"SUVr Braak 5-6\": \"suvr_braak_5_6\", \"SUVr Braak total\": \"suvr_total_neocortex\"}\n",
    "    path_to_excel='data\\csv\\csv\\TAU_Dg neuro complet_FINAL_JANV_2024.csv'\n",
    "    sql = \"select * from pet_tau_info\"\n",
    "    df_db = pd.read_sql(sql, con=mysql.connector.connect(**conn_new_db), coerce_float=True)  \n",
    "    df_pat = pd.read_sql(\"select * from patient\", con=mysql.connector.connect(**conn_new_db), coerce_float=False) \n",
    "    df_date = pd.read_csv(path_to_excel, delimiter=\";\") \n",
    "    for c in df_date.columns:\n",
    "        if pd.api.types.is_string_dtype(df_date[c].dtype):\n",
    "            df_date[c] = df_date[c].str.strip()\n",
    "    \n",
    "    df_date[\"Date_PET-TAU\"] = pd.to_datetime(df_date[\"Date_PET-TAU\"], format=\"%d/%m/%Y\").dt.date\n",
    "    df_date = df_date[[\"MEDEX\", \"Date_PET-TAU\"]]\n",
    "    df_date[\"MEDEX\"] = df_date[\"MEDEX\"].str.upper()\n",
    "    df_date.rename(columns={\"Date_PET-TAU\": \"date_pet_tau_info\"}, inplace=True)\n",
    "    df_date.dropna(subset=[\"date_pet_tau_info\"], inplace=True)\n",
    "    df_date = pd.merge(df_date, df_pat[[\"MEDEX\", \"PET_TAU\", \"id_patient\"]].copy(), on=\"MEDEX\", how=\"left\")\n",
    "    df_date[\"PET_TAU\"] = df_date[\"PET_TAU\"].astype(\"Int64\")\n",
    "    df_date[\"id_patient\"] = df_date[\"id_patient\"].astype(\"Int64\")\n",
    "    df_date.dropna(subset=[\"PET_TAU\"], inplace=True)\n",
    "     \n",
    "    df_atteinte = pd.read_csv('data\\csv\\csv\\Récap pourcentage atteinte Multiseuil.csv', delimiter=\";\")  \n",
    "    df_atteinte[\"timepoint\"] = 1\n",
    "    df_suvr = pd.read_csv('data\\csv\\csv\\SUVr Braak + total psf.csv', delimiter=\";\")   \n",
    "    df_suvr[\"timepoint\"] = 1\n",
    "    df_visuel = pd.read_csv('data\\csv\\csv\\Analyse visuelle.csv', delimiter=\";\")   \n",
    "    df_visuel[\"timepoint\"] = 1\n",
    "    \n",
    "    for index, row in df_atteinte.iterrows():\n",
    "        if \"T2\" in row[\"PET_TAU\"]:\n",
    "            df_atteinte.loc[index, 'timepoint'] = 2\n",
    "    for index, row in df_suvr.iterrows():\n",
    "        if \"T2\" in row[\"PET_TAU\"]:\n",
    "            df_suvr.loc[index, 'timepoint'] = 2\n",
    "    for index, row in df_visuel.iterrows():\n",
    "        if \"T2\" in row[\"PET_TAU\"]:\n",
    "            df_visuel.loc[index, 'timepoint'] = 2\n",
    "            \n",
    "    df_atteinte[\"PET_TAU\"] = df_atteinte[\"PET_TAU\"].map(lambda x: str(x).split('-')[0].split(\"P\")[-1] if not pd.isna(x) else x)\n",
    "    df_suvr[\"PET_TAU\"] = df_suvr[\"PET_TAU\"].map(lambda x: str(x).split('-')[0].split(\"P\")[-1] if not pd.isna(x) else x)\n",
    "    df_visuel[\"PET_TAU\"] = df_visuel[\"PET_TAU\"].map(lambda x: str(x).split('-')[0].split(\"P\")[-1] if not pd.isna(x) else x)\n",
    "    \n",
    "    final_df = pd.merge(df_atteinte, df_suvr, on=[\"PET_TAU\", \"timepoint\"], how=\"outer\")\n",
    "    final_df = pd.merge(final_df, df_visuel, on=[\"PET_TAU\", \"timepoint\"], how=\"outer\")\n",
    "    final_df = final_df[list(keep.keys())]\n",
    "    final_df[\"PET_TAU\"] = final_df[\"PET_TAU\"].astype(\"Int64\")\n",
    "    \n",
    "    final_df[\"Braak 1-2\"] = convert_coma(final_df[\"Braak 1-2\"]).astype(\"float\")\n",
    "    final_df[\"Braak 3-4\"] = convert_coma(final_df[\"Braak 3-4\"]).astype(\"float\")\n",
    "    final_df[\"Braak 5-6\"] = convert_coma(final_df[\"Braak 5-6\"]).astype(\"float\")\n",
    "    final_df[\"Global\"] = convert_coma(final_df[\"Global\"])\n",
    "    final_df[\"Braak 1-2\"] = final_df[\"Braak 1-2\"].astype(\"float\").astype(\"float\")\n",
    "    final_df[\"Braak 3-4\"] = final_df[\"Braak 3-4\"].astype(\"float\").astype(\"float\")\n",
    "    final_df[\"Braak 5-6\"] = final_df[\"Braak 5-6\"].astype(\"float\").astype(\"float\")\n",
    "    final_df[\"Global\"] = final_df[\"Global\"].astype(\"float\")\n",
    "    \n",
    "    final_df[\"SUVr Braak 1-2\"] = convert_coma(final_df[\"SUVr Braak 1-2\"]).astype(\"float\")\n",
    "    final_df[\"SUVr Braak 3-4\"] = convert_coma(final_df[\"SUVr Braak 3-4\"]).astype(\"float\")\n",
    "    final_df[\"SUVr Braak 5-6\"] = convert_coma(final_df[\"SUVr Braak 5-6\"]).astype(\"float\")\n",
    "    final_df[\"SUVr Braak total\"] = convert_coma(final_df[\"SUVr Braak total\"]).astype(\"float\")\n",
    "    final_df[\"SUVr Braak 1-2\"] = final_df[\"SUVr Braak 1-2\"].astype(\"float\")\n",
    "    final_df[\"SUVr Braak 3-4\"] = final_df[\"SUVr Braak 3-4\"].astype(\"float\")\n",
    "    final_df[\"SUVr Braak 5-6\"] = final_df[\"SUVr Braak 5-6\"].astype(\"float\")\n",
    "    final_df[\"SUVr Braak total\"] = final_df[\"SUVr Braak total\"].astype(\"float\")\n",
    "    \n",
    "    final_df[\"Analyse visuelle TG Braak\"] = final_df[\"Analyse visuelle TG Braak\"].astype(\"Int64\")\n",
    "    final_df[\"Certitude\"] = final_df[\"Certitude\"].map(lambda x: 1 if not pd.isna(x) and \"oui\" in x else (0 if not pd.isna(x) and \"non\" in x else np.nan))\n",
    "    final_df[\"Certitude\"] = final_df[\"Certitude\"].astype(\"Int64\")\n",
    "    del keep['PET_TAU']\n",
    "    del keep['timepoint']\n",
    "    final_df.dropna(subset=list(keep.keys()), inplace=True, how='all')\n",
    "    merged_df = pd.merge(df_date, final_df, on='PET_TAU')\n",
    "    \n",
    "    merged_df['date_pet_tau_info'] = pd.to_datetime(merged_df['date_pet_tau_info']).dt.date\n",
    "\n",
    "    grouped_df = merged_df.groupby('PET_TAU', group_keys=False).apply(adjust_dates)\n",
    "    df_excel = grouped_df.drop_duplicates(subset=[\"PET_TAU\", \"adjusted_date\", \"timepoint\"])\n",
    "    df_excel[\"date_pet_tau_info\"] = df_excel[\"adjusted_date\"].copy()\n",
    "    df_excel.reset_index(inplace=True)\n",
    "    df_excel.drop(columns=[\"index\", \"adjusted_date\"], inplace=True)\n",
    "    df_excel.rename(columns=keep, inplace=True)\n",
    "    print(\"ATTENTION: ces lignes sont des T2 mais n'ont pas de deuxième date enregistrées, elles ne seront donc pas ajoutées!\")\n",
    "    display(df_excel[df_excel.duplicated(subset=[\"PET_TAU\", \"date_pet_tau_info\"])])\n",
    "    df_excel.drop(list(df_excel[df_excel.duplicated(subset=[\"PET_TAU\", \"date_pet_tau_info\"])].index), inplace=True)\n",
    "\n",
    "    # on va scroll ligne par ligne dans l'excel\n",
    "    up = \"\"\n",
    "    for index, row in df_excel.iterrows():\n",
    "        r = df_db.loc[df_db[\"id_patient\"] == row[\"id_patient\"]]\n",
    "        r = r.loc[r[\"date_pet_tau_info\"] == row[\"date_pet_tau_info\"]]\n",
    "        condition = \"\\nWHERE \"\n",
    "        condition += \" id_patient = \\\"\"+str(row[\"id_patient\"])+\"\\\"\"\n",
    "        condition += \" and date_pet_tau_info = \\\"\"+str(row[\"date_pet_tau_info\"])+\"\\\"\"\n",
    "\n",
    "        if len(r) == 1:\n",
    "            up2add = \"UPDATE pet_tau_info \\nSET \"#address = %s WHERE address = %s\"\n",
    "            avant = \"\\n# BEFORE: \"\n",
    "            col2change = \"\"\n",
    "            for h in list(df_db.columns):\n",
    "                if \"date\" not in h and h != \"id_patient\":\n",
    "                    indb = r[h][r.index[0]]\n",
    "                    inexcel = row[h]\n",
    "                    if pd.isna(indb):\n",
    "                        if not pd.isna(inexcel):\n",
    "                            col2change += str(h)+\" = '\"+str(inexcel)+\"', \"\n",
    "                            avant += str(h)+\" = '\"+str(indb)+\"', \"\n",
    "                    elif not pd.isna(inexcel):\n",
    "                        if indb != inexcel:\n",
    "                            if isinstance(indb, float) and isinstance(inexcel, float):\n",
    "                                if abs(indb - inexcel) >= 0.0001:\n",
    "                                    col2change += str(h)+\" = '\"+str(inexcel)+\"', \"\n",
    "                                    avant += str(h)+\" = '\"+str(indb)+\"', \"\n",
    "                            else:\n",
    "                                col2change += str(h)+\" = '\"+str(inexcel)+\"', \"\n",
    "                                avant += str(h)+\" = '\"+str(indb)+\"', \"\n",
    "            if len(col2change) > 0:       \n",
    "                col2change = col2change[:-2]\n",
    "                up2add += col2change + avant + condition\n",
    "                up += up2add + \";\\n\\n\"\n",
    "            df_excel.drop(index, inplace=True)\n",
    "        if len(r) > 1:\n",
    "            print(\"\")\n",
    "            print(\"error\")\n",
    "            sys.exit()\n",
    "    if len(up) > 0:\n",
    "        with open(\"sql_code/update_pet_tau_info_from_analyse_suvr_recap.txt\", 'w') as f:\n",
    "            f.write(up)\n",
    "    df_excel.drop_duplicates(subset=[\"id_patient\", \"date_pet_tau_info\"], keep='first', inplace=True)\n",
    "    if len(df_excel) > 0:\n",
    "        print(\"we will add:\\nin: PET TAU INFO\")\n",
    "        colonne = list(keep.values())\n",
    "        colonne.append(\"date_pet_tau_info\")\n",
    "        colonne.append(\"id_patient\")\n",
    "        display(df_excel[colonne])\n",
    "        if input(\"ajouter les valeur manquantes de PET TAU INFO à la db?\") == \"y\":\n",
    "            df_excel[colonne].to_sql(name=table, con=engine, if_exists=\"append\", index=False)  \n",
    "            print(\"les données ont bien été ajoutées!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf2f3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee8bcdd",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# on supprime toutes les tables avec irm devant avant\n",
    "\n",
    "request = \"SELECT concat('TRUNCATE TABLE `', TABLE_NAME, '`;') FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = 'new_db'\"\n",
    "\n",
    "with mysql.connector.connect(**conn_new_db) as db :\n",
    "    with db.cursor() as c:\n",
    "        c.execute(request)\n",
    "        r = c.fetchall()\n",
    "        col = [i[0] for i in c.description]\n",
    "        new_r = \"\\n\".join(list(zip(*r))[0])\n",
    "        print(new_r)\n",
    "        c2 = MySQLCursor(db)\n",
    "        c2.execute(new_r)\n",
    "        db.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4185088",
   "metadata": {},
   "source": [
    "# STEP 1: push data from old DB to new DB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccdcdb4-b557-44b7-84fa-2ae9e35ae2d4",
   "metadata": {},
   "source": [
    "### 1. transfer info from patients "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bc98e4-9e8f-461d-9874-87073e6f11a0",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "##TODO: check for the education column if needed and if ranked\n",
    "\n",
    "request = \"select * from names n\"\n",
    "\n",
    "\n",
    "## Get the info we already have in db\n",
    "\n",
    "with mysql.connector.connect(**conn_datassistant) as db :\n",
    "    with db.cursor() as c:\n",
    "        c.execute(request)\n",
    "        resultats_dataassistant = c.fetchall()\n",
    "        col_dataassistant = [i[0] for i in c.description]\n",
    "        print(col_dataassistant)\n",
    "        c.execute(\"select * from demographics d\")\n",
    "        r = c.fetchall()\n",
    "        col = [i[0] for i in c.description]\n",
    "        print(col)\n",
    "        print(set(col).intersection(set(col_dataassistant)))\n",
    "\n",
    "df1 = pd.DataFrame(r, columns=col)\n",
    "df2 = pd.DataFrame(resultats_dataassistant, columns=col_dataassistant)\n",
    "print(\"shape of df1 before: \"+str(df1.shape))\n",
    "print(\"shape of df2 before: \"+str(df2.shape))\n",
    "df = pd.merge(df1, df2, on=[\"id\"], how=\"outer\")\n",
    "print(\"shape of df after: \"+str(df.shape))\n",
    "df.drop(columns=[\"E4_Status\", \"MaritalStatus\", \"Education\", \"AppleGameID\", \"Job\", \"DOB_x\"], inplace=True)\n",
    "\n",
    "df.rename(columns={\"LastName\": \"last_name\", \"FirstName\": \"first_name\", \"Contact\": \"mail\", \"APOE_Genotype\": \"APOE\", \n",
    "                   \"id\": \"id_patient\", \"UCL_2\": \"UCL2\", \"TAU_PET_ID\": \"PET_TAU\", \"BioMarqueursID\": \"BM\", \"Gender\": \"sexe\", \n",
    "                   \"Phone\": \"phone\", \"Phone2\": \"attendant\", \"DOB_y\": \"DOB\"}, inplace=True)\n",
    "\n",
    "df.insert(2, \"UCL_AD\", np.nan)\n",
    "df.insert(4, \"GABA\", np.nan)\n",
    "df.insert(6, \"origin\", np.nan)\n",
    "df.insert(len(df.columns), \"recontact_post_study\", np.nan)\n",
    "df['last_name'] = df['last_name'].str.upper()\n",
    "df['first_name'] = df['first_name'].str.lower()\n",
    "df['MEDEX'] = df['MEDEX'].str.upper()\n",
    "df[\"UCL2\"] = df[\"UCL2\"].astype(\"Int64\")\n",
    "df[\"PET_TAU\"] = df[\"PET_TAU\"].astype(\"Int64\")\n",
    "df[\"BM\"] = df[\"BM\"].astype(\"Int64\")\n",
    "df[\"UCL_AD\"] = df[\"UCL_AD\"].astype(\"Int64\")\n",
    "df[\"GABA\"] = df[\"GABA\"].astype(\"Int64\")\n",
    "print(\"\\n FINAL LOOK\")\n",
    "display(df)\n",
    "if input(\"want to add the data? [y/n]\") == \"y\":\n",
    "    df.to_sql(name=\"patient\", con=engine, if_exists=\"append\", index=False)\n",
    "    print(\"DATA WERE ADDED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b8fbbc-e3f7-4cd3-9328-cc55ddbffc5e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### 2. transfer info from csf (ponction lombaire) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd6923d-fe2c-4cfc-902b-b927ec6d65c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "request = \"select * from csf_peptide\"\n",
    "\n",
    "## Get the info we already have in db\n",
    "\n",
    "with mysql.connector.connect(**conn_datassistant) as db :\n",
    "    with db.cursor() as c:\n",
    "        c.execute(request)\n",
    "        r = c.fetchall()\n",
    "        col = [i[0] for i in c.description]\n",
    "        print(col)\n",
    "\n",
    "df = pd.DataFrame(r, columns=col)\n",
    "df.rename(columns={\"id\": \"id_patient\", \"Date_CSF_Peptide\": \"date_lcr\", \"Ab42_CSF_Peptide\": \"ab42\", \n",
    "                   \"Tau_total_CSF_Peptide\": \"tau_total\", \"P_Tau_CSF_Peptide\": \"p_tau\"}, inplace=True)\n",
    "\n",
    "print(\"\\n FINAL LOOK\")\n",
    "display(df)\n",
    "if input(\"want to add the data in analyse_lcr? [y/n]\") == \"y\":\n",
    "    df.to_sql(name=\"analyse_lcr\", con=engine, if_exists=\"append\", index=False)\n",
    "    print(\"DATA WERE ADDED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90896cab-5390-49a1-b1d2-893b2a7a32e8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### 3. transfer info from analyse quantitative et visuelle pour braak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95720212-b146-47ae-9813-4830ec0a641c",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_v = \"select * from analyse_quantitative\"\n",
    "request_q = \"select * from analyse_visuelle\"\n",
    "\n",
    "\n",
    "## Get the info we already have in db\n",
    "\n",
    "with mysql.connector.connect(**conn_datassistant) as db :\n",
    "    with db.cursor() as c:\n",
    "        c.execute(request_v)\n",
    "        rv = c.fetchall()\n",
    "        colv = [i[0] for i in c.description]\n",
    "        print(colv)\n",
    "        c.execute(request_q)\n",
    "        rq = c.fetchall()\n",
    "        colq = [i[0] for i in c.description]\n",
    "        print(colq)\n",
    "\n",
    "dfv = pd.DataFrame(rv, columns=colv)\n",
    "dfq = pd.DataFrame(rq, columns=colq)\n",
    "\n",
    "new_df = pd.merge(dfq, dfv, on='id', how=\"outer\")\n",
    "print(\"before dropping column \\\"Date_Analyse_quantitative\\\", is the other date the same?\")\n",
    "display(new_df)\n",
    "new_df.drop(columns=\"Date_Analyse_quantitative\", inplace=True)\n",
    "new_df.rename(columns={\"id\": \"id_patient\", \"Date_Analyse_Visuelle\": \"date_pet_tau_info\", \"Stade_braak_visuel\": \"stade_braak_visuel\", \n",
    "                   \"Certitude_Analyse_Visuelle\": \"certitude_analyse_visuelle\", \"Certitude_commentaire_Analyse_Visuelle\": \"commentaire_certitude\", \n",
    "                       \"Commentaire_Analyse_Visuelle\": \"commentaire_analyse\", \"Atteinte_en_region_Braak_1_2\": \"atteinte_braak_1_2\", \n",
    "                       \"Atteinte_en_region_Braak_3_4\": \"atteinte_braak_3_4\", \"Atteinte_en_region_Braak_5_6\": \"atteinte_braak_5_6\", \n",
    "                       \"Atteinte_ensemble_neocortex\": \"atteinte_total_neocortex\", \"SUVr_Braak_1_2\": \"suvr_braak_1_2\", \n",
    "                       \"SUVr_Braak_3_4\": \"suvr_braak_3_4\", \"SUVr_Braak_5_6\": \"suvr_braak_5_6\", \"SUVr_neocortex_total\": \"suvr_total_neocortex\"}, inplace=True)\n",
    "\n",
    "new_df[\"stade_braak_visuel\"] = new_df[\"stade_braak_visuel\"].astype(\"Int64\")\n",
    "new_df[\"certitude_analyse_visuelle\"] = new_df[\"certitude_analyse_visuelle\"].astype(\"Int64\")\n",
    "print(\"\\n FINAL LOOK\")\n",
    "display(new_df)\n",
    "if input(\"want to add the data? [y/n]\") == \"y\":\n",
    "    new_df.to_sql(name=\"pet_tau_info\", con=engine, if_exists=\"append\", index=False)\n",
    "    print(\"DATA WERE ADDED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f405f365-0487-4eca-a4a1-767f02697358",
   "metadata": {},
   "source": [
    "### 4. transfer info from plasma_peptide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8207e8f5-d92d-481f-ac0c-daf433e9cbde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6181361",
   "metadata": {},
   "source": [
    "### 5. get the regions from each stat files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c44b15",
   "metadata": {},
   "source": [
    "we only keep the following atlas:  <br>\n",
    "        $\\;\\;\\;\\;\\;\\;$ amygdalar <br>\n",
    "        $\\;\\;\\;\\;\\;\\;$ hipposubfields <br>\n",
    "        $\\;\\;\\;\\;\\;\\;$ ba_exvivo <br>\n",
    "        $\\;\\;\\;\\;\\;\\;$ a2009s <br>\n",
    "        $\\;\\;\\;\\;\\;\\;$ DKTatlas <br>\n",
    "        $\\;\\;\\;\\;\\;\\;$ aparc.stats <br>\n",
    "        $\\;\\;\\;\\;\\;\\;$ wmparc.stats <br>\n",
    "        $\\;\\;\\;\\;\\;\\;$ aseg.stats <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd8673d",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# This part of code will get \n",
    "dataInFiles = {}\n",
    "pattern = re.compile(\"^\\s+|\\s*,\\s*|\\s+|\\s*$\")\n",
    "\n",
    "for file in os.listdir(path_irm+\"720/LISE_UCL2_4/stats/\"):\n",
    "    accessName = path_irm+\"720/LISE_UCL2_4/stats/\" + file\n",
    "    accessN = path_irm+\"720/LISE_UCL2_4/stats/\" + file\n",
    "    if file.endswith(\".stats\") and (file == \"aseg.stats\" or\n",
    "                                    file == \"wmparc.stats\" or\n",
    "                                    file.__contains__(\"a2009s\") or\n",
    "                                    file.__contains__(\"DKTatlas\") or\n",
    "                                    file.__contains__(\"aparc.stats\") or\n",
    "                                    file.__contains__(\"BA_exvivo.stats\")):\n",
    "        file = file.lower().replace(\".\", \"_\")\n",
    "        dataInFiles[file] = []\n",
    "\n",
    "        # pour plus tard.... namefile = file.split()\n",
    "        f = open(accessName, 'r')\n",
    "        Lines = f.readlines()\n",
    "\n",
    "        mesure = []\n",
    "        colnames = []\n",
    "        # On prend d'abord les mesures dans le # et les noms de colonnes\n",
    "        for line in Lines:\n",
    "            words = [x for x in pattern.split(line) if x]\n",
    "            if len(words) > 1:\n",
    "                if words[1] == \"Measure\" and file == \"aseg.stats\":\n",
    "                    mesure.append(words[2])\n",
    "                elif words[1] == \"ColHeaders\":\n",
    "                    colnames = words[2:].copy()\n",
    "                    break\n",
    "        f.close()\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(accessName, header=None, names=colnames, comment='#', delim_whitespace=True)\n",
    "        except:\n",
    "            df = pd.DataFrame()\n",
    "        dataInFiles[file].append(mesure.copy())\n",
    "        dataInFiles[file].append(list(df.StructName.copy()))\n",
    "        \n",
    "    if (file.__contains__(\"amygdalar\") or file.__contains__(\"hipposubfields\")) and file.endswith(\".stats\"):\n",
    "        file = file.lower().replace(\".\", \"_\")\n",
    "        dataInFiles[file] = []\n",
    "        dataInFiles[file].append([])\n",
    "        try:\n",
    "            df = pd.read_csv(accessName, header=None, comment='#', delim_whitespace=True)\n",
    "        except:\n",
    "            df = pd.DataFrame()\n",
    "\n",
    "        dataInFiles[file].append(list(df[df.columns[len(df.columns)-1]].copy()))\n",
    "        \n",
    "for h in dataInFiles:\n",
    "    print(\"\\n\")\n",
    "    print(h)\n",
    "    print(dataInFiles[h])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40015ce",
   "metadata": {},
   "source": [
    "### 5. push IRM data from db to new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bb6fdb",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# INSERT DATA IN TABLE IRM\n",
    "\n",
    "paral = {\"lh_aparc_a2009s_stats\": \"destrieux\", \"lh_aparc_dktatlas_stats\": \"dkt\", \"lh_aparc_stats\": \"desikan_killiani\", \n",
    "         \"rh_aparc_a2009s_stats\": \"destrieux\", \"rh_aparc_dktatlas_stats\": \"dkt\", \"rh_aparc_stats\": \"desikan_killiani\", \n",
    "         \"lh_ba_exvivo_stats\": \"ba_exvivo\", \"rh_ba_exvivo_stats\": \"ba_exvivo\", \"aseg_stats\": \"segsouscort\", \n",
    "         \"wmparc_stats\": \"wmparc\"}\n",
    "measures = {\"lh_aparc_a2009s_stats\": [\"grayvol\", \"thickavg\", \"surfarea\"], \n",
    "            \"lh_aparc_dktatlas_stats\": [\"grayvol\", \"thickavg\", \"surfarea\"], \n",
    "            \"lh_aparc_stats\": [\"grayvol\", \"thickavg\", \"surfarea\"], \n",
    "            \"rh_aparc_a2009s_stats\": [\"grayvol\", \"thickavg\", \"surfarea\"], \n",
    "            \"rh_aparc_dktatlas_stats\": [\"grayvol\", \"thickavg\", \"surfarea\"], \n",
    "            \"rh_aparc_stats\": [\"grayvol\", \"thickavg\", \"surfarea\"], \n",
    "            \"lh_ba_exvivo_stats\": [\"grayvol\", \"thickavg\", \"surfarea\"], \n",
    "            \"rh_ba_exvivo_stats\": [\"grayvol\", \"thickavg\", \"surfarea\"], \n",
    "            \"aseg_stats\": [\"vol_mm3\"], \n",
    "            \"wmparc_stats\": [\"vol_mm3\"]}\n",
    "        \n",
    "        \n",
    "db_datass = mysql.connector.connect(**conn_datassistant)\n",
    "db_new = mysql.connector.connect(**conn_new_db)\n",
    "data = {}\n",
    "\n",
    "print(dataInFiles.keys())\n",
    "version = [\"6.0.0\", \"7.2.0\"]\n",
    "\n",
    "c_datass = db_datass.cursor()\n",
    "for h in paralo:\n",
    "    print(\"we doing \"+h)\n",
    "    request = \"select * from \"+h\n",
    "    try:\n",
    "        c_datass.execute(request)\n",
    "        res = c_datass.fetchall()\n",
    "        colnames = [i[0].lower().split(\"_\"+h)[0] for i in c_datass.description]\n",
    "        # Dataframe de ce qu'on a en db \n",
    "        df = pd.DataFrame(res, columns=colnames).copy()\n",
    "        key = [key for key in dataInFiles if h in key.lower()]\n",
    "        print(key)\n",
    "\n",
    "        # here we add the values\n",
    "        for m in measures[h]:\n",
    "            for v in version:\n",
    "                df_ass = df[df['version'].str.contains(v, na=False)].copy()\n",
    "                col = [\"id\", \"date\"]\n",
    "                col.extend([i for i in df if i.startswith(m)])\n",
    "                exp_regions = set([x.lower().replace(\"-\", \"_\").replace(\"&\", \"_and_\") for x in dataInFiles[key[0]][1]])\n",
    "                \n",
    "                df_new = df_ass[col].copy()\n",
    "                df_new.columns = df_new.columns.str.replace(m+\"_\", '')\n",
    "                if h.startswith(\"lh\"):\n",
    "                    df_new[[\"hemispher\"]] = \"L\"\n",
    "                elif h.startswith(\"rh\"):\n",
    "                    df_new[[\"hemispher\"]] = \"R\"\n",
    "                df_new.rename(columns={\"id\": \"id_patient\", \"date\": \"date_irm\"}, inplace=True)\n",
    "                if len(set(df_new.columns).symmetric_difference(exp_regions)) == 3 or (len(set(df_new.columns).symmetric_difference(exp_regions)) == 2 and m == \"vol_mm3\"):\n",
    "                    display(df_new)\n",
    "                    print(\"measure: \"+m)\n",
    "                    print(\"with \"+h+\" version \"+v)\n",
    "                    if len(df_new) >0:\n",
    "                        print(\"add to table: \"+\"irm_\"+paral[h]+\"_v\"+v.replace(\".\", \"\")+\"_\"+m)\n",
    "                        if input(\"want to add the data? [y/n]\") == \"y\":\n",
    "                            df_new.to_sql(name=\"irm_\"+paral[h]+\"_v\"+v.replace(\".\", \"\")+\"_\"+m, con=engine, if_exists=\"append\", index=False)\n",
    "                            print(\"DATA WERE ADDED\")\n",
    "                else:\n",
    "                    print(\"THERE HAS BEEN AN ERROR\") \n",
    "                    print(\"measure: \"+m)\n",
    "                    print(\"with \"+h)\n",
    "                    print(len(set(df_new.columns).symmetric_difference(exp_regions)))\n",
    "                    print(set(df_new.columns).symmetric_difference(exp_regions))\n",
    "                    display(df_new)\n",
    "                    print(exp_regions)\n",
    "                    sys.exit()\n",
    "    except Exception  as error:\n",
    "        print(error)\n",
    "        display(df_new)\n",
    "        c_datass.close()\n",
    "        db_datass.close()\n",
    "        db_new.close()\n",
    "        sys.exit()\n",
    "db_datass.close()\n",
    "db_new.close()\n",
    "c_datass.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bfed98-087f-439b-8ca5-3158d8f16aa0",
   "metadata": {},
   "source": [
    "### 6. transfer info for psychology test\n",
    "\n",
    "we keep: <br>\n",
    "$\\;\\;\\;\\;\\;\\;$ $\\;\\;\\;\\;\\;\\;$cerad_figure_copy, cerad_figure_copy_z, cerad_figure_copy_z_parameter <br>\n",
    "$\\;\\;\\;\\;\\;\\;$ $\\;\\;\\;\\;\\;\\;$clock, clock_z, clock_parameter, <br>\n",
    "$\\;\\;\\;\\;\\;\\;$ $\\;\\;\\;\\;\\;\\;$fcsrt, fcsrt_parameter, fcsrt_z, <br>\n",
    "$\\;\\;\\;\\;\\;\\;$ $\\;\\;\\;\\;\\;\\;$fluency, fluency_parameter, fluency_z, <br>\n",
    "$\\;\\;\\;\\;\\;\\;$ $\\;\\;\\;\\;\\;\\;$logical_memory, <br>\n",
    "$\\;\\;\\;\\;\\;\\;$ $\\;\\;\\;\\;\\;\\;$luria, luria_parameter, luria_z, <br>\n",
    "$\\;\\;\\;\\;\\;\\;$ $\\;\\;\\;\\;\\;\\;$mmse, <br>\n",
    "$\\;\\;\\;\\;\\;\\;$ $\\;\\;\\;\\;\\;\\;$naming, naming_lexis_parameter, naming_lexis_z, <br>\n",
    "$\\;\\;\\;\\;\\;\\;$ $\\;\\;\\;\\;\\;\\;$shapes, <br>\n",
    "$\\;\\;\\;\\;\\;\\;$ $\\;\\;\\;\\;\\;\\;$trail_making_test, trail_making_test_parameter, trail_making_test_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f33cb5b-53cb-4e02-b71e-2a191926d651",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tables = {'clock': 'neuro_clock', \n",
    "          'clock_parameter': 'neuro_clock_parameter', \n",
    "          'clock_z': 'neuro_clock_z', \n",
    "          'luria': 'neuro_luria', \n",
    "          'luria_parameter': 'neuro_luria_parameter', \n",
    "          'luria_z': 'neuro_luria_z', \n",
    "          'fcsrt': 'neuro_fcsrt', \n",
    "          'fcsrt_parameter': 'neuro_fcsrt_parameter', \n",
    "          'fcsrt_z': 'neuro_fcsrt_z', \n",
    "          'fluency': 'neuro_fluency',\n",
    "          'fluency_parameter': 'neuro_fluency_parameter',\n",
    "          'fluency_z': 'neuro_fluency_z', \n",
    "          'naming': 'neuro_naming', \n",
    "          'naming_lexis_parameter': 'neuro_naming_lexis_parameter', \n",
    "          'naming_lexis_z': 'neuro_naming_lexis_z',\n",
    "          'cerad_figure_copy': 'neuro_cerad_figure_copy', \n",
    "          'cerad_figure_copy_z_parameter': 'neuro_cerad_figure_copy_parameter',\n",
    "          'cerad_figure_copy_z': 'neuro_cerad_figure_copy_z',  \n",
    "          'trail_making_test': 'neuro_trail_making_test', \n",
    "          'trail_making_test_parameter': 'neuro_trail_making_test_parameter', \n",
    "          'trail_making_test_z': 'neuro_trail_making_test_z', \n",
    "          'logical_memory': 'neuro_logical_memory',\n",
    "          'composite': 'neuro_composite',\n",
    "          'mmse': 'neuro_mmse', \n",
    "          'shapes': 'neuro_shapes', \n",
    "          'code': 'neuro_code'}\n",
    "\n",
    "for h_db in tables:\n",
    "    df_db = pd.read_sql(\"select * from \"+h_db, con=mysql.connector.connect(**conn_datassistant), coerce_float=False) \n",
    "    df_new = pd.read_sql(\"select * from \"+tables[h_db], con=mysql.connector.connect(**conn_new_db), coerce_float=False)\n",
    "    print(\"\\ncheck for table \"+h_db)\n",
    "    print(\"before:\")\n",
    "    display(df_db)\n",
    "    \n",
    "    list2change = df_db.columns\n",
    "    list2change = list2change.str.lower()\n",
    "    list2change = list(list2change)\n",
    "    # j enleve le pattern du nom de table si ça commence par le nom de table ou que ça finit ou que ça commence sans _ \n",
    "    # ou a l envers\n",
    "    for i in range(len(list2change)):\n",
    "        if list2change[i] != h_db:\n",
    "            if list2change[i].startswith(h_db) or list2change[i].startswith(\"\".join(h_db.split(\"_\"))):\n",
    "                list2change[i] = \"\".join(list2change[i].split(h_db+\"_\"))\n",
    "                list2change[i] = \"\".join(list2change[i].split(\"\".join(h_db.split(\"_\"))))\n",
    "            elif list2change[i].startswith(\"_\".join(reversed(h_db.split(\"_\")))):\n",
    "                list2change[i] = \"\".join(list2change[i].split(\"_\".join(reversed(h_db.split(\"_\")))+\"_\"))\n",
    "                list2change[i] = \"\".join(list2change[i].split(\"_\"+ \"_\".join(reversed(h_db.split(\"_\")))))\n",
    "            else:\n",
    "                list2change[i] = \"\".join(list2change[i].split(\"_\" + h_db))\n",
    "                list2change[i] = \"\".join(list2change[i].split(\"\".join(h_db.split(\"_\"))))\n",
    "                list2change[i] = \"\".join(list2change[i].split(\"_\" + h_db.split(\"_\")[0]))\n",
    "                \n",
    "            if list2change[i] == \"id\":\n",
    "                list2change[i] = \"id_patient\"\n",
    "            if list2change[i] == \"date\":\n",
    "                list2change[i] = \"date_\"+h_db\n",
    "            if \"standard_deviation\" in list2change[i]:\n",
    "                list2change[i] = \"sd_\"+\"\".join(list2change[i].split(\"standard_deviation_\"))\n",
    "            if \"deviation\" in list2change[i]:\n",
    "                list2change[i] = \"\".join(list2change[i].split(\"deviation_\"))\n",
    "        \n",
    "    if len(set(list2change).intersection(set(df_new))) != len(df_new.columns):\n",
    "        new_2match = set(df_new).difference(set(list2change))\n",
    "        db_2match = set(list2change).difference(set(df_new))\n",
    "        for i_new in new_2match:\n",
    "            for i_db in db_2match:\n",
    "                if i_new in i_db:\n",
    "                    list2change[list2change.index(i_db)] = i_new\n",
    "                    break\n",
    "        new_2match = set(df_new).difference(set(list2change))\n",
    "        db_2match = set(list2change).difference(set(df_new))\n",
    "    df_db.columns = list2change\n",
    "    print(\"\\nafter:\")\n",
    "    display(df_db)\n",
    "    display(df_new)\n",
    "    print(len(set(df_db).intersection(set(df_new))))\n",
    "    print(len(df_new.columns))\n",
    "    \n",
    "    if len(set(list2change).intersection(set(df_new))) != len(df_new.columns):\n",
    "        print(\"error\")\n",
    "        print(\"here\")\n",
    "        new_2match = set(df_new).difference(set(list2change))\n",
    "        db_2match = set(list2change).difference(set(df_new))\n",
    "        print(new_2match)\n",
    "        print(db_2match)\n",
    "        sys.exit()\n",
    "        \n",
    "    print(\"gonna add:\")\n",
    "    display(df_db[list(df_new.columns)])\n",
    "    if input(\"can i add it for table \"+tables[h_db]+\"?\\n\") == \"y\":\n",
    "        df_db[list(df_new.columns)].to_sql(name=tables[h_db], con=engine, if_exists=\"append\", index=False)\n",
    "        clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b934f129",
   "metadata": {},
   "source": [
    "# STEP 2: push data from MRI to new DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed769216",
   "metadata": {},
   "outputs": [],
   "source": [
    "paral = {\"lh_aparc_a2009s_stats\": \"destrieux\", \"lh_aparc_dktatlas_stats\": \"dkt\", \"lh_aparc_stats\": \"desikan_killiani\", \n",
    "         \"rh_aparc_a2009s_stats\": \"destrieux\", \"rh_aparc_dktatlas_stats\": \"dkt\", \"rh_aparc_stats\": \"desikan_killiani\", \n",
    "         \"lh_ba_exvivo_stats\": \"ba_exvivo\", \"rh_ba_exvivo_stats\": \"ba_exvivo\", \"aseg_stats\": \"segsouscort\", \n",
    "         \"wmparc_stats\": \"wmparc\"}\n",
    "measures = {\"lh_aparc_a2009s_stats\": [\"GrayVol\", \"ThickAvg\", \"SurfArea\"], \n",
    "            \"lh_aparc_dktatlas_stats\": [\"GrayVol\", \"ThickAvg\", \"SurfArea\"], \n",
    "            \"lh_aparc_stats\": [\"GrayVol\", \"ThickAvg\", \"SurfArea\"], \n",
    "            \"rh_aparc_a2009s_stats\": [\"GrayVol\", \"ThickAvg\", \"SurfArea\"], \n",
    "            \"rh_aparc_dktatlas_stats\": [\"GrayVol\", \"ThickAvg\", \"SurfArea\"], \n",
    "            \"rh_aparc_stats\": [\"GrayVol\", \"ThickAvg\", \"SurfArea\"], \n",
    "            \"lh_ba_exvivo_stats\": [\"GrayVol\", \"ThickAvg\", \"SurfArea\"], \n",
    "            \"rh_ba_exvivo_stats\": [\"GrayVol\", \"ThickAvg\", \"SurfArea\"], \n",
    "            \"aseg_stats\": [\"vol\"], \n",
    "            \"wmparc_stats\": [\"vol\"]}\n",
    "\n",
    "\n",
    "def adjust_dates(group):\n",
    "    if len(group) == 1:\n",
    "        group[\"adjusted_date\"] = group[\"date\"]\n",
    "        return group\n",
    "    else:\n",
    "        try:\n",
    "            sorted_group = group.sort_values('date')\n",
    "            sorted_date = sorted(list(dict.fromkeys(list(group[\"date\"]))))\n",
    "            sorted_time = sorted(list(dict.fromkeys(list(group[\"timepoint\"]))))\n",
    "            min_date = sorted_group['date'].min()\n",
    "            max_date = sorted_group['date'].max()\n",
    "            timepoint_mapping = {}\n",
    "            for ind, timepoint in enumerate(sorted_time):\n",
    "                if ind < len(sorted_date):\n",
    "                    timepoint_mapping[timepoint] = sorted_date[ind]\n",
    "                else:\n",
    "                    timepoint_mapping[timepoint] = sorted_date[0]\n",
    "            sorted_group['adjusted_date'] = sorted_group['timepoint'].map(timepoint_mapping)\n",
    "            return sorted_group\n",
    "        except:\n",
    "            print(range(0, max(group['timepoint']) + 1))\n",
    "            display(group)\n",
    "            raise Exception(\"problem with this group \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0397f0-2f79-4fc9-9b39-abe002dbd191",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./data/zirm_pc\"\n",
    "# ATTENTION A LA DATE \n",
    "###### ATTENTION: dans LISE_UCL2_118, rh.aparc.DKTatlas, il a y une structure en plus \"temporalpole\"\n",
    "    \n",
    "file2check = {\"aseg.stats\": \"segsouscort\", \n",
    "              \"wmparc.stats\": \"wmparc\", \n",
    "              \"a2009s\": \"destrieux\",\n",
    "              \"DKTatlas\": \"dkt\", \n",
    "              \"aparc.stats\": \"desikan_killiani\", \n",
    "              \"BA_exvivo.stats\": \"ba_exvivo\", \n",
    "              \"amygdalar\": \"amygdala\", \n",
    "              \"hipposubfields\": \"hippo\"} \n",
    "pattern = re.compile(\"^\\s+|\\s*,\\s*|\\s+|\\s*$\")\n",
    "\n",
    "date_tau = pd.read_csv('data\\csv\\csv\\TAU_Dg neuro complet_FINAL_JANV_2024.csv', delimiter=\";\")\n",
    "date_tau = date_tau[[\"ID\", \"date IRM\"]]\n",
    "date_tau.rename(columns={\"ID\": \"PET_TAU\", \"date IRM\": \"date\"}, inplace=True)\n",
    "date_tau.dropna(subset=[\"PET_TAU\", \"date\"], inplace=True)\n",
    "date_tau[\"PET_TAU\"] = date_tau.PET_TAU.astype(\"Int64\")\n",
    "date_tau[\"date\"] = pd.to_datetime(date_tau[\"date\"], format=\"%d/%m/%Y\").dt.date\n",
    "\n",
    "date_ucl2 = pd.read_csv('data\\zrandom\\csv\\participants_MRI.csv', delimiter=\";\")\n",
    "date_ucl2 = date_ucl2[[\"Num_UCL2\", \"Date_IRM\"]]\n",
    "date_ucl2.rename(columns={\"Num_UCL2\": \"UCL2\", \"Date_IRM\": \"date\"}, inplace=True)\n",
    "date_ucl2[\"UCL2\"] = date_ucl2['UCL2'].replace(\"UCL2_\", \"\", regex=True)\n",
    "date_ucl2.dropna(subset=[\"UCL2\", \"date\"], inplace=True)\n",
    "date_ucl2[\"UCL2\"] = date_ucl2.UCL2.astype(\"Int64\")\n",
    "date_ucl2[\"date\"] = pd.to_datetime(date_ucl2[\"date\"], format=\"%d/%m/%Y\").dt.date\n",
    "\n",
    "\n",
    "irm_ucl2 = {'UCL2': [], \"path\": [], \"timepoint\": [], \"version\": []}\n",
    "irm_tau = {'PET_TAU': [], \"path\": [], \"timepoint\": [], \"version\": []}\n",
    "for version in os.listdir(path):\n",
    "    for id_pat in os.listdir(path+\"/\" + version):\n",
    "        s = id_pat.split(\"_\")\n",
    "        time = True\n",
    "        stat = \"\"\n",
    "        t = 0\n",
    "        for ind, el in enumerate(s):\n",
    "            if re.match(\"[0-9]+\", el):\n",
    "                if s[ind-1] == \"UCL2\":\n",
    "                    irm_ucl2[\"UCL2\"].append(int(el)) \n",
    "                    irm_ucl2[\"version\"].append(version) \n",
    "                    irm_ucl2['path'].append(path+\"/\"+version+\"/\"+id_pat)\n",
    "                    stat = \"ucl2\"\n",
    "                elif s[ind-1] == \"TAU\":\n",
    "                    irm_tau[\"PET_TAU\"].append(int(el)) \n",
    "                    irm_tau[\"version\"].append(version) \n",
    "                    irm_tau['path'].append(path+\"/\"+version+\"/\"+id_pat)\n",
    "                    stat = \"pet\"\n",
    "            elif re.match(\"T[0-9]+\", el) and time:\n",
    "                t=el.split(\"T\")[-1]\n",
    "                time = False\n",
    "        if stat == \"pet\":\n",
    "            irm_tau[\"timepoint\"].append(t) \n",
    "        elif stat == \"ucl2\":\n",
    "            irm_ucl2[\"timepoint\"].append(t) \n",
    "        else:\n",
    "            print(id_pat)\n",
    "\n",
    "sql = \"select * from patient\"\n",
    "df_db = pd.read_sql(sql, con=mysql.connector.connect(**conn_new_db), coerce_float=False)\n",
    "df_db[\"id_patient\"] = df_db.id_patient.astype(\"int\")\n",
    "df_db[\"UCL2\"] = df_db.UCL2.astype(\"Int64\")\n",
    "df_db[\"PET_TAU\"] = df_db.PET_TAU.astype(\"Int64\")\n",
    "\n",
    "df_ucl2 = pd.DataFrame.from_dict(irm_ucl2)\n",
    "df_ucl2[\"timepoint\"] = df_ucl2[\"timepoint\"].astype(\"int\")\n",
    "df_ucl2 = pd.merge(df_ucl2, df_db[[\"UCL2\", \"id_patient\"]].copy().dropna(), on=\"UCL2\", how=\"left\")\n",
    "df_ucl2 = pd.merge(df_ucl2, date_ucl2, on=\"UCL2\", how=\"left\")\n",
    "\n",
    "df_tau = pd.DataFrame.from_dict(irm_tau)\n",
    "df_tau[\"timepoint\"] = df_tau[\"timepoint\"].astype(\"int\")\n",
    "df_tau = pd.merge(df_tau, df_db[[\"PET_TAU\", \"id_patient\"]].copy().dropna(), on=\"PET_TAU\", how=\"left\")\n",
    "df_tau = pd.merge(df_tau, date_tau, on=\"PET_TAU\", how=\"left\")\n",
    "\n",
    "print(\"ATTENTION: les prochaines lignes n'ont pas de dates associées, elles seront supprimées\")\n",
    "display(df_tau.loc[df_tau[\"date\"].isnull()])\n",
    "df_tau.drop(list(df_tau.loc[df_tau[\"date\"].isnull()].index), inplace=True)\n",
    "\n",
    "grouped_df = df_ucl2.groupby('UCL2', group_keys=False).apply(adjust_dates)\n",
    "df_ucl2 = grouped_df.drop_duplicates(subset=[\"UCL2\", \"adjusted_date\", \"timepoint\"])\n",
    "df_ucl2[\"date\"] = df_ucl2[\"adjusted_date\"].copy()\n",
    "df_ucl2.reset_index(inplace=True)\n",
    "df_ucl2.drop(columns=[\"index\", \"adjusted_date\"], inplace=True)\n",
    "print(\"ATTENTION: ces lignes sont des suivis mais pas de deuxiemes dates enregistrées pour UCL2, elles ne seront donc pas ajoutées!\")\n",
    "display(df_ucl2[df_ucl2.duplicated(subset=[\"UCL2\", \"date\"])])\n",
    "df_ucl2.drop(list(df_ucl2[df_ucl2.duplicated(subset=[\"UCL2\", \"date\"])].index), inplace=True)\n",
    "df_ucl2.dropna(subset=[\"date\", \"id_patient\"], inplace=True)\n",
    "\n",
    "grouped_df = df_tau.groupby('PET_TAU', group_keys=False).apply(adjust_dates)\n",
    "df_tau = grouped_df.drop_duplicates(subset=[\"PET_TAU\", \"adjusted_date\", \"timepoint\"])\n",
    "df_tau[\"date\"] = df_tau[\"adjusted_date\"].copy()\n",
    "df_tau.reset_index(inplace=True)\n",
    "df_tau.drop(columns=[\"index\", \"adjusted_date\"], inplace=True)\n",
    "print(\"ATTENTION: ces lignes sont des suivis mais pas de deuxiemes dates enregistrées pour pet tau, elles ne seront donc pas ajoutées!\")\n",
    "display(df_tau[df_tau.duplicated(subset=[\"PET_TAU\", \"date\"])])\n",
    "df_tau.drop(list(df_tau[df_tau.duplicated(subset=[\"PET_TAU\", \"date\"])].index), inplace=True)\n",
    "df_tau.dropna(subset=[\"date\", \"id_patient\"], inplace=True)\n",
    "\n",
    "display(df_ucl2)\n",
    "up = \"\"\n",
    "for databases in [df_ucl2, df_tau]:\n",
    "    for index, row in databases.iterrows():\n",
    "        for file in os.listdir(row[\"path\"]+\"/stats\"):\n",
    "            f = file.lower()\n",
    "            f = f.replace(\".\", \"_\")\n",
    "            if f in paral:\n",
    "                print(\"non\")\n",
    "                print(row[\"path\"]+\"/stats\")\n",
    "                print(f)\n",
    "                # pour plus tard.... namefile = file.split()\n",
    "                accessName = row[\"path\"]+\"/stats/\"+file\n",
    "                f_open = open(accessName, 'r')\n",
    "                Lines = f_open.readlines()\n",
    "\n",
    "                mesure = []\n",
    "                colnames = []\n",
    "                # On prend d'abord les mesures dans le # et les noms de colonnes\n",
    "                count = 0\n",
    "                date = None\n",
    "                ligne =\"\"\n",
    "                for line in Lines:\n",
    "                    words = [x for x in pattern.split(line) if x]\n",
    "                    if len(words) > 1:\n",
    "                        if words[1].lower() == \"colheaders\":\n",
    "                            colnames = words[2:]\n",
    "                    count += 1\n",
    "\n",
    "                f_open.close()\n",
    "                # maintenant on prend la matrice dans un panda dataset\n",
    "                df_measures = pd.read_csv(accessName, header=None, names=colnames, comment='#', delim_whitespace=True)\n",
    "                columns = []\n",
    "                for x in df_measures[\"StructName\"]:\n",
    "                    if x == \"Left-Thalamus-Proper\":\n",
    "                        columns.append(\"left_thalamus\")\n",
    "                    elif x == \"Right-Thalamus-Proper\":\n",
    "                        columns.append(\"right_thalamus\")\n",
    "                    else:\n",
    "                        columns.append(x.lower().replace(\"-\", \"_\").replace(\"&\", \"_and_\"))\n",
    "                for m in measures[f]:\n",
    "                    if m == \"vol\":\n",
    "                        r = [float(x) for x in df_measures[\"Volume_mm3\"]]\n",
    "                    else:\n",
    "                        r = [float(x) for x in df_measures[m]]\n",
    "                    df = pd.DataFrame([r], columns=columns)\n",
    "                        \n",
    "                    df[\"id_patient\"] = row[\"id_patient\"]\n",
    "                    df[\"date_irm\"] = row[\"date\"]\n",
    "                    t = \"irm_\"+paral[f]+\"_v\"+row[\"version\"]+\"_\"+m.lower()\n",
    "                    \n",
    "                    if file.startswith(\"lh\"):\n",
    "                        df[\"hemispher\"] = \"L\"\n",
    "                    if file.startswith(\"rh\"):\n",
    "                        df[\"hemispher\"] = \"R\"\n",
    "\n",
    "                    sql = \"select * from \"+t\n",
    "                    df_db = pd.read_sql(sql, con=mysql.connector.connect(**conn_new_db), coerce_float=False)\n",
    "                    r = df_db.loc[df_db[\"id_patient\"] == row[\"id_patient\"]]\n",
    "                    r = r.loc[r[\"date_irm\"] == row[\"date\"]]\n",
    "                    if \"hemispher\" in df_db.columns and file.startswith(\"lh\"):\n",
    "                        r = r.loc[r[\"hemispher\"] == \"L\"]\n",
    "                    elif \"hemispher\" in df_db.columns and file.startswith(\"rh\"):\n",
    "                        r = r.loc[r[\"hemispher\"] == \"R\"]\n",
    "                        \n",
    "                    condition = \"\\nWHERE \"\n",
    "                    condition += \" id_patient = \\\"\"+str(row[\"id_patient\"])+\"\\\"\"\n",
    "                    condition += \" and date_irm = \\\"\"+str(row[\"date\"])+\"\\\"\"\n",
    "\n",
    "                    if len(r) == 1:\n",
    "                        up2add = \"UPDATE \"+t+\" \\nSET \"#address = %s WHERE address = %s\"\n",
    "                        avant = \"\\n# BEFORE: \"\n",
    "                        col2change = \"\"\n",
    "                        for h in list(df_db.columns):\n",
    "                            if \"date\" not in h and h != \"id_patient\":\n",
    "                                indb = r[h][r.index[0]]\n",
    "                                inexcel = df.loc[0].at[h]\n",
    "                                if pd.isna(indb):\n",
    "                                    if not pd.isna(inexcel):\n",
    "                                        col2change += str(h)+\" = '\"+str(inexcel)+\"', \"\n",
    "                                        avant += str(h)+\" = '\"+str(indb)+\"', \"\n",
    "                                elif not pd.isna(inexcel):\n",
    "                                    if indb != inexcel:\n",
    "                                        col2change += str(h)+\" = '\"+str(inexcel)+\"', \"\n",
    "                                        avant += str(h)+\" = '\"+str(indb)+\"', \"\n",
    "                        if len(col2change) > 0:       \n",
    "                            col2change = col2change[:-2]\n",
    "                            up2add += col2change + avant + condition\n",
    "                            up += up2add + \";\\n\\n\"\n",
    "                    elif len(r) > 1:\n",
    "                        print(\"\")\n",
    "                        print(\"error\")\n",
    "                        sys.exit()\n",
    "                    else:\n",
    "                        print(\"we will add:\\n in: \"+t)\n",
    "                        display(df)\n",
    "                        print(\"ADDING IT NOW\")\n",
    "                        df.to_sql(name=t, con=engine, if_exists=\"append\", index=False)  \n",
    "                        print(\"les données ont bien été ajoutées!\")\n",
    "if len(up) > 0:\n",
    "    with open(\"sql_code/update_irm_from_files_freesurfer.txt\", 'w') as f:\n",
    "        f.write(up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfb736d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
