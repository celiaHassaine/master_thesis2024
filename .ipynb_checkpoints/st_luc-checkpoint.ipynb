{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a60476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import chardet\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "import mysql.connector\n",
    "import numpy as np\n",
    "from mysql.connector.cursor import MySQLCursor\n",
    "import joblib\n",
    "import sys\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shap\n",
    "from utils_mem import *\n",
    "\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC \n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, RepeatedStratifiedKFold, StratifiedGroupKFold\n",
    "import statistics \n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from datetime import timedelta, datetime\n",
    "from tqdm import tqdm\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "# precision = TP/(TP+FP) good for imbalanced and to punish false positive \n",
    "# recall = TP/(TP+FN) good for imbalanced and punish false negative\n",
    "# Fscore is an harmonic mean between recall and precision\n",
    "# ROC AUC?\n",
    "from sklearn.metrics import roc_auc_score, precision_score, classification_report, recall_score, ConfusionMatrixDisplay, precision_recall_fscore_support, confusion_matrix\n",
    "\n",
    "conn_datassistant = {\n",
    "    'host': \"localhost\",\n",
    "    'user': \"root\",\n",
    "    'password': \"CX29Mhky\",\n",
    "    'database': \"databaseassistant\"\n",
    "}\n",
    "\n",
    "conn_new_db = {\n",
    "    'host': \"localhost\",\n",
    "    'user': \"root\",\n",
    "    'password': \"CX29Mhky\",\n",
    "    'database': \"new_db\"\n",
    "}\n",
    "\n",
    "\n",
    "engine = create_engine(\"mysql+mysqlconnector://root:CX29Mhky@localhost/new_db\")\n",
    "\n",
    "path_irm = \"./data/zirm_pc/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08bd6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_columns(df, merged_keys):\n",
    "    \n",
    "    # Séparez les DataFrames selon la valeur de 'hemisphere'\n",
    "    df_left = df[df['hemispher'] == 'L'].copy()\n",
    "    df_right = df[df['hemispher'] == 'R'].copy()\n",
    "\n",
    "    # Renommez les colonnes pour chaque DataFrame séparée\n",
    "    df_left = df_left.rename(columns=lambda x: f'left_{x}' if x not in merged_keys else x)\n",
    "    df_right = df_right.rename(columns=lambda x: f'right_{x}' if x not in merged_keys else x)\n",
    "\n",
    "    # Supprimez la colonne 'hemisphere' de chaque DataFrame\n",
    "    df_left.drop(columns=['left_hemispher'], inplace=True)\n",
    "    df_right.drop(columns=['right_hemispher'], inplace=True)\n",
    "\n",
    "    # Fusionnez les deux DataFrames sur les clés de fusion\n",
    "    df_merged = pd.merge(df_left, df_right, on=merged_keys, how='outer')\n",
    "\n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98d1eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"select * from patient\"\n",
    "\n",
    "df = pd.read_sql(sql, con=mysql.connector.connect(**conn_new_db), coerce_float=False) \n",
    "\n",
    "print(\"Nombre total de patients enregistrés: \"+str(len(df)))\n",
    "\n",
    "print(\"Nombre de personne dans UCL2: \"+str(len(df.loc[~df[\"UCL2\"].isnull()])))\n",
    "print(\"Nombre de personne dans UCL_AD: \"+str(len(df.loc[~df[\"UCL_AD\"].isnull()])))\n",
    "print(\"Nombre de personne dans PET_TAU: \"+str(len(df.loc[~df[\"PET_TAU\"].isnull()])))\n",
    "print(\"Nombre de personne dans BM: \"+str(len(df.loc[~df[\"BM\"].isnull()])))\n",
    "print(\"Nombre de personne dans GABA: \"+str(len(df.loc[~df[\"GABA\"].isnull()])))\n",
    "print(\"Nombre de personne avec leur MEDEX: \"+str(len(df.loc[~df[\"MEDEX\"].isnull()])))\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9661b22f",
   "metadata": {},
   "source": [
    "# prepare tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0167f6",
   "metadata": {},
   "source": [
    "## IRM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c062bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "request = \"SELECT * \\\n",
    "FROM INFORMATION_SCHEMA.TABLES \\\n",
    "WHERE TABLE_NAME LIKE 'irm_%' and TABLE_SCHEMA = 'new_db'\"\n",
    "\n",
    "df = pd.read_sql(request, con=mysql.connector.connect(**conn_new_db), coerce_float=False)\n",
    "#display(df[[\"TABLE_NAME\", \"TABLE_ROWS\", \"TABLE_SCHEMA\"]])\n",
    "#print(df.columns)\n",
    "\n",
    "#print(\"ces colonnes sont vides, donc on ne prend pas: \"+str(list((df.loc[(df[\"TABLE_ROWS\"] == 0)])[\"TABLE_NAME\"])))\n",
    "\n",
    "t = list((df.loc[~(df[\"TABLE_ROWS\"] == 0)])[\"TABLE_NAME\"])\n",
    "dfs = []\n",
    "for i in range(1, len(t)):\n",
    "    if \"v720\" in t[i]:\n",
    "        sql = \"SELECT * FROM \"+str(t[i])\n",
    "        df_sql = pd.read_sql(sql, con=mysql.connector.connect(**conn_new_db), coerce_float=False)\n",
    "        name = t[i].split(\"_\")[-1]\n",
    "        dic = {}\n",
    "        if \"hemispher\" in df_sql.columns:\n",
    "            columns_to_keep = ['id_patient', 'date_irm']\n",
    "            display(df_sql)\n",
    "            df_sql = rename_columns(df_sql, columns_to_keep)\n",
    "            display(df_sql)\n",
    "        \n",
    "        for c in df_sql.columns:\n",
    "            if c != \"id_patient\" and c != \"date_irm\":            \n",
    "                dic[c] = c+\"_\"+t[i]\n",
    "            \n",
    "        df_sql.rename(columns=dic, inplace=True)\n",
    "        dfs.append(df_sql.copy())\n",
    "    \n",
    "for d in range(1, len(dfs)):\n",
    "    if d == 1:\n",
    "        df_irm = pd.merge(dfs[d-1], dfs[d], on=[\"id_patient\", \"date_irm\"], how=\"inner\")\n",
    "    else:\n",
    "        df_irm = pd.merge(df_irm, dfs[d], on=[\"id_patient\", \"date_irm\"], how=\"inner\")\n",
    "            \n",
    "\n",
    "df_irm['date_irm'] = pd.to_datetime(df_irm['date_irm'])\n",
    "display(df_irm)\n",
    "\n",
    "#df_irm.replace(columns={\"date_irm\": \"date\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ab8868",
   "metadata": {},
   "source": [
    "## Neuropsy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00086124",
   "metadata": {},
   "outputs": [],
   "source": [
    "request = \"SELECT * \\\n",
    "FROM INFORMATION_SCHEMA.TABLES \\\n",
    "WHERE TABLE_NAME LIKE 'neuro_%' and TABLE_SCHEMA = 'new_db'\"\n",
    "\n",
    "df_neuro = pd.read_sql(request, con=mysql.connector.connect(**conn_new_db), coerce_float=False)\n",
    "\n",
    "print(\"ces colonnes sont vides, donc on ne prend pas: \"+str(list((df_neuro.loc[(df_neuro[\"TABLE_ROWS\"] == 0)])[\"TABLE_NAME\"])))\n",
    "\n",
    "t = list((df_neuro.loc[~(df_neuro[\"TABLE_ROWS\"] == 0)])[\"TABLE_NAME\"])\n",
    "dfs = []\n",
    "for i in range(1, len(t)):\n",
    "    if not \"parameter\" in t[i]:\n",
    "        sql = \"SELECT * FROM \"+str(t[i])\n",
    "        df_sql = pd.read_sql(sql, con=mysql.connector.connect(**conn_new_db), coerce_float=False)\n",
    "        todrop = []\n",
    "        dic = {}\n",
    "        for c in df_sql.columns:\n",
    "            if 'date' in c:\n",
    "                df_sql.rename(columns={c: \"date_neuro\"}, inplace=True)\n",
    "            if c == \"timepoint\" or c == \"parameter\":\n",
    "                todrop.append(c)\n",
    "            elif c != \"id_patient\":          \n",
    "                dic[c] = c+\"_\"+t[i]\n",
    "        df_sql.drop(columns=todrop, inplace=True)\n",
    "        df_sql.rename(columns=dic, inplace=True)\n",
    "        dfs.append(df_sql.copy())\n",
    "\n",
    "        \n",
    "for d in tqdm(range(1, len(dfs))):\n",
    "    if d == 1:\n",
    "        df_neuro = pd.merge(dfs[d-1], dfs[d], on=[\"id_patient\", \"date_neuro\"], how=\"outer\")\n",
    "    else:\n",
    "        df_neuro = pd.merge(df_neuro, dfs[d], on=[\"id_patient\", \"date_neuro\"], how=\"outer\")\n",
    "        \n",
    "\n",
    "df_neuro['date_neuro'] = pd.to_datetime(df_neuro['date_neuro'])\n",
    "# handles nan values\n",
    "\n",
    "p = (df_neuro.isna().sum().sum()) / (df_neuro.shape[0]*df_neuro.shape[1])\n",
    "print(\"total nan values before: \"+str(df_neuro.isna().sum().sum()))\n",
    "print(\"percentage of nan: \"+str(p*100)+\"%\")\n",
    "\n",
    "df_neuro = supp_col(df_neuro, 0.3)\n",
    "df_neuro = supp_row(df_neuro, 0.3)\n",
    "\n",
    "p = (df_neuro.isna().sum().sum()) / (df_neuro.shape[0]*df_neuro.shape[1])\n",
    "print(\"total nan values after: \"+str(df_neuro.isna().sum().sum()))\n",
    "print(\"percentage of nan: \"+str(p*100)+\"%\")\n",
    "display(df_neuro)\n",
    "\n",
    "#df_neuro.replace(columns={\"date_neuro\": \"date\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c88be68",
   "metadata": {},
   "source": [
    "### Feature patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e5be22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: ajouter annee etude mais etre certain du rang\n",
    "def func_for_rank(nbr):\n",
    "    if pd.isna(nbr): \n",
    "        return nbr\n",
    "    elif nbr <= 15:\n",
    "        return 1\n",
    "    elif nbr <= 18:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "request = \"select id_patient, date_feat, diagnostic, anneeetude_rank, anneeetude_nbr \\\n",
    "from features_patients\"\n",
    "\n",
    "df_feat = pd.read_sql(request, con=mysql.connector.connect(**conn_new_db), coerce_float=False)\n",
    "\n",
    "display(df_feat[df_feat[\"id_patient\"] == 298])\n",
    "df_feat['date_feat'] = pd.to_datetime(df_feat['date_feat'])\n",
    "df_feat[\"anneeetude_rank\"] = df_feat[\"anneeetude_rank\"].astype(int, errors='ignore')\n",
    "df_feat[\"anneeetude_nbr\"] = df_feat[\"anneeetude_nbr\"].astype(int, errors='ignore')\n",
    "df_feat[\"new_rank\"] = df_feat[\"anneeetude_nbr\"].copy()\n",
    "df_feat[\"new_rank\"] = df_feat[\"new_rank\"].apply(func_for_rank)\n",
    "df_feat[\"anneeetude_rank\"] = df_feat[\"anneeetude_rank\"].fillna(df_feat[\"new_rank\"])\n",
    "\n",
    "display(df_feat[df_feat[\"id_patient\"] == 298])\n",
    "df_feat.drop(columns=[\"new_rank\", \"anneeetude_nbr\", \"anneeetude_rank\"], inplace=True)\n",
    "display(df_feat)\n",
    "#df_feat.replace(columns={\"date_feat\": \"date\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe4d22e",
   "metadata": {},
   "source": [
    "## Patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbab1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "request = \"select id_patient, DOB from patient\"\n",
    "\n",
    "df_pat = pd.read_sql(request, con=mysql.connector.connect(**conn_new_db), coerce_float=False)\n",
    "\n",
    "df_pat['DOB'] = pd.to_datetime(df_pat['DOB'])\n",
    "display(df_pat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ced3137",
   "metadata": {},
   "source": [
    "## Amyloid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04bd26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#soit venant d un pet amy soit d une PL\n",
    "def seuil_clf(nbr, lvl):\n",
    "    if pd.isna(nbr):\n",
    "        return nbr\n",
    "    elif nbr < 348:\n",
    "        return \"pos\"\n",
    "    elif nbr < 650:\n",
    "        if lvl == 3:\n",
    "            return \"border\"\n",
    "        elif nbr < 499:\n",
    "            return \"pos\"\n",
    "        else:\n",
    "            return \"neg\"\n",
    "    else:\n",
    "        return \"neg\"\n",
    "    \n",
    "def seuil_amy(nbr, lvl):\n",
    "    if pd.isna(nbr):\n",
    "        return nbr\n",
    "    elif nbr < 12:\n",
    "        return \"neg\"\n",
    "    elif nbr < 40:\n",
    "        if lvl == 3:\n",
    "            return \"border\"\n",
    "        elif nbr < 26:\n",
    "            return \"neg\"\n",
    "        else:\n",
    "            return \"pos\"\n",
    "    else:\n",
    "        return \"pos\"\n",
    "\n",
    "request_clf = \"select id_patient,  date_lcr as date_amy, ab42, tau_total, p_tau from analyse_lcr\"\n",
    "request_pa = \"select id_patient,  date_pet_amy_info as date_amy, centilloide from pet_amy_info\"\n",
    "\n",
    "df_clf = pd.read_sql(request_clf, con=mysql.connector.connect(**conn_new_db), coerce_float=False)\n",
    "df_pa = pd.read_sql(request_pa, con=mysql.connector.connect(**conn_new_db), coerce_float=False)\n",
    "\n",
    "df_pa[\"amy_3\"] = df_pa[\"centilloide\"].apply(seuil_amy, lvl=3)\n",
    "df_clf[\"amy_3\"] = df_clf[\"ab42\"].apply(seuil_clf, lvl=3)\n",
    "df_pa[\"amy_2\"] = df_pa[\"centilloide\"].apply(seuil_amy, lvl=2)\n",
    "df_clf[\"amy_2\"] = df_clf[\"ab42\"].apply(seuil_clf, lvl=2)\n",
    "\n",
    "df_clf.dropna(inplace=True)\n",
    "df_pa.dropna(inplace=True)\n",
    "display(df_clf)\n",
    "display(df_pa)\n",
    "df_amy = pd.concat([df_pa[[\"id_patient\", \"date_amy\", \"amy_3\", \"amy_2\"]], df_clf[[\"id_patient\", \"date_amy\", \"amy_3\", \"amy_2\"]]])\n",
    "df_amy['date_amy'] = pd.to_datetime(df_amy['date_amy'])\n",
    "display(df_amy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4698db",
   "metadata": {},
   "source": [
    "## Diagnostic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624a57c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# commun = IRM, APOE, Neuropsy + autres?\n",
    "# Braak, sans pet tau mais avec amyloid sur 3 niveaux\n",
    "target = \"braak\"\n",
    "\n",
    "#dfs = [(df_irm, \"date_irm\"), (df_neuro, \"date_neuro\"), (df_amy, \"date_amy\")]\n",
    "dfs = [(df_neuro, \"date_neuro\"), (df_irm, \"date_irm\"), (df_amy, \"date_amy\")]\n",
    "df_excel = df_pat.copy()\n",
    "#df_excel = pd.DataFrame(columns=[\"id_patient\"])\n",
    "for x in dfs:\n",
    "    df = x[0]\n",
    "    df_excel = pd.merge(df_excel, df.dropna(subset=[x[1]], how=\"any\"), on=[\"id_patient\"], how=\"inner\")\n",
    "print(\"before braak\")\n",
    "display(df_excel)\n",
    "df_braak = pd.read_sql(\"select id_patient, date_pet_tau_info as date_braak, stade_braak_visuel as diagnostic from pet_tau_info where stade_braak_visuel is not Null\", \n",
    "                       con=mysql.connector.connect(**conn_new_db), coerce_float=False) \n",
    "print(\"len of diagnostics when sql: \"+str(len(df_braak[\"diagnostic\"])))\n",
    "dfs.append((df_braak, \"date_braak\"))\n",
    "df_braak['date_braak'] = pd.to_datetime(df_braak['date_braak'])\n",
    "#df_excel.drop(columns={\"diagnostic\"}, inplace=True)\n",
    "df_excel = pd.merge(df_excel, df_braak, on=[\"id_patient\"], how=\"right\")\n",
    "df_excel.dropna(subset=[\"diagnostic\"], inplace=True)\n",
    "df_excel[\"diagnostic\"] = df_excel[\"diagnostic\"].astype(int)\n",
    "print(df_excel.isnull().sum())\n",
    "df_excel = supp_col(df_excel, 0.3)\n",
    "print(\"drop na\")\n",
    "print(\"len of diagnostics after merging: \"+str(len(df_excel[\"diagnostic\"])))\n",
    "df_excel_copy = df_excel.dropna(subset=[x[1] for x in dfs], how='any')\n",
    "print(\"from drop na => no neuro test recorded\")\n",
    "missed = list(set(df_excel.index).difference(set(df_excel_copy.index)))\n",
    "print(len(set(df_excel.index).difference(set(df_excel_copy.index))))\n",
    "print(len((df_excel.loc[missed, :])[\"id_patient\"].unique()))\n",
    "s = \"(\"\n",
    "for i in (df_excel.loc[missed, :])[\"id_patient\"].unique():\n",
    "    s += str(i)+\", \"\n",
    "print(s)\n",
    "display(df_excel)\n",
    "print(\"\\n\\nafter: date\")\n",
    "#display(df_excel[[\"date_irm\", \"date_neuro\", \"date_braak\", \"date_amy\"]])\n",
    "df_excel = df_excel_copy.copy()\n",
    "df_excel_copy = date_within(df_excel, 400, [x[1] for x in dfs], v=True)\n",
    "\n",
    "display(df_excel_copy)\n",
    "missed = list(set(df_excel.index).difference(set(df_excel_copy.index)))\n",
    "print(str(len(set(df_excel.index).difference(set(df_excel_copy.index))))+\" lines is missing the neuro\")\n",
    "print(str(len((df_excel.loc[missed, :])[\"id_patient\"].unique()))+\" patients is missing the neuro\")\n",
    "print(\"len of diagnostics after drop too far medical exams: \"+str(len(df_excel[\"diagnostic\"])))\n",
    "\n",
    "s = \"(\"\n",
    "for i in (df_excel.loc[missed, :])[\"id_patient\"].unique():\n",
    "    s += str(i)+\", \"\n",
    "print(s)\n",
    "\n",
    "df_excel = df_excel_copy.copy()\n",
    "\n",
    "df_excel[\"age\"] = df_excel[dfs[0][1]].dt.year - df_excel[\"DOB\"].dt.year #- ((df_feat[\"date_feat\"].dt.month, df_feat[\"date_feat\"].dt.day) < (df_feat[\"DOB\"].dt.month, df_feat[\"DOB\"].dt.day))\n",
    "print(df_excel.isna().sum().sum())\n",
    "\n",
    "c = [\"id_patient\", \"diagnostic\"]\n",
    "\"\"\"for x in df_excel.columns:\n",
    "    nan = df_excel[x].isna().sum()\n",
    "    if \"date\" in x and \"vol\" not in x:\n",
    "        c.append(x)\n",
    "    if nan > 0:\n",
    "        print(x+\" with \"+str(nan)+\" # of nan\")\"\"\"\n",
    "        \n",
    "bo = df_excel.isnull().sum(axis=1).ge(1)\n",
    "#print(sum(bool(x) for x in bo.tolist()))\n",
    "df_excel = supp_col(df_excel, 0.08, verbose=True)\n",
    "\n",
    "bo = df_excel.isnull().sum(axis=1).ge(1)\n",
    "# print(sum(bool(x) for x in bo.tolist()))\n",
    "\n",
    "df_excel.dropna(inplace=True, how=\"any\")\n",
    "\n",
    "df_excel.reset_index(inplace=True)\n",
    "df_excel.drop(columns={\"index\", \"DOB\"}.union({x[1] for x in dfs}), inplace=True)\n",
    "\n",
    "#print_diag_rep(df_excel)\n",
    "df_excel[\"diagnostic\"].replace(2,1, inplace=True)\n",
    "df_excel[\"diagnostic\"].replace(4,3, inplace=True)\n",
    "df_excel[\"diagnostic\"].replace(6,5, inplace=True)\n",
    "display(df_excel)\n",
    "print(str(len(df_excel[\"id_patient\"].unique()))+\" patients is in the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea2bf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    from imblearn.over_sampling import RandomOverSampler\n",
    "    ros = RandomOverSampler(random_state=0, sampling_strategy={1: 16, 3: 22, 5:42})\n",
    "    \n",
    "    X_copy, y_copy = ros.fit_resample(df_excel.drop(columns=[\"diagnostic\"]), df_excel[\"diagnostic\"])\n",
    "    df_excel_under = pd.DataFrame(X_copy.copy(), columns=df_excel.columns)\n",
    "    df_excel_under['diagnostic'] = y_copy.copy()\n",
    "    df_excel_under.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    print(df_excel_under['diagnostic'].value_counts())\n",
    "    df_excel = df_excel_under.copy()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01be33bf",
   "metadata": {},
   "source": [
    "# Sub-DB creations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c1b1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cop = df_excel.copy()\n",
    "\n",
    "print(\"total nan values before: \" + str(df_cop.isna().sum().sum()))\n",
    "xx = pd.get_dummies(df_cop, columns=[\"amy_2\", \"amy_3\"])\n",
    "fs = SelectKBest(score_func=f_classif, k=10)\n",
    "fs.fit(xx.drop(columns=[\"diagnostic\"]), xx[\"diagnostic\"])\n",
    "df_fs = pd.DataFrame({\"name\": fs.feature_names_in_, \"scores\": fs.scores_, \"p-value\": fs.pvalues_})\n",
    "display(df_fs.sort_values(by=\"scores\", ascending=False).head(5))\n",
    "\n",
    "strat_dbs = build_all_datasets(df_cop)\n",
    "print(strat_dbs.keys())\n",
    "\n",
    "#strategies_DB = [\"CN_only\", \"atlases\", \"amy\", \"feature_selec\", \"APOE\"] \n",
    "strategies_DB = [\"amy\", \"atlas\"] \n",
    "strategies_ML = [\"nbr_trees\", \"depth_trees\", \"kernels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04be563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supposons que df soit votre DataFrame\n",
    "# Vous devez avoir calculé préalablement la matrice de corrélation corr_matrix\n",
    "for data_strat in strat_dbs:\n",
    "    df_excel = strat_dbs[data_strat].copy()\n",
    "    display(df_excel)\n",
    "    corr_matrix = df_excel.corr()\n",
    "    correlated_pairs = set()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            if abs(corr_matrix.iloc[i, j]) > 0.9:\n",
    "                correlated_pairs.add((corr_matrix.columns[i], corr_matrix.columns[j]))\n",
    "\n",
    "    # Supprimer une colonne de chaque paire et renommer la colonne restante\n",
    "    for col1, col2 in correlated_pairs:\n",
    "        if col1 in df_excel.columns and col2 in df_excel.columns:\n",
    "            # Supprimer une colonne de chaque paire\n",
    "            df_excel.drop(columns=[col1], inplace=True)\n",
    "            # Renommer la colonne restante par la jointure des noms des deux colonnes\n",
    "            new_col_name = col1+\"_\"+col2\n",
    "            df_excel.rename(columns={col2: new_col_name}, inplace=True)\n",
    "    print(\"END\")\n",
    "    # Afficher le DataFrame résultant\n",
    "    display(df_excel)\n",
    "    print(correlated_pairs)\n",
    "    print(len(correlated_pairs))\n",
    "    \n",
    "    strat_dbs[data_strat] = df_excel.copy()\n",
    "print(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15abc15",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c943036",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_option = [\"with_fs\", \"without_fs\"]\n",
    "feat_selec = f_option[1]\n",
    "onehot_scaled = False \n",
    "target_nbr = 20\n",
    "\n",
    "new_dbs = {}\n",
    "for options in strat_dbs:\n",
    "    new_dbs[options] = {}\n",
    "    curr_db = strat_dbs[options].copy()\n",
    "\n",
    "    print(\"in the strategies \"+options)\n",
    "    y = curr_db[[\"diagnostic\"]].copy()\n",
    "    X = curr_db.drop(columns=[\"diagnostic\", \"id_patient\"]).copy()\n",
    "    grouping = curr_db[\"id_patient\"].copy()\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    if onehot_scaled:\n",
    "        col_2_scale = X.columns\n",
    "    else:\n",
    "        col_2_scale = [x for x in X if \"amy_2\" not in x and \"amy_3\" not in x]\n",
    "    X[col_2_scale] = scaler.fit_transform(X[col_2_scale], y[\"diagnostic\"])\n",
    "\n",
    "    fs = SelectKBest(score_func=f_classif, k=target_nbr)\n",
    "    fs.fit(X, y[\"diagnostic\"])\n",
    "    # apply feature selection\n",
    "    if feat_selec == \"with_fs\":\n",
    "        X_new = fs.transform(X)\n",
    "        X = pd.DataFrame(X_new, columns=fs.get_feature_names_out())\n",
    "\n",
    "    new_dbs[options] = [X.copy(), y.copy(), fs, grouping.copy()]\n",
    "print(\"start\")\n",
    "for i in new_dbs:\n",
    "    print(\"\\n\\n\"+i)\n",
    "    fs = new_dbs[i][2]\n",
    "    df_fs = pd.DataFrame({\"name\": fs.feature_names_in_, \"scores\": fs.scores_, \"pvalue\": fs.pvalues_})\n",
    "    df_fs = df_fs.sort_values(by=\"scores\", ascending=False).reset_index().drop(columns=[\"index\"])\n",
    "    display(df_fs.head(3))\n",
    "    display(df_fs[df_fs[\"name\"].str.contains(\"amy\")])\n",
    "    display(df_fs[df_fs[\"name\"].str.contains(\"z_\")])\n",
    "        \n",
    "        \n",
    "if feat_selec == \"with_fs\":\n",
    "    name=[]\n",
    "    column_dfs = []\n",
    "    for i in new_dbs:\n",
    "        name.append(i)\n",
    "        column_dfs.append(set(new_dbs[i][0].columns))\n",
    "        \n",
    "    # Création d'un dictionnaire pour stocker les ensembles en utilisant leurs éléments concaténés comme clés\n",
    "    sets_dict = {}\n",
    "\n",
    "    # Parcours des listes\n",
    "    for nom, ensemble in zip(name, column_dfs):\n",
    "        # Convertir l'ensemble en tuple trié pour l'utiliser comme clé dans le dictionnaire\n",
    "        ensemble_trié = tuple(sorted(ensemble))\n",
    "        # Vérifier si l'ensemble existe déjà dans le dictionnaire\n",
    "        if ensemble_trié in sets_dict:\n",
    "            # Fusionner les noms\n",
    "            sets_dict[ensemble_trié] += \"-\"+nom\n",
    "        else:\n",
    "            # Ajouter le nom et l'ensemble dans le dictionnaire\n",
    "            sets_dict[ensemble_trié] = nom\n",
    "\n",
    "    # Extraire les noms uniques et les ensembles correspondants\n",
    "    nouveaux_noms = []\n",
    "    ensembles_uniques = []\n",
    "    for ensemble, nom in sets_dict.items():\n",
    "        nouveaux_noms.append(nom)\n",
    "        ensembles_uniques.append(set(ensemble))\n",
    "    print(nouveaux_noms)  # Sortie : [\"ACD\", \"B\"]\n",
    "    #print(ensembles_uniques)  # Sortie : [{1, 2}, {3, 4}]\n",
    "    print(new_dbs.keys())\n",
    "    a = 2\n",
    "    b = 8\n",
    "    print(nouveaux_noms[a]+\" and \"+nouveaux_noms[b])\n",
    "    print(ensembles_uniques[a].difference(ensembles_uniques[b]))\n",
    "    print(ensembles_uniques[b].difference(ensembles_uniques[a]))\n",
    "    new_new_dbs = {}\n",
    "    for name in nouveaux_noms:\n",
    "        new_new_dbs[\"_\".join(name.split(\"-\"))] = new_dbs[name.split(\"-\")[0]]\n",
    "    # ba exvivo garde le amy 3 pos, \n",
    "    print(new_new_dbs.keys())\n",
    "    new_dbs = new_new_dbs.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b698725",
   "metadata": {},
   "source": [
    "# Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de04255",
   "metadata": {},
   "source": [
    "### GridSearch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d012b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics \n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "rf_run = True\n",
    "\n",
    "scores_options = {\"auc_roc\": roc_auc_score, \"f1score\": f1_score}\n",
    "scoring = \"f1score\"\n",
    "binary = False\n",
    "\n",
    "custom_scorer = make_scorer(multiclass_score, greater_is_better=True, targets=[1], score=scores_options[scoring], \n",
    "                            binary=binary)\n",
    "\n",
    "today = datetime.now().strftime(\"%d-%m-%Y_%Hh%Mm%Ss\")\n",
    "path=\"./backup_models/Braak/RF/\"+feat_selec+\"/\"+scoring+\"/\"+today+\"/\"\n",
    "if feat_selec == \"with_fs\":\n",
    "    path=\"./backup_models/Braak/RF/\"+feat_selec+\"/\"+str(target_nbr)+\"/\"+scoring+\"/\"+today+\"/\"\n",
    "    \n",
    "if rf_run:\n",
    "    #varier nbr of trees ? et plus de depth? \n",
    "    strat_ml = {\n",
    "        \"depth\":{\n",
    "            \"10\":{\n",
    "                'n_estimators': [10, 50, 100], 'criterion': ['gini', 'entropy'],'max_depth': [10], 'max_features': ['sqrt', 'log2', None, 0.5,0.8]\n",
    "            },\n",
    "            \"100\":{\n",
    "                'n_estimators': [10, 50, 100], 'criterion': ['gini', 'entropy'],'max_depth': [100], 'max_features': ['sqrt', 'log2', None, 0.5,0.8]\n",
    "            },\n",
    "            \"1000\":{\n",
    "                'n_estimators': [10, 50, 100], 'criterion': ['gini', 'entropy'],'max_depth': [1000], 'max_features': ['sqrt', 'log2', None, 0.5,0.8]\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    os.makedirs(path+\"gridsearch_models/\", exist_ok=True)\n",
    "    os.makedirs(path+\"test_train_set/\", exist_ok=True)\n",
    "    os.makedirs(path+\"dataset/\", exist_ok=True)\n",
    "    os.makedirs(path+\"roc_curve/\", exist_ok=True)\n",
    "    os.makedirs(path+\"cm/\", exist_ok=True)\n",
    "    os.makedirs(path+\"report/\", exist_ok=True)\n",
    "    \n",
    "    conf_mat = []\n",
    "    top5_combinaison = []\n",
    "    exclude = {}\n",
    "    \n",
    "    for stratml in strat_ml:\n",
    "        strat_df_best_score = pd.DataFrame(columns=strat_ml[stratml].keys(), index=new_dbs.keys())\n",
    "        strat_df_score_test = pd.DataFrame(columns=strat_ml[stratml].keys(), index=new_dbs.keys())\n",
    "        strat_df_score_cross_val = pd.DataFrame(columns=strat_ml[stratml].keys(), index=new_dbs.keys())\n",
    "        strat_df_score_cross_val_std = pd.DataFrame(columns=strat_ml[stratml].keys(), index=new_dbs.keys())\n",
    "        for k in strat_ml[stratml]:\n",
    "            for options in new_dbs:\n",
    "                if len(top5_combinaison) == 0 or stratml+\"_\"+options in top5_combinaison:\n",
    "                    name_file = stratml+\"_\"+k+\"_\"+options+\"_score_\"+scoring\n",
    "                    print(name_file)\n",
    "\n",
    "                    X = new_dbs[options][0]\n",
    "                    y = new_dbs[options][1]\n",
    "                    group_list = new_dbs[options][-1]\n",
    "                    groupkfold = StratifiedGroupKFold(n_splits=5)\n",
    "                    if options+\".csv\" not in os.listdir(path+\"dataset\"):\n",
    "                        pd.concat([group_list, X, y], axis=1).to_csv(path + \"dataset/\" + options + \".csv\")\n",
    "                    \n",
    "                    \n",
    "                    best_params, best_score, score_on_test, model = grid_searches(X, y, RandomForestClassifier, \n",
    "                                                                               custom_scorer, strat_ml[stratml][k], \n",
    "                                                                               path, name_file, v= 3 , cv_grid=groupkfold, \n",
    "                                                                               cv_out_grid=groupkfold, group=group_list)\n",
    "                    \n",
    "                    \n",
    "                    ####### cross val + CM  + roc curve + scores  + STD\n",
    "                    \n",
    "                    \n",
    "                    y = y[\"diagnostic\"]\n",
    "                    y_bin = label_binarize(y, classes=np.unique(y))\n",
    "\n",
    "                    splitting = groupkfold.split(X, y, group_list)\n",
    "\n",
    "                    conf_matrix_list_of_arrays = []\n",
    "                    scores = []\n",
    "\n",
    "                    # Initialiser les variables pour les courbes ROC et les matrices de confusion\n",
    "                    tprs = {i: [] for i in range(len(np.unique(y)))}\n",
    "                    mean_fpr = np.linspace(0, 1, 100)\n",
    "                    fig, ax = plt.subplots()\n",
    "                    df_scores = []\n",
    "                    for indice, (train_index, test_index) in enumerate(splitting):\n",
    "\n",
    "                        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "                        y_train, y_test = y_bin[train_index], y_bin[test_index]\n",
    "                        classifier = OneVsRestClassifier(model)\n",
    "                        classifier.fit(X_train, y_train)\n",
    "                        model.fit(X_train, y.iloc[train_index])\n",
    "                        # df_probs = pd.DataFrame(clf.predict_proba(X_test), columns=[i for i in clf.classes_])\n",
    "                        if hasattr(classifier, \"decision_function\"):\n",
    "                            y_pred_proba = classifier.decision_function(X_test)\n",
    "                        else:\n",
    "                            y_pred_proba = classifier.predict_proba(X_test)\n",
    "                        scores.append(f1_score(y.iloc[test_index], model.predict(X_test), average=\"weighted\"))\n",
    "\n",
    "                        # Calculer les courbes ROC pour chaque classe et déterminer les seuils optimaux\n",
    "                        fpr = dict()\n",
    "                        tpr = dict()\n",
    "                        thresholds = dict()\n",
    "                        roc_auc = dict()\n",
    "                        optimal_thresholds = []\n",
    "\n",
    "                        for i, lab in zip(range(len(np.unique(y))), [0,1,3,5]):\n",
    "                            fpr[i], tpr[i], thresholds[i] = roc_curve(y_test[:, i], y_pred_proba[:, i], )\n",
    "                            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "                            # Interpoler les tpr à la moyenne des fpr\n",
    "                            tprs[i].append(np.interp(mean_fpr, fpr[i], tpr[i]))\n",
    "                            tprs[i][-1][0] = 0.0\n",
    "\n",
    "                            # Calculer le point optimal pour chaque classe en utilisant Youden's J statistic\n",
    "                            optimal_idx = np.argmax(tpr[i] - fpr[i])\n",
    "                            optimal_threshold = thresholds[i][optimal_idx]\n",
    "                            optimal_thresholds.append(optimal_threshold)\n",
    "                        \n",
    "                        \n",
    "                        y_pred_adjusted = np.zeros_like(y_pred_proba)\n",
    "\n",
    "                        for i in range(len(np.unique(y))):\n",
    "                            y_pred_adjusted[:, i] = (y_pred_proba[:, i] >= optimal_thresholds[i]).astype(int)* (y_pred_proba[:, i])\n",
    "                        \n",
    "                        \n",
    "                        y_pred_labels = np.argmax(y_pred_adjusted, axis=1)\n",
    "                        y_pred_labels[y_pred_labels==3] = 5\n",
    "                        y_pred_labels[y_pred_labels==2] = 3\n",
    "\n",
    "                        conf_matrix = confusion_matrix(y.iloc[test_index], model.predict(X_test), labels=[0,1,3,5])\n",
    "\n",
    "                        conf_matrix_list_of_arrays.append(conf_matrix)\n",
    "                        \n",
    "                        \n",
    "                        report = classification_report(y.iloc[test_index], model.predict(X_test), labels=[0,1,3,5], output_dict=True)\n",
    "                        df_for_scoring = pd.DataFrame(report).transpose()\n",
    "\n",
    "                        new_line = [0]*(len(df_for_scoring.index)-2)\n",
    "                        score_auc = roc_auc_score(y_test, y_pred_proba, average=\"macro\", multi_class='ovr', labels=[0,1, 3,5])\n",
    "                        new_line.append(score_auc)\n",
    "                        score_auc = roc_auc_score(y_test, y_pred_proba, average=\"weighted\", multi_class='ovr', labels=[0,1,3,5])\n",
    "                        new_line.append(score_auc)\n",
    "                        df_for_scoring[\"auc\"] = new_line\n",
    "\n",
    "                        df_scores.append(df_for_scoring)\n",
    "                        \n",
    "                    \n",
    "                    # Tracer la moyenne des courbes ROC pour chaque classe\n",
    "                    for i, lab in zip(range(len(np.unique(y))), [0,1,3,5]):\n",
    "                        std_tpr = np.std(tprs[i], axis=0)\n",
    "                        mean_tpr = np.mean(tprs[i], axis=0)\n",
    "                        mean_tpr[-1] = 1.0\n",
    "                        mean_auc = auc(mean_fpr, mean_tpr)\n",
    "                        std_auc = np.std([auc(fpr, tpr) for fpr, tpr in zip([mean_fpr]*len(tprs[i]), tprs[i])])\n",
    "                        plt.plot(mean_fpr, mean_tpr, linestyle='-', linewidth=2, alpha=0.3,\n",
    "                                 label='Mean ROC class {0} (area = {1:0.2f} ± {2:0.2f})'.format(lab, mean_auc, std_auc))\n",
    "                        \n",
    "                    \n",
    "                    # Calculer la courbe ROC moyenne globale\n",
    "                    all_tprs = np.mean([np.mean(tprs[i], axis=0) for i in range(len(np.unique(y)))], axis=0)\n",
    "                    all_auc = auc(mean_fpr, all_tprs)\n",
    "\n",
    "                    std_tpr = np.std([np.std(tprs[i], axis=0) for i in range(len(np.unique(y)))], axis=0)\n",
    "                    auc_for_std = []\n",
    "                    for i in range(len(np.unique(y))):\n",
    "                        auc_for_std.append([auc(fpr, tpr) for fpr, tpr in zip([mean_fpr]*len(tprs[i]), tprs[i])])\n",
    "                    std_auc = np.std(auc_for_std)\n",
    "                    plt.plot(mean_fpr, all_tprs, color='b', linestyle='-', linewidth=2,\n",
    "                             label='Mean ROC all classes (area = {0:0.2f})'.format(all_auc))\n",
    "\n",
    "                    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "                    plt.xlim([0.0, 1.0])\n",
    "                    plt.ylim([0.0, 1.05])\n",
    "                    plt.xlabel('False Positive Rate')\n",
    "                    plt.ylabel('True Positive Rate')\n",
    "                    plt.title('Receiver Operating Characteristic')\n",
    "                    plt.legend(loc=\"lower right\")\n",
    "                    if name_file is not None:\n",
    "                        plt.savefig(path + \"roc_curve/\" + name_file + \"_roc_curve.png\")\n",
    "                    plt.show()\n",
    "                    plt.clf()\n",
    "\n",
    "                    sum_conf_mat = np.sum(conf_matrix_list_of_arrays, axis=0)\n",
    "                    \n",
    "                    plt.title(\"confusion matrix from RF\")\n",
    "                    # Calcul de la somme des éléments de chaque ligne\n",
    "                    row_sums = np.sum(sum_conf_mat, axis=1)\n",
    "\n",
    "                    # Mise à l'échelle en divisant chaque élément par la somme de sa ligne\n",
    "                    scaled_arr = sum_conf_mat / row_sums[:, np.newaxis]\n",
    "                    # cm = ConfusionMatrixDisplay(sum_conf_mat / np.sum(sum_conf_mat), display_labels=classes).plot()\n",
    "                    ConfusionMatrixDisplay(sum_conf_mat, display_labels=[0,1,3,5]).plot()\n",
    "\n",
    "                    plt.savefig(path + \"cm/\" + name_file + \"_cm.png\")\n",
    "                    plt.show()\n",
    "                    plt.clf()\n",
    "                    \n",
    "                    # Calculer la moyenne de chaque élément\n",
    "                    mean_df = pd.concat(df_scores).groupby(level=0).mean().round(3)\n",
    "\n",
    "                    # Calculer l'écart type de chaque élément\n",
    "                    std_df = pd.concat(df_scores).groupby(level=0).std().round(3)\n",
    "                    mean_df.to_csv(path + \"report/\" + name_file + \"_mean.csv\")\n",
    "                    std_df.to_csv(path + \"report/\" + name_file + \"_std.csv\")\n",
    "                    display(mean_df)\n",
    "                    print(\"coming from\")\n",
    "                    for lesdf in df_scores:\n",
    "                        display(lesdf)\n",
    "                    ################ fin cross val\n",
    "                    \n",
    "                    \n",
    "                    print(best_params)\n",
    "                    \n",
    "                    strat_df_best_score.at[options, k] = best_score\n",
    "                    strat_df_score_test.at[options, k] = score_on_test\n",
    "                    strat_df_score_cross_val.at[options, k] = sum(scores)/len(scores)\n",
    "                    strat_df_score_cross_val_std.at[options, k] = statistics.pstdev(scores)\n",
    "\n",
    "            \n",
    "        #for i in conf_mat:\n",
    "            #print(i[0])\n",
    "            #ConfusionMatrixDisplay(i[1], display_labels=i[2]).plot()\n",
    "\n",
    "        strat_df_best_score.dropna(inplace=True)\n",
    "        strat_df_score_test.dropna(inplace=True)\n",
    "        strat_df_score_cross_val.dropna(inplace=True)\n",
    "        strat_df_score_cross_val_std.dropna(inplace=True)\n",
    "\n",
    "        for i in strat_df_best_score.columns:\n",
    "            strat_df_best_score[i] = strat_df_best_score[i].astype(float)\n",
    "            strat_df_score_test[i] = strat_df_score_test[i].astype(float)\n",
    "            strat_df_score_cross_val[i] = strat_df_score_cross_val[i].astype(float)\n",
    "            strat_df_score_cross_val_std[i] = strat_df_score_cross_val_std[i].astype(float)\n",
    "\n",
    "        \n",
    "        display(strat_df_best_score)\n",
    "        display(strat_df_score_test)\n",
    "        display(strat_df_score_cross_val)\n",
    "        display(strat_df_score_cross_val_std)\n",
    "    \n",
    "        strat_df_best_score.to_csv(path+stratml+\"_best_scores_in_grid.csv\")\n",
    "        strat_df_score_test.to_csv(path+stratml+\"_score_on_test.csv\")\n",
    "        strat_df_score_cross_val.to_csv(path+stratml+\"_score_on_cross_val.csv\")\n",
    "        strat_df_score_cross_val_std.to_csv(path+stratml+\"_score_on_cross_val_std.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca9394d",
   "metadata": {},
   "source": [
    "### Plot scores & Learning Curve\n",
    "\n",
    "ATTENTION: si on ajoute des strategies ML, il faudra stocker dans une liste et itérer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332eb573",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    path=\"./backup_models/Braak/RF/\"+feat_selec+\"/\"+scoring\n",
    "    if feat_selec == \"with_fs\":\n",
    "        path=\"./backup_models/Braak/RF/\"+feat_selec+\"/\"+str(target_nbr)+\"/\"+scoring\n",
    "\n",
    "    plot_scores(path, plots=[\"score_on_cross_val\"], v=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903ea491",
   "metadata": {},
   "source": [
    "### Learning Curve & XAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffa6713",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    path=\"./backup_models/Braak/RF/\"+feat_selec+\"/\"+scoring\n",
    "    if feat_selec == \"with_fs\":\n",
    "        path=\"./backup_models/Braak/RF/\"+feat_selec+\"/\"+str(target_nbr)+\"/\"+scoring\n",
    "        \n",
    "    files = os.listdir(path)\n",
    "    dir_grid = max([datetime.strptime(f, \"%d-%m-%Y_%Hh%Mm%Ss\") for f in files if os.path.isdir(path+'/'+f)]).strftime(\"%d-%m-%Y_%Hh%Mm%Ss\")\n",
    "    for scores in os.listdir(path+\"/\"+dir_grid):\n",
    "        if \"score_on_cross_val\" in scores:\n",
    "            strat_df_score_cross_val = pd.read_csv(path+\"/\"+dir_grid+\"/\"+scores, index_col=0)\n",
    "            break\n",
    "    \n",
    "    max_value = strat_df_score_cross_val.values.max()\n",
    "\n",
    "    max_positions = list(zip(*np.where(strat_df_score_cross_val == max_value)))\n",
    "\n",
    "    # Create the associations\n",
    "    targets = [f\"{strat_df_score_cross_val.columns[col]}_{strat_df_score_cross_val.index[row]}\" for row, col in max_positions]\n",
    "    #targets = ['10_irm2_APOE_nbr_allelle_4_irm2_APOE_one_hot', '100_irm2_APOE_nbr_allelle_4_irm2_APOE_one_hot', '1000_irm2_APOE_nbr_allelle_4_irm2_APOE_one_hot']\n",
    "    print(\"targets: \"+str(targets))\n",
    "    print(new_dbs.keys())\n",
    "    print(path)\n",
    "    groupkfold = StratifiedGroupKFold(n_splits=5)\n",
    "    XAI_analyses(new_dbs, path, targets, custom_scorer, v=True, nsplits_exp=5, shap=False)\n",
    "    #plot_confusion_matrices(new_dbs, path, targets, groupkfold,scores_options[scoring], binary, [0,1], [1], summat=False)\n",
    "    #plot_roc_curve(new_dbs, path, targets, groupkfold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1778341c",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d199a902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics \n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "rf_run = True\n",
    "\n",
    "scores_options = {\"auc_roc\": roc_auc_score, \"f1score\": f1_score}\n",
    "scoring = \"f1score\"\n",
    "binary = False\n",
    "\n",
    "custom_scorer = make_scorer(multiclass_score, greater_is_better=True, targets=[1], score=scores_options[scoring], \n",
    "                            binary=binary)\n",
    "\n",
    "today = datetime.now().strftime(\"%d-%m-%Y_%Hh%Mm%Ss\")\n",
    "path=\"./backup_models/Braak/SVM/\"+feat_selec+\"/\"+scoring+\"/\"+today+\"/\"\n",
    "if feat_selec == \"with_fs\":\n",
    "    path=\"./backup_models/Braak/SVM/\"+feat_selec+\"/\"+str(target_nbr)+\"/\"+scoring+\"/\"+today+\"/\"\n",
    "    \n",
    "if rf_run:\n",
    "    #varier nbr of trees ? et plus de depth? \n",
    "    strat_ml = {\n",
    "        \"kernel\": {\"linear\": {'C': [1, 10, 100], 'kernel': ['linear']}, \n",
    "               \"rbf\": {'C': [1, 10, 100], 'gamma': ['scale', 0.001, 0.0001], 'kernel': ['rbf']}, \n",
    "               \"poly\": {'C': [1, 10, 100], 'gamma': ['scale', 0.001, 0.0001], 'kernel': ['poly'], 'coef0': [0.0, 0.1, 0.5, 1.0],'degree': [3, 5, 7, 9]}, \n",
    "               \"sigmoid\": {'C': [1, 10, 100], 'gamma': ['scale', 0.001, 0.0001], 'kernel': ['sigmoid'],'coef0': [0.0, 0.1, 0.5, 1.0]}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    os.makedirs(path+\"gridsearch_models/\", exist_ok=True)\n",
    "    os.makedirs(path+\"test_train_set/\", exist_ok=True)\n",
    "    os.makedirs(path+\"dataset/\", exist_ok=True)\n",
    "    os.makedirs(path+\"roc_curve/\", exist_ok=True)\n",
    "    os.makedirs(path+\"cm/\", exist_ok=True)\n",
    "    os.makedirs(path+\"report/\", exist_ok=True)\n",
    "    \n",
    "    conf_mat = []\n",
    "    top5_combinaison = []\n",
    "    exclude = {}\n",
    "    \n",
    "    for stratml in strat_ml:\n",
    "        strat_df_best_score = pd.DataFrame(columns=strat_ml[stratml].keys(), index=new_dbs.keys())\n",
    "        strat_df_score_test = pd.DataFrame(columns=strat_ml[stratml].keys(), index=new_dbs.keys())\n",
    "        strat_df_score_cross_val = pd.DataFrame(columns=strat_ml[stratml].keys(), index=new_dbs.keys())\n",
    "        strat_df_score_cross_val_std = pd.DataFrame(columns=strat_ml[stratml].keys(), index=new_dbs.keys())\n",
    "        for k in strat_ml[stratml]:\n",
    "            for options in new_dbs:\n",
    "                if len(top5_combinaison) == 0 or stratml+\"_\"+options in top5_combinaison:\n",
    "                    name_file = stratml+\"_\"+k+\"_\"+options+\"_score_\"+scoring\n",
    "                    print(name_file)\n",
    "\n",
    "                    X = new_dbs[options][0]\n",
    "                    y = new_dbs[options][1]\n",
    "                    group_list = new_dbs[options][-1]\n",
    "                    groupkfold = StratifiedGroupKFold(n_splits=3)\n",
    "                    if options+\".csv\" not in os.listdir(path+\"dataset\"):\n",
    "                        pd.concat([group_list, X, y], axis=1).to_csv(path + \"dataset/\" + options + \".csv\")\n",
    "                    \n",
    "                    \n",
    "                    best_params, best_score, score_on_test, model = grid_searches(X, y, SVC, \n",
    "                                                                               custom_scorer, strat_ml[stratml][k], \n",
    "                                                                               path, name_file, v= 3 , cv_grid=groupkfold, \n",
    "                                                                               cv_out_grid=groupkfold, group=group_list, basic_par={\"probability\": True})\n",
    "                    \n",
    "                    \n",
    "                    ####### cross val + CM  + roc curve + scores  + STD\n",
    "                    \n",
    "                    best_params[\"probability\"] = True\n",
    "                    model = SVC(**best_params)\n",
    "                    y = y[\"diagnostic\"]\n",
    "                    y_bin = label_binarize(y, classes=np.unique(y))\n",
    "\n",
    "                    splitting = groupkfold.split(X, y, group_list)\n",
    "\n",
    "                    conf_matrix_list_of_arrays = []\n",
    "                    scores = []\n",
    "\n",
    "                    # Initialiser les variables pour les courbes ROC et les matrices de confusion\n",
    "                    tprs = {i: [] for i in range(len(np.unique(y)))}\n",
    "                    mean_fpr = np.linspace(0, 1, 100)\n",
    "                    fig, ax = plt.subplots()\n",
    "                    df_scores = []\n",
    "                    for indice, (train_index, test_index) in enumerate(splitting):\n",
    "\n",
    "                        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "                        y_train, y_test = y_bin[train_index], y_bin[test_index]\n",
    "                        classifier = OneVsRestClassifier(model)\n",
    "                        classifier.fit(X_train, y_train)\n",
    "                        model.fit(X_train, y.iloc[train_index])\n",
    "                        # df_probs = pd.DataFrame(clf.predict_proba(X_test), columns=[i for i in clf.classes_])\n",
    "                        if hasattr(classifier, \"decision_function\"):\n",
    "                            y_pred_proba = classifier.decision_function(X_test)\n",
    "                        else:\n",
    "                            y_pred_proba = classifier.predict_proba(X_test)\n",
    "                        scores.append(f1_score(y.iloc[test_index], model.predict(X_test), average=\"weighted\"))\n",
    "\n",
    "                        # Calculer les courbes ROC pour chaque classe et déterminer les seuils optimaux\n",
    "                        fpr = dict()\n",
    "                        tpr = dict()\n",
    "                        thresholds = dict()\n",
    "                        roc_auc = dict()\n",
    "                        optimal_thresholds = []\n",
    "\n",
    "                        for i, lab in zip(range(len(np.unique(y))), [0,1,3,5]):\n",
    "                            fpr[i], tpr[i], thresholds[i] = roc_curve(y_test[:, i], y_pred_proba[:, i], )\n",
    "                            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "                            # Interpoler les tpr à la moyenne des fpr\n",
    "                            tprs[i].append(np.interp(mean_fpr, fpr[i], tpr[i]))\n",
    "                            tprs[i][-1][0] = 0.0\n",
    "\n",
    "                            # Calculer le point optimal pour chaque classe en utilisant Youden's J statistic\n",
    "                            optimal_idx = np.argmax(tpr[i] - fpr[i])\n",
    "                            optimal_threshold = thresholds[i][optimal_idx]\n",
    "                            optimal_thresholds.append(optimal_threshold)\n",
    "                        \n",
    "                        \n",
    "                        y_pred_adjusted = np.zeros_like(y_pred_proba)\n",
    "\n",
    "                        for i in range(len(np.unique(y))):\n",
    "                            y_pred_adjusted[:, i] = (y_pred_proba[:, i] >= optimal_thresholds[i]).astype(int)* (y_pred_proba[:, i])\n",
    "                        \n",
    "                        \n",
    "                        y_pred_labels = np.argmax(y_pred_adjusted, axis=1)\n",
    "                        y_pred_labels[y_pred_labels==3] = 5\n",
    "                        y_pred_labels[y_pred_labels==2] = 3\n",
    "\n",
    "                        conf_matrix = confusion_matrix(y.iloc[test_index], model.predict(X_test), labels=[0,1,3,5])\n",
    "\n",
    "                        conf_matrix_list_of_arrays.append(conf_matrix)\n",
    "                        \n",
    "                        \n",
    "                        report = classification_report(y.iloc[test_index], model.predict(X_test), labels=[0,1,3,5], output_dict=True)\n",
    "                        df_for_scoring = pd.DataFrame(report).transpose()\n",
    "\n",
    "                        new_line = [0]*(len(df_for_scoring.index)-2)\n",
    "                        score_auc = roc_auc_score(y_test, y_pred_proba, average=\"macro\", multi_class='ovr', labels=[0,1, 3,5])\n",
    "                        new_line.append(score_auc)\n",
    "                        score_auc = roc_auc_score(y_test, y_pred_proba, average=\"weighted\", multi_class='ovr', labels=[0,1,3,5])\n",
    "                        new_line.append(score_auc)\n",
    "                        df_for_scoring[\"auc\"] = new_line\n",
    "\n",
    "                        df_scores.append(df_for_scoring)\n",
    "                        \n",
    "                    \n",
    "                    # Tracer la moyenne des courbes ROC pour chaque classe\n",
    "                    for i, lab in zip(range(len(np.unique(y))), [0,1,3,5]):\n",
    "                        std_tpr = np.std(tprs[i], axis=0)\n",
    "                        mean_tpr = np.mean(tprs[i], axis=0)\n",
    "                        mean_tpr[-1] = 1.0\n",
    "                        mean_auc = auc(mean_fpr, mean_tpr)\n",
    "                        std_auc = np.std([auc(fpr, tpr) for fpr, tpr in zip([mean_fpr]*len(tprs[i]), tprs[i])])\n",
    "                        plt.plot(mean_fpr, mean_tpr, linestyle='-', linewidth=2, alpha=0.3,\n",
    "                                 label='Mean ROC class {0} (area = {1:0.2f} ± {2:0.2f})'.format(lab, mean_auc, std_auc))\n",
    "                        \n",
    "                    \n",
    "                    # Calculer la courbe ROC moyenne globale\n",
    "                    all_tprs = np.mean([np.mean(tprs[i], axis=0) for i in range(len(np.unique(y)))], axis=0)\n",
    "                    all_auc = auc(mean_fpr, all_tprs)\n",
    "\n",
    "                    std_tpr = np.std([np.std(tprs[i], axis=0) for i in range(len(np.unique(y)))], axis=0)\n",
    "                    auc_for_std = []\n",
    "                    for i in range(len(np.unique(y))):\n",
    "                        auc_for_std.append([auc(fpr, tpr) for fpr, tpr in zip([mean_fpr]*len(tprs[i]), tprs[i])])\n",
    "                    std_auc = np.std(auc_for_std)\n",
    "                    plt.plot(mean_fpr, all_tprs, color='b', linestyle='-', linewidth=2,\n",
    "                             label='Mean ROC all classes (area = {0:0.2f})'.format(all_auc))\n",
    "\n",
    "                    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "                    plt.xlim([0.0, 1.0])\n",
    "                    plt.ylim([0.0, 1.05])\n",
    "                    plt.xlabel('False Positive Rate')\n",
    "                    plt.ylabel('True Positive Rate')\n",
    "                    plt.title('Receiver Operating Characteristic')\n",
    "                    plt.legend(loc=\"lower right\")\n",
    "                    if name_file is not None:\n",
    "                        plt.savefig(path + \"roc_curve/\" + name_file + \"_roc_curve.png\")\n",
    "                    plt.show()\n",
    "                    plt.clf()\n",
    "\n",
    "                    sum_conf_mat = np.sum(conf_matrix_list_of_arrays, axis=0)\n",
    "                    \n",
    "                    plt.title(\"confusion matrix from RF\")\n",
    "                    # Calcul de la somme des éléments de chaque ligne\n",
    "                    row_sums = np.sum(sum_conf_mat, axis=1)\n",
    "\n",
    "                    # Mise à l'échelle en divisant chaque élément par la somme de sa ligne\n",
    "                    scaled_arr = sum_conf_mat / row_sums[:, np.newaxis]\n",
    "                    # cm = ConfusionMatrixDisplay(sum_conf_mat / np.sum(sum_conf_mat), display_labels=classes).plot()\n",
    "                    ConfusionMatrixDisplay(sum_conf_mat, display_labels=[0,1,3,5]).plot()\n",
    "\n",
    "                    plt.savefig(path + \"cm/\" + name_file + \"_cm.png\")\n",
    "                    plt.show()\n",
    "                    plt.clf()\n",
    "                    \n",
    "                    # Calculer la moyenne de chaque élément\n",
    "                    mean_df = pd.concat(df_scores).groupby(level=0).mean().round(3)\n",
    "\n",
    "                    # Calculer l'écart type de chaque élément\n",
    "                    std_df = pd.concat(df_scores).groupby(level=0).std().round(3)\n",
    "                    mean_df.to_csv(path + \"report/\" + name_file + \"_mean.csv\")\n",
    "                    std_df.to_csv(path + \"report/\" + name_file + \"_std.csv\")\n",
    "                    display(mean_df)\n",
    "                    print(\"coming from\")\n",
    "                    for lesdf in df_scores:\n",
    "                        display(lesdf)\n",
    "                    ################ fin cross val\n",
    "                    \n",
    "                    \n",
    "                    print(best_params)\n",
    "                    \n",
    "                    strat_df_best_score.at[options, k] = best_score\n",
    "                    strat_df_score_test.at[options, k] = score_on_test\n",
    "                    strat_df_score_cross_val.at[options, k] = sum(scores)/len(scores)\n",
    "                    strat_df_score_cross_val_std.at[options, k] = statistics.pstdev(scores)\n",
    "\n",
    "            \n",
    "        #for i in conf_mat:\n",
    "            #print(i[0])\n",
    "            #ConfusionMatrixDisplay(i[1], display_labels=i[2]).plot()\n",
    "\n",
    "        strat_df_best_score.dropna(inplace=True)\n",
    "        strat_df_score_test.dropna(inplace=True)\n",
    "        strat_df_score_cross_val.dropna(inplace=True)\n",
    "        strat_df_score_cross_val_std.dropna(inplace=True)\n",
    "\n",
    "        for i in strat_df_best_score.columns:\n",
    "            strat_df_best_score[i] = strat_df_best_score[i].astype(float)\n",
    "            strat_df_score_test[i] = strat_df_score_test[i].astype(float)\n",
    "            strat_df_score_cross_val[i] = strat_df_score_cross_val[i].astype(float)\n",
    "            strat_df_score_cross_val_std[i] = strat_df_score_cross_val_std[i].astype(float)\n",
    "\n",
    "        \n",
    "        display(strat_df_best_score)\n",
    "        display(strat_df_score_test)\n",
    "        display(strat_df_score_cross_val)\n",
    "        display(strat_df_score_cross_val_std)\n",
    "    \n",
    "        strat_df_best_score.to_csv(path+stratml+\"_best_scores_in_grid.csv\")\n",
    "        strat_df_score_test.to_csv(path+stratml+\"_score_on_test.csv\")\n",
    "        strat_df_score_cross_val.to_csv(path+stratml+\"_score_on_cross_val.csv\")\n",
    "        strat_df_score_cross_val_std.to_csv(path+stratml+\"_score_on_cross_val_std.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43418814",
   "metadata": {},
   "source": [
    "### Plot scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb534e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    path=\"./backup_models/Braak/SVM/\"+feat_selec+\"/\"+scoring\n",
    "    if feat_selec == \"with_fs\":\n",
    "        path=\"./backup_models/Braak/SVM/\"+feat_selec+\"/\"+str(target_nbr)+\"/\"+scoring\n",
    "\n",
    "    plot_scores(path, plots=[\"score_on_cross_val\"], v=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0905d4e3",
   "metadata": {},
   "source": [
    "### Learning Curve & XAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866cb873",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    path=\"./backup_models/Braak/SVM/\"+feat_selec+\"/\"+scoring\n",
    "    if feat_selec == \"with_fs\":\n",
    "        path=\"./backup_models/Braak/SVM/\"+feat_selec+\"/\"+str(target_nbr)+\"/\"+scoring\n",
    "        \n",
    "    \n",
    "    dir_grid = max([datetime.strptime(f, \"%d-%m-%Y_%Hh%Mm%Ss\") for f in files if os.path.isdir(path+'/'+f)]).strftime(\"%d-%m-%Y_%Hh%Mm%Ss\")\n",
    "    for scores in os.listdir(path+\"/\"+dir_grid):\n",
    "        if \"score_on_cross_val\" in scores:\n",
    "            strat_df_score_cross_val = pd.read_csv(path+\"/\"+dir_grid+\"/\"+scores, index_col=0)\n",
    "            break\n",
    "    \n",
    "    max_value = strat_df_score_cross_val.max(axis=1).max()\n",
    "    targets = strat_df_score_cross_val[strat_df_score_cross_val.eq(max_value).any(axis=1)].index.tolist()\n",
    "    print(\"targets: \"+str(targets))\n",
    "    \n",
    "    groupkfold = StratifiedGroupKFold(n_splits=5)\n",
    "    #plot_learning_curves(new_dbs, path, targets, 5, 3, v=True, scoring=f1_score, binary=False)\n",
    "    XAI_analyses(new_dbs, path, targets, custom_scorer, v=True, nsplits_exp=5, shap=False)\n",
    "    plot_confusion_matrices(new_dbs, path, targets, groupkfold,scores_options[scoring], binary, [0,1, 3, 5], [1], summat=False)\n",
    "    plot_roc_curve(new_dbs, path, targets, groupkfold)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee03688",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
