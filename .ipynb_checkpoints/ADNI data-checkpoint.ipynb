{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9106346",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import chardet\n",
    "import sqlalchemy\n",
    "import matplotlib.pyplot as plt\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.metrics import roc_auc_score, precision_score, classification_report, recall_score, ConfusionMatrixDisplay, precision_recall_fscore_support, confusion_matrix\n",
    "from utils_mem import *\n",
    "import mysql.connector\n",
    "from mysql.connector import FieldType\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "import numpy as np\n",
    "import unidecode\n",
    "from bs4 import BeautifulSoup\n",
    "from mysql.connector.cursor import MySQLCursor\n",
    "import sys\n",
    "import requests\n",
    "from IPython.display import clear_output\n",
    "import re\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, RepeatedStratifiedKFold, LeaveOneGroupOut, StratifiedGroupKFold\n",
    "import statistics \n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "path_adni = \"./data/csv/ADNI\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3337e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    csv = {\"NeuroPsy/ADAS_ADNI1_07Mar2024_Alzheimer's Disease Assessment Scale (ADAS) [ADNI1].csv\": 'ADAS', \n",
    "            'NeuroPsy/ADASSCORES_07Mar2024_ADAS Sub-Scores and Total Scores [ADNI1].csv': 'ADASSCORES', \n",
    "            'NeuroPsy/MOCA_07Mar2024_Montreal Cognitive Assessment (MoCA) [ADNIGO,2,3].csv': 'MOCA', \n",
    "            'NeuroPsy/ADNI_EMBICDCB_07Mar2024_Embic Corporation â€“ Digital Cognitive Biomarkers [ADNI1,GO,2,3].csv': 'EMBICDCB',\n",
    "            'NeuroPsy/MMSE_07Mar2024_Mini-Mental State Examination (MMSE) [ADNI1,GO,2,3].csv': 'MMSE', \n",
    "            'NeuroPsy/UWNPSYCHSUM_07Mar2024_UW - Neuropsych Summary Scores [ADNI1,GO,2,3].csv': 'UWNPSYCHSUM', \n",
    "            'NeuroPsy/EMBICqCP_07Mar2024_Embic Corporation Quantified Cognitive Processes [ADNI1,GO,2,3].csv': 'EMBICDCB', \n",
    "            'NeuroPsy/NEUROBAT_07Mar2024_EMBICDCB.csv': 'NEUROBAT', # going with UWNPSYCHSUM\n",
    "            'biospecimen/ADMCGUTMETABOLITESLONG_12_13_21_07Mar2024_Hawaii Gut Metabolites Serum Longitudinal [ADNI1,GO,2].csv': 'ADMCGUTMETABOLITESLONG',\n",
    "            \"demograph/PTDEMOG_11Mar2024.csv\": \"PTDEMOG\", \n",
    "           \"NeuroPsy/MMSE_Mini-Mental State Examination (MMSE) [ADNI1,GO,2,3,4].csv\": \"MMSE\",\n",
    "           \"MRI/UCSFFSX51_11_08_19_13May2024_UCSF - Cross-Sectional FreeSurfer (5.1) [ADNI1,GO,2].csv\": \"UCSFFSX51\"\n",
    "          }\n",
    "    chosen = -1\n",
    "    hasdate=True\n",
    "    url = \"https://adni.loni.usc.edu/data-dictionary-search/?q=DXPATYP\"\n",
    "    pattern = \"E:/Celia/data/csv/ADNI/\" + list(csv.keys())[chosen]\n",
    "    print(pattern)\n",
    "    print(csv[list(csv.keys())[chosen]])\n",
    "    if len(pattern) == 0:\n",
    "        for dire in os.listdir(path_adni):\n",
    "            for file in os.listdir(path_adni+\"/\"+dire):\n",
    "                print(path_adni+\"/\"+dire+\"/\"+file)\n",
    "                df = pd.read_csv(path_adni+\"/\"+dire+\"/\"+file)\n",
    "                display(df)\n",
    "                go=False\n",
    "                meet_date=False\n",
    "                for col in df.columns:\n",
    "                    if meet_date and go and \"id\" not in col.lower():\n",
    "                        print(col)\n",
    "                        df_display = get_info_col(col, file.split(\"_\")[0])\n",
    "                        if \"aparc\" not in df_display.loc[1, \"Definition\"] and \"aseg\" not in df_display.loc[1, \"Definition\"]:\n",
    "                            display(df_display)\n",
    "                    elif meet_date:\n",
    "                        if \"date\" not in col.lower():\n",
    "                            go=True\n",
    "                    else:\n",
    "                        if \"date\" in col.lower():\n",
    "                            meet_date=True\n",
    "    else:\n",
    "        file = pattern.split(\"/\")[-1]\n",
    "        df = pd.read_csv(pattern, skiprows=1)\n",
    "        display(df)\n",
    "        go=False\n",
    "        meet_date=False\n",
    "        for col in df.columns:\n",
    "            if (meet_date and go) or (not hasdate):\n",
    "                print(col)\n",
    "                df_display = get_info_col(col, csv[list(csv.keys())[chosen]])\n",
    "                df_display.reset_index(inplace=True)\n",
    "                if not pd.isna(df_display.loc[0, \"Definition\"]):\n",
    "                    if \"aparc\" not in df_display.loc[0, \"Definition\"] and \"aseg\" not in df_display.loc[0, \"Definition\"]:\n",
    "                        display(df_display)\n",
    "            elif meet_date:\n",
    "                if \"date\" not in col.lower():\n",
    "                    go=True\n",
    "            else:\n",
    "                if \"date\" in col.lower():\n",
    "                    meet_date=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d583d48",
   "metadata": {},
   "source": [
    "# Diagnosis set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb658bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diag_pre(s):\n",
    "    if s[\"cog_diag\"] == \"CN\" and s[\"AMYLOID_STATUS\"] == 1:\n",
    "        return 1\n",
    "    elif pd.isna(s[\"cog_diag\"]):\n",
    "        return s[\"cog_diag\"]\n",
    "    elif pd.isna(s[\"AMYLOID_STATUS\"]):\n",
    "        return s[\"AMYLOID_STATUS\"]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def diag_pre_composite(s):\n",
    "    if s[\"cog_diag\"] == \"CN\" and s[\"AMYLOID_STATUS_COMPOSITE_REF\"] == 1:\n",
    "        return 1\n",
    "    elif pd.isna(s[\"cog_diag\"]):\n",
    "        return s[\"cog_diag\"]\n",
    "    elif pd.isna(s[\"AMYLOID_STATUS_COMPOSITE_REF\"]):\n",
    "        return s[\"AMYLOID_STATUS_COMPOSITE_REF\"]\n",
    "    else:\n",
    "        return 0\n",
    "diagnosis = pd.read_csv(\"E:/Celia/data/csv/ADNI/diagnosis/DXSUM_PDXCONV_ADNIALL_22Feb2024_diag and changes.csv\", skiprows=1, usecols=[\"RID\", \"VISDATE\", \"DXCURREN\", \"DXCHANGE\", \"DXCONV\", \"DIAGNOSIS\", \"VISCODE2\"])\n",
    "diagnosis.columns = diagnosis.columns.map(lambda x : x+'_diag' if x !='RID' else x)\n",
    "\n",
    "display(diagnosis)\n",
    "diagnosis[\"cog_diag\"] = diagnosis[\"DXCHANGE_diag\"].copy().map({1: \"CN\", 2: \"MCI\", 3: \"DEM\", 4: \"MCI\", 5: \"DEM\", 6: \"DEM\", 7: \"CN\", 8: \"MCI\", 9: \"CN\"})\n",
    "\n",
    "display(diagnosis)\n",
    "diagnosis[\"cog_diag\"] = diagnosis[\"cog_diag\"].fillna(diagnosis[\"DXCURREN_diag\"].copy().map({1: \"CN\", 2: \"MCI\", 3: \"DEM\"}))\n",
    "display(diagnosis)\n",
    "diagnosis = diagnosis[diagnosis[\"cog_diag\"] == \"CN\"]\n",
    "display(diagnosis)\n",
    "\n",
    "amy = pd.read_csv(\"E:/Celia/data/csv/ADNI/Amy/UCBERKELEY_AMY_6MM_22Feb2024.csv\", skiprows=1, usecols=[\"RID\", \"SCANDATE\", \"PROCESSDATE\", \"AMYLOID_STATUS\", \"AMYLOID_STATUS_COMPOSITE_REF\"])\n",
    "amy[\"AMYLOID_STATUS_COMPOSITE_REF\"] = amy[\"AMYLOID_STATUS_COMPOSITE_REF\"].astype(\"Int64\")\n",
    "amy[\"AMYLOID_STATUS\"] = amy[\"AMYLOID_STATUS\"].astype(\"Int64\")\n",
    "\n",
    "df = pd.merge(diagnosis[[\"RID\", \"VISDATE_diag\", \"cog_diag\"]], amy, on=[\"RID\"], how=\"inner\", suffixes=(\"_diag\", \"_amy\"))\n",
    "display(df)\n",
    "\n",
    "df.replace(to_replace = \"-1\", value = np.nan, inplace = True)\n",
    "df.replace(to_replace = \"-4\", value = np.nan, inplace = True)\n",
    "df.replace(to_replace = -1, value = np.nan, inplace = True)\n",
    "df.replace(to_replace = -4, value = np.nan, inplace = True)\n",
    "df[[\"VISDATE_diag\", \"SCANDATE\"]] = make_dates(df[[\"VISDATE_diag\", \"SCANDATE\"]])\n",
    "\n",
    "df = date_within(df, 365, [\"VISDATE_diag\", 'SCANDATE'])\n",
    "\n",
    "display(df)\n",
    "\n",
    "df['diag'] = df.apply(diag_pre, axis=1)\n",
    "df['diag_composite'] = df.apply(diag_pre_composite, axis=1)\n",
    "df.dropna(subset=[\"diag\", \"diag_composite\"], how=\"any\", inplace=True)\n",
    "\n",
    "for x in df.columns:\n",
    "    nan = df[x].isna().sum()\n",
    "    if nan > 0:\n",
    "        print(x+\" with \"+str(nan)+\" # of nan\")\n",
    "        \n",
    "df.dropna(axis=1, inplace=True)\n",
    "\n",
    "print(df[\"diag\"].value_counts())\n",
    "print(df[\"diag_composite\"].value_counts())\n",
    "df.rename(columns={\"PTID_diag\": \"PTID\", \"EXAMDATE_diag\": \"EXAMDATE\"}, inplace=True)\n",
    "\n",
    "print(\"final:\")\n",
    "diag = df[[\"RID\", \"VISDATE_diag\", \"SCANDATE\", \"diag\", \"diag_composite\"]].copy()\n",
    "\n",
    "diag[\"diag\"] = diag[\"diag\"].astype(int)\n",
    "diag[\"diag_composite\"] = diag[\"diag_composite\"].astype(int)\n",
    "display(diag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b802e9",
   "metadata": {},
   "source": [
    "## Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab437e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "APOE = pd.read_csv(\"E:/Celia/data/csv/ADNI/APOE/YASSINE_PLASMA_15Feb2024.csv\", skiprows=1, usecols=[\"RID\", \"Phenotype\"])\n",
    "APOE.dropna(subset=[\"Phenotype\"], inplace=True)\n",
    "APOE[\"Phenotype\"] = APOE[\"Phenotype\"].apply(lambda x: ''.join(c for c in x if c.isdigit()))\n",
    "APOE[\"Phenotype\"] = APOE[\"Phenotype\"].astype(int)\n",
    "\n",
    "feat = pd.read_csv(\"E:/Celia/data/csv/ADNI/demograph/PTDEMOG_11Mar2024.csv\", skiprows=1, \n",
    "                   usecols=[\"RID\", \"PTGENDER\", \"PTEDUCAT\", \"PTDOBYY\", \"VISDATE\"])\n",
    "\n",
    "feat = pd.merge(APOE, feat, on=\"RID\", how=\"inner\")\n",
    "feat.replace(-4, np.nan, inplace=True)\n",
    "feat.replace(-1, np.nan, inplace=True)\n",
    "feat.dropna(inplace=True)\n",
    "feat[\"PTGENDER\"] = feat[\"PTGENDER\"].astype(int)\n",
    "feat[\"PTEDUCAT\"] = feat[\"PTEDUCAT\"].astype(int)\n",
    "feat[\"PTDOBYY\"] = feat[\"PTDOBYY\"].astype(int)\n",
    "feat.rename(columns={\"Phenotype\": \"APOE\"}, inplace=True)\n",
    "print(feat.columns)\n",
    "display(feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d770f26",
   "metadata": {},
   "source": [
    "# IRM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7420b7db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "irm1 = pd.read_csv(\"E:/Celia/data/csv/ADNI/MRI/UCSFFSL51_03_01_22_18May2024_UCSF - Longitudinal FreeSurfer (5.1) - Final Run w All A [ADNIGO,2].csv\", skiprows=1)\n",
    "display(irm1)\n",
    "irm1.drop(columns={\"COLPROT\", \"VISCODE\", \"VERSION\", \"LONISID\", \"LONIUID\", \n",
    "                   \"IMAGEUID\", \"RUNDATE\", \"STATUS\", \"RUNDATE\", \"OVERALLQC\", \"TEMPQC\", \"FRONTQC\", \"PARQC\", \"INSULAQC\", \n",
    "                   \"OCCQC\", \"BGQC\", \"CWMQC\", \"VENTQC\", \"LHIPQC\", \"RHIPQC\", \"update_stamp\"}, \n",
    "          inplace=True)\n",
    "\n",
    "irm1 = supp_col(irm1, 0.3, verbose=True)\n",
    "\n",
    "for x in irm1.columns:\n",
    "    nan = irm1[x].isna().sum()\n",
    "    if nan > 0:\n",
    "        print(x+\" with \"+str(nan)+\" # of nan\")\n",
    "irm1.dropna(inplace=True)\n",
    "\n",
    "ren = {}\n",
    "\"\"\"for i in irm1.columns:\n",
    "    if i != \"RID\" and \"date\" not in i.lower() and i.lower() != \"viscode2\":\n",
    "        ren[i] = \"irmL_\"+i\"\"\"\n",
    "irm1.rename(columns=ren, inplace=True)\n",
    "display(irm1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70135ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "irm2 = pd.read_csv(\"E:/Celia/data/csv/ADNI/MRI/UCSFFSX51_11_08_19_13May2024_UCSF - Cross-Sectional FreeSurfer (5.1) [ADNI1,GO,2].csv\", skiprows=1)\n",
    "\n",
    "irm2.drop(columns={\"COLPROT\", \"VISCODE\", \"VERSION\", \"LONISID\", \"LONIUID\", \"IMAGEUID\", \"IMAGETYPE\", \"RUNDATE\", \"STATUS\", \n",
    "                   \"update_stamp\", \"OVERALLQC\", \"TEMPQC\", \"FRONTQC\", \"PARQC\", \"INSULAQC\", \"OCCQC\", \"BGQC\", \"CWMQC\", \"VENTQC\", \"LHIPQC\", \"RHIPQC\"}, \n",
    "          inplace=True)\n",
    "\n",
    "\n",
    "ren = {}\n",
    "\"\"\"for i in irm2.columns:\n",
    "    if i != \"RID\" and \"date\" not in i.lower() and i.lower() != \"viscode2\":\n",
    "        ren[i] = \"irmC_\"+i\"\"\"\n",
    "irm2.rename(columns=ren, inplace=True)\n",
    "irm2 = supp_col(irm2, 0.3, verbose=True)\n",
    "irm2.dropna(inplace=True)\n",
    "irm2 = supp_col(irm2, 0.2, verbose=True)\n",
    "display(irm2)\n",
    "#irm1 = irm2.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3105ab3",
   "metadata": {},
   "source": [
    "# NeuroPsy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3978f403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ceux en commun avec DB\n",
    "neuro = pd.read_csv(\"E:/Celia/data/csv/ADNI/NeuroPsy/NEUROBAT_17May2024_Neuropsychological Battery [ADNI1,GO,2,3].csv\", skiprows=1)\n",
    "\n",
    "neuro.replace(-4, np.nan, inplace=True)\n",
    "neuro.replace(-1, np.nan, inplace=True)\n",
    "neuro.replace(\"-4\", np.nan, inplace=True)\n",
    "neuro.replace(\"-1\", np.nan, inplace=True)\n",
    "neuro = neuro.drop(columns={\"Phase\", \"ID\", \"PTID\", \"SITEID\", \"USERDATE\", \"USERDATE2\", \"VISDATE\", \"update_stamp\", \"DATE\", \"SOURCE\"})\n",
    "display(neuro)\n",
    "neuro = supp_row(neuro, 0.8, verbose=True, excet=[\"RID\", \"VISCODE\", \"VISCODE2\", \"EXAMDATE\"])\n",
    "display(neuro)\n",
    "neuro = supp_col(neuro, 0.3, verbose=True, excet=[\"EXAMDATE\"])\n",
    "display(neuro)\n",
    "\"\"\"\n",
    "#counting nbr of nan per column\n",
    "percent_missing = neuro.isnull().sum() * 100 / len(neuro)\n",
    "missing_value_df = pd.DataFrame({'column_name': neuro.columns,\n",
    "                                 'percent_missing': percent_missing})\n",
    "display(missing_value_df)\"\"\"\n",
    "\n",
    "neuro.dropna(inplace=True)\n",
    "neuro.drop(columns=[\"VISCODE\"], inplace=True)\n",
    "\n",
    "\n",
    "ren = {}\n",
    "for i in neuro.columns:\n",
    "    if i != \"RID\" and i != \"VISCODE2\":\n",
    "        ren[i] = \"neurobat_\"+i\n",
    "    if \"date\" in i.lower():\n",
    "        print(i)\n",
    "neuro.rename(columns=ren, inplace=True)\n",
    "display(neuro)\n",
    "neuro_bat = neuro.copy()\n",
    "#new_df = pd.merge(diag, irm1, on=[\"RID\"], how=\"inner\", suffixes=(\"\", \"_irm\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292533f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ceux en commun avec DB\n",
    "neuro_comp = pd.read_csv(\"E:/Celia/data/csv/ADNI/NeuroPsy/UWNPSYCHSUM_07Mar2024_UW - Neuropsych Summary Scores [ADNI1,GO,2,3].csv\", skiprows=1)\n",
    "neuro_comp.drop(columns={\"VISCODE\", \"update_stamp\", \"PHASE\"}, inplace=True)\n",
    "\n",
    "neuro_comp.replace(-4, np.nan, inplace=True)\n",
    "neuro_comp.replace(-1, np.nan, inplace=True)\n",
    "neuro_comp.replace(\"-4\", np.nan, inplace=True)\n",
    "neuro_comp.replace(\"-1\", np.nan, inplace=True)\n",
    "display(neuro_comp)\n",
    "neuro_comp = supp_col(neuro_comp, 0.3, verbose=True, excet=[\"RID\", \"EXAMDATE\"])\n",
    "\n",
    "ren = {}\n",
    "for i in neuro_comp.columns:\n",
    "    if i != \"RID\" and i != \"VISCODE2\":\n",
    "        ren[i] = \"neuro_comp_\"+i\n",
    "print(ren)\n",
    "neuro_comp.rename(columns=ren, inplace=True)\n",
    "\n",
    "display(neuro_comp)\n",
    "\n",
    "#new_df = pd.merge(diag, irm1, on=[\"RID\"], how=\"inner\", suffixes=(\"\", \"_irm\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c2dbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ceux en commun avec DB\n",
    "neuro_mmse = pd.read_csv(\"E:/Celia/data/csv/ADNI/NeuroPsy/MMSE_Mini-Mental State Examination (MMSE) [ADNI1,GO,2,3,4].csv\", skiprows=1)\n",
    "make_bin = [\"MMDATE\", \"MMYEAR\", \"MMMONTH\", \"MMDAY\", \"MMSEASON\", \"MMHOSPIT\", \"MMFLOOR\", \"MMCITY\", \"MMAREA\", \"MMSTATE\", \"WORD1\", \"WORD2\", \"WORD3\", \"MMD\", \"MML\", \"MMR\", \"MMO\", \"MMW\", \"WORD1DL\", \"WORD2DL\", \"WORD3DL\", \"MMWATCH\", \"MMPENCIL\", \"MMREPEAT\", \"MMHAND\", \"MMFOLD\", \"MMONFLR\", \"MMREAD\", \"MMWRITE\", \"MMDRAW\" ]\n",
    "\n",
    "neuro_mmse.replace(-4, np.nan, inplace=True)\n",
    "neuro_mmse.replace(-1, np.nan, inplace=True)\n",
    "\n",
    "neuro_mmse = neuro_mmse[neuro_mmse[\"DONE\"] == 1]\n",
    "neuro_mmse.dropna(subset=[\"VISDATE\"], inplace=True)\n",
    "neuro_mmse.drop(columns={\"VISCODE\", \"PTID\", \"update_stamp\", \"PHASE\", \"NDREASON\", \"SOURCE\", \"DATE\", \"DONE\"}, inplace=True)\n",
    "neuro_mmse.drop(columns=neuro_mmse.columns[-7:], inplace=True)\n",
    "\n",
    "print(neuro_mmse.columns)\n",
    "display(neuro_mmse)\n",
    "for col in make_bin:\n",
    "    # drop empty columns\n",
    "    if len(neuro_mmse[col].unique()) < 2:\n",
    "        neuro_mmse.drop(columns=[col], inplace=True)\n",
    "    else:\n",
    "        #display(neuro_mmse[[col]])\n",
    "        origin = neuro_mmse.columns.get_loc(col)\n",
    "        neuro_mmse[col] = neuro_mmse[col].astype(\"Int64\")\n",
    "        neuro_mmse = pd.get_dummies(neuro_mmse, columns=[col])\n",
    "        dropping = [x for x in neuro_mmse if col+\"_\" in x and x != col+\"_1\"]\n",
    "        #print(dropping)\n",
    "        neuro_mmse.drop(columns=dropping, inplace=True)\n",
    "        neuro_mmse.rename(columns={col+\"_1\": col}, inplace=True)\n",
    "\n",
    "        # reput the column at its initial spot\n",
    "        #display(neuro_mmse)\n",
    "        col2replace = neuro_mmse.pop(col)\n",
    "        neuro_mmse.insert(origin, col2replace.name, col2replace)\n",
    "    \n",
    "neuro_mmse = supp_col(neuro_mmse, 0.3, verbose=True, excet=[\"RID\", \"VISDATE\"])\n",
    "\n",
    "ren = {}\n",
    "for i in neuro_mmse.columns:\n",
    "    if i != \"RID\" and i != \"VISCODE2\":\n",
    "        ren[i] = \"neuro_mmse_\"+i\n",
    "print(ren)\n",
    "neuro_mmse.rename(columns=ren, inplace=True)\n",
    "\n",
    "display(neuro_mmse)\n",
    "neuro_mmse.dropna(inplace=True)\n",
    "display(neuro_mmse)\n",
    "#new_df = pd.merge(diag, irm1, on=[\"RID\"], how=\"inner\", suffixes=(\"\", \"_irm\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edad49b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ceux en commun avec DB\n",
    "\n",
    "neuro_ecog = pd.read_csv(\"E:/Celia/data/csv/ADNI/NeuroPsy/ECOGPT_17May2024_Everyday Cognition - Participant Self Report [ADNIGO,2,3].csv\", skiprows=1)\n",
    "\n",
    "neuro_ecog.replace(-4, np.nan, inplace=True)\n",
    "neuro_ecog.replace(-1, np.nan, inplace=True)\n",
    "neuro_ecog.replace(\"-4\", np.nan, inplace=True)\n",
    "neuro_ecog.replace(\"-1\", np.nan, inplace=True)\n",
    "\n",
    "neuro_ecog.drop(columns={\"VISCODE\", \"PTID\", \"ID\", \"SITEID\", \"Phase\", \"USERDATE\", \"USERDATE2\", \"CONCERN\"}, inplace=True)\n",
    "neuro_ecog.drop(columns=neuro_ecog.columns[-10:], inplace=True)\n",
    "\n",
    "display(neuro_ecog)\n",
    "\n",
    "makeint = [x for x in neuro_ecog if \"memory\" in x.lower() or \"lang\" in x.lower() or \"visspat\" in x.lower() or \"plan\" in x.lower() or \"organ\" in x.lower() or \"divatt\" in x.lower()]\n",
    "makeint.append(\"STAFFASST\")\n",
    "makeint.append(\"VALIDITY\")\n",
    "for col in makeint:\n",
    "        neuro_ecog[col] = neuro_ecog[col].astype(\"Int64\")\n",
    "    \n",
    "neuro_ecog = supp_col(neuro_ecog, 0.3, verbose=True, excet=[\"RID\", \"VISDATE\"])\n",
    "\n",
    "ren = {}\n",
    "for i in neuro_ecog.columns:\n",
    "    if i != \"RID\" and i != \"VISCODE2\":\n",
    "        ren[i] = \"neuro_ecog_\"+i\n",
    "print(ren)\n",
    "neuro_ecog.rename(columns=ren, inplace=True)\n",
    "\n",
    "display(neuro_ecog)\n",
    "neuro_ecog.dropna(inplace=True)\n",
    "display(neuro_ecog)\n",
    "#new_df = pd.merge(diag, irm1, on=[\"RID\"], how=\"inner\", suffixes=(\"\", \"_irm\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac583af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "neuro_moca = pd.read_csv(\"E:/Celia/data/csv/ADNI/NeuroPsy/MOCA_07Mar2024_Montreal Cognitive Assessment (MoCA) [ADNIGO,2,3].csv\", skiprows=1)\n",
    "\n",
    "neuro_moca.replace(-4, np.nan, inplace=True)\n",
    "neuro_moca.replace(-1, np.nan, inplace=True)\n",
    "neuro_moca.replace(\"-4\", np.nan, inplace=True)\n",
    "neuro_moca.replace(\"-1\", np.nan, inplace=True)\n",
    "display(neuro_moca)\n",
    "neuro_moca.drop(columns={\"VISCODE\", \"PTID\", \"ID\", \"SITEID\", \"Phase\", \"USERDATE\", \"USERDATE2\", \"SOURCE\", \"MOCA\", \"MCDATE\", \"update_stamp\"}, inplace=True)\n",
    "#neuro_moca.drop(columns=neuro_ecog.columns[-10:], inplace=True)\n",
    "neuro_moca[neuro_moca.columns[3:]] = neuro_moca[neuro_moca.columns[3:]].astype(\"Int64\")\n",
    "display(neuro_moca)\n",
    "\n",
    "\n",
    "ren = {}\n",
    "for i in neuro_moca.columns:\n",
    "    if i != \"RID\" and i != \"VISCODE2\":\n",
    "        ren[i] = \"neuro_moca_\"+i\n",
    "print(ren)\n",
    "neuro_moca.rename(columns=ren, inplace=True)\n",
    "\n",
    "neuro_moca = supp_col(neuro_moca, 0.3, verbose=True, excet=[\"RID\", \"neuro_moca_VISDATE\"])\n",
    "\n",
    "display(neuro_moca)\n",
    "neuro_moca.dropna(inplace=True)\n",
    "display(neuro_moca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d452f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mix up NEURO\n",
    "display(neuro_comp)\n",
    "display(neuro_mmse)\n",
    "display(neuro_bat)\n",
    "print(\"by merging\")\n",
    "df_neuro_final = pd.merge(neuro_ecog.copy(), neuro_comp.copy(), on=[\"RID\", \"VISCODE2\"], suffixes=(\"_neuro_ecog\", \"_neuro_comp\"), how='inner')\n",
    "df_neuro_final = pd.merge(df_neuro_final, neuro_moca.copy(), on=[\"RID\", \"VISCODE2\"], suffixes=(\"\", \"_neuro_moca\"), how='inner')\n",
    "print(\"neuro ecog + neuro comp + neuro moca\")\n",
    "display(df_neuro_final[[\"RID\", \"VISCODE2\"]+[x for x in df_neuro_final.columns if \"date\" in x.lower() and x != \"neuro_mmse_MMDATE\"]])\n",
    "supp_col(df_neuro_final, 0.3, verbose=True)\n",
    "df_neuro_final.dropna(inplace=True)\n",
    "supp_col(df_neuro_final, 0.3, verbose=True)\n",
    "display(df_neuro_final[[\"RID\", \"VISCODE2\"]+[x for x in df_neuro_final.columns if \"date\" in x.lower() and x != \"neuro_mmse_MMDATE\"]])\n",
    "dates_d = [x for x in df_neuro_final.columns if \"date\" in x.lower() and x != \"neuro_mmse_MMDATE\"]\n",
    "display((df_neuro_final[df_neuro_final[dates_d].nunique(axis=1) == len(dates_d)])[[\"RID\", \"VISCODE2\"]+dates_d])\n",
    "#df_neuro_final = neuro_comp.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da35ef84",
   "metadata": {},
   "source": [
    "## MIX-UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9499042a",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"comps\"\n",
    "dfs = {}\n",
    "for irms in [[\"irm1\", irm1], [\"irm2\", irm2]]:\n",
    "    df = pd.merge(diag.copy(), irms[1].copy(), on=[\"RID\"], suffixes=(\"_diag\", \"_irm1\"))\n",
    "    d = [x for x in df.columns if \"visdate\" in x.lower() or \"examdate\" in x.lower() or \"scandate\" in x.lower()]\n",
    "    df[d] = make_dates(df[d])\n",
    "    print(d)\n",
    "    df = date_within(df, 365, d)\n",
    "    \n",
    "    df = pd.merge(df, df_neuro_final.copy(), on=[\"RID\"], suffixes=(\"\", \"_neuro_final\"))\n",
    "    d = [x for x in df.columns if \"visdate\" in x.lower() or \"examdate\" in x.lower() or \"scandate\" in x.lower()]\n",
    "    df[d] = make_dates(df[d])\n",
    "    print(d)\n",
    "    df = date_within(df, 365, d)\n",
    "    \"\"\"\n",
    "    df = pd.merge(df, neuro_comp.copy(), on=[\"RID\"], suffixes=(\"\", \"_neuro_ref\"))\n",
    "    d = [x for x in df.columns if \"visdate\" in x.lower() or \"examdate\" in x.lower()]\n",
    "    df[d] = make_dates(df[d])\n",
    "    print(d)\n",
    "    df = date_within(df, 365, d)\n",
    "    df = pd.merge(df, neuro_ecog.copy(), on=[\"RID\"], suffixes=(\"\", \"_neuro_ecog\"))\n",
    "    d = [x for x in df.columns if \"visdate\" in x.lower() or \"examdate\" in x.lower()]\n",
    "    df[d] = make_dates(df[d])\n",
    "    print(d)\n",
    "    df = date_within(df, 365, d)\n",
    "    df = pd.merge(df, neuro_moca.copy(), on=[\"RID\"], suffixes=(\"\", \"_neuro_moca\"))\n",
    "    d = [x for x in df.columns if \"visdate\" in x.lower() or \"examdate\" in x.lower()]\n",
    "    df[d] = make_dates(df[d])\n",
    "    print(d)\n",
    "    df = date_within(df, 365, d)\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.merge(df, feat.copy(), on=[\"RID\"], suffixes=(\"\", \"_feat\"))\n",
    "    \n",
    "    print(\"after merging\")\n",
    "    \n",
    "    df.drop(columns=[x for x in df.columns if \"viscode2\" in x.lower()], inplace=True)\n",
    "\n",
    "\n",
    "    for i in df[\"diag\"].unique():\n",
    "        print(str(i)+\" has \"+str(len(df[df[\"diag\"] == i])))\n",
    "    for i in df[\"diag_composite\"].unique():\n",
    "        print(str(i)+\" has \"+str(len(df[df[\"diag_composite\"] == i])))\n",
    "\n",
    "    print(\"# of patients: \"+str(len(df[\"RID\"].unique())))\n",
    "\n",
    "    dates = [x for x in df.columns if \"visdate\" in x.lower() or \"examdate\" in x.lower() or \"scandate\" in x.lower()]\n",
    "    display(df[[\"RID\"]+dates])\n",
    "    \n",
    "    df[\"age\"] = df[dates[0]].dt.year - df[\"PTDOBYY\"]\n",
    "\n",
    "    #for i in dates:\n",
    "    #    df.drop_duplicates(subset=[\"RID\", i], inplace=True)\n",
    "\n",
    "    if target == \"comp\":\n",
    "        df.drop(columns={\"diag\"}.union(set(dates)), inplace=True)\n",
    "        df = df.rename(columns={\"diag_composite\": \"diagnostic\"})\n",
    "    else:\n",
    "        df.drop(columns={\"diag_composite\"}.union(set(dates)), inplace=True)\n",
    "        df = df.rename(columns={\"diag\": \"diagnostic\"})\n",
    "    df.drop(columns={\"PTGENDER\", \"PTEDUCAT\", \"PTDOBYY\"}, inplace=True)\n",
    "    display(df)\n",
    "\n",
    "    print(df[\"diagnostic\"].value_counts())\n",
    "    display(df)\n",
    "    dfs[irms[0]] = df.copy()\n",
    "    print([x for x in df.columns if \"date\" in x.lower()])\n",
    "    fs = SelectKBest(score_func=f_classif, k=20)\n",
    "    fs.fit(df.drop(columns=[\"diagnostic\"]), df[\"diagnostic\"])\n",
    "    df_fs = pd.DataFrame({\"name\": fs.feature_names_in_, \"scores\": fs.scores_, \"p-value\": fs.pvalues_})\n",
    "    print(\"TOP 5 features\")\n",
    "    display(df_fs.sort_values(by=\"scores\", ascending=False).head(5))\n",
    "print(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897f982a",
   "metadata": {},
   "source": [
    "# Delete correlated columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cbda8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supposons que df soit votre DataFrame\n",
    "# Vous devez avoir calculÃ© prÃ©alablement la matrice de corrÃ©lation corr_matrix\n",
    "correlated_pairs = set()\n",
    "for data_strat, prefix in [[\"irm1\", \"irmL_\"], [\"irm2\", \"irmC_\"]]:\n",
    "    df_excel = dfs[data_strat].copy()\n",
    "    display(df_excel)\n",
    "    corr_matrix = df_excel.corr()\n",
    "    # Trouver les paires de colonnes corrÃ©lÃ©es Ã  plus de 90%\n",
    "    if len(correlated_pairs) == 0:\n",
    "        for i in range(len(corr_matrix.columns)):\n",
    "            for j in range(i+1, len(corr_matrix.columns)):\n",
    "                if abs(corr_matrix.iloc[i, j]) > 0.9:\n",
    "                    correlated_pairs.add((\"\".join(corr_matrix.columns[i].split(prefix)), \"\".join(corr_matrix.columns[j].split(prefix))))\n",
    "\n",
    "    # Supprimer une colonne de chaque paire et renommer la colonne restante\n",
    "    for col1, col2 in correlated_pairs:\n",
    "        if prefix+col1 in df_excel.columns and prefix+col2 in df_excel.columns:\n",
    "            # Supprimer une colonne de chaque paire\n",
    "            df_excel.drop(columns=[prefix+col1], inplace=True)\n",
    "            # Renommer la colonne restante par la jointure des noms des deux colonnes\n",
    "            new_col_name = prefix+col1+\"_\"+col2\n",
    "            df_excel.rename(columns={prefix+col2: new_col_name}, inplace=True)\n",
    "    print(\"END\")\n",
    "    # Afficher le DataFrame rÃ©sultant\n",
    "    display(df_excel)\n",
    "    print(correlated_pairs)\n",
    "    print(len(correlated_pairs))\n",
    "    \n",
    "    dfs[data_strat] = df_excel.copy()\n",
    "\n",
    "print(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478a6f6e",
   "metadata": {},
   "source": [
    "# Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be53589a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only same patiens\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "df1 = dfs[\"irm1\"]\n",
    "df2 = dfs[\"irm2\"]\n",
    "\n",
    "df1_cop = df1.copy()\n",
    "df2_cop = df2.copy()\n",
    "display(df1)\n",
    "display(df2)\n",
    "\n",
    "\n",
    "\n",
    "#########################################################################################################\n",
    "############################### BOTH ###########################################################\n",
    "###########################################################################################################\n",
    "\n",
    "######## PREMIER####################################\n",
    "\n",
    "distance1 = len(df1[df1[\"diagnostic\"] == 0]) - len(df1[df1[\"diagnostic\"] == 1])\n",
    "\n",
    "ros = RandomOverSampler(random_state=0, sampling_strategy={1: int(len(df1[df1[\"diagnostic\"] == 1]) + (distance1/2))})\n",
    "rus = RandomUnderSampler(random_state=0, sampling_strategy={0: int(len(df1[df1[\"diagnostic\"] == 0]) - (distance1/2))})\n",
    "\n",
    "# Apply oversampling to the minority class\n",
    "X_resampled, y_resampled = ros.fit_resample(df1.drop(columns=[\"diagnostic\"]), df1[\"diagnostic\"])\n",
    "\n",
    "df_resampled = pd.DataFrame(X_resampled, columns=df1.columns)\n",
    "df_resampled['diagnostic'] = y_resampled\n",
    "\n",
    "X_final, y_final = rus.fit_resample(df_resampled.drop('diagnostic', axis=1), df_resampled['diagnostic'])\n",
    "\n",
    "df1 = pd.DataFrame(X_final, columns=df1.columns)\n",
    "df1['diagnostic'] = y_final\n",
    "df1.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print(df1['diagnostic'].value_counts())\n",
    "\n",
    "\n",
    "######## DEUXIEME ####################################\n",
    "\n",
    "distance2 = len(df2[df2[\"diagnostic\"] == 0]) - len(df2[df2[\"diagnostic\"] == 1])\n",
    "\n",
    "ros = RandomOverSampler(random_state=0, sampling_strategy={1: int(len(df2[df2[\"diagnostic\"] == 1]) + (distance2/2))})\n",
    "rus = RandomUnderSampler(random_state=0, sampling_strategy={0: int(len(df2[df2[\"diagnostic\"] == 0]) - (distance2/2))})\n",
    "\n",
    "\n",
    "# Apply oversampling to the minority class\n",
    "X_resampled, y_resampled = ros.fit_resample(df2.drop(columns=[\"diagnostic\"]), df2[\"diagnostic\"])\n",
    "\n",
    "df_resampled = pd.DataFrame(X_resampled, columns=df2.columns)\n",
    "df_resampled['diagnostic'] = y_resampled\n",
    "\n",
    "X_final, y_final = rus.fit_resample(df_resampled.drop('diagnostic', axis=1), df_resampled['diagnostic'])\n",
    "\n",
    "df2 = pd.DataFrame(X_final, columns=df2.columns)\n",
    "df2['diagnostic'] = y_final\n",
    "df2.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print(df2['diagnostic'].value_counts())\n",
    "\n",
    "#########################################################################################################\n",
    "############################### Undersampling ###########################################################\n",
    "###########################################################################################################\n",
    "\n",
    "rus = RandomUnderSampler(random_state=0)\n",
    "\n",
    "X_copy, y_copy = rus.fit_resample(df1_cop.drop(columns=[\"diagnostic\"]), df1_cop[\"diagnostic\"])\n",
    "df1_under = pd.DataFrame(X_copy.copy(), columns=df1_cop.columns)\n",
    "df1_under['diagnostic'] = y_copy.copy()\n",
    "df1_under.reset_index(inplace=True, drop=True)\n",
    "\n",
    "\n",
    "X_copy, y_copy = rus.fit_resample(df2_cop.drop(columns=[\"diagnostic\"]), df2_cop[\"diagnostic\"])\n",
    "df2_under = pd.DataFrame(X_copy.copy(), columns=df2_cop.columns)\n",
    "df2_under['diagnostic'] = y_copy.copy()\n",
    "df2_under.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print(\"END\")\n",
    "display(df1)\n",
    "display(df2)\n",
    "display(df1_under)\n",
    "display(df2_under)\n",
    "print(df1_under[\"diagnostic\"].value_counts())\n",
    "print(df2_under[\"diagnostic\"].value_counts())\n",
    "\n",
    "dfs = {}\n",
    "dfs[\"irm1_both\"] = df1.copy()\n",
    "dfs[\"irm2_both\"] = df2.copy()\n",
    "dfs[\"irm1_under\"] = df1_under.copy()\n",
    "dfs[\"irm2_under\"] = df2_under.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292249f7",
   "metadata": {},
   "source": [
    "# PrÃ©dictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb2a539",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_selec = \"without_fs\"\n",
    "target_nbr = 350\n",
    "new_dbs = {}\n",
    "\n",
    "if not recup:\n",
    "    for title_irm in dfs:\n",
    "        print(title_irm)\n",
    "        df = dfs[title_irm].copy()\n",
    "        display(df)\n",
    "        df.reset_index(inplace=True, drop=True)\n",
    "        y = df[[\"diagnostic\"]].copy()\n",
    "        X = df.drop(columns=[\"diagnostic\"])\n",
    "        grouping = X[\"RID\"].copy()\n",
    "        X.drop(columns=[\"RID\"], inplace=True)\n",
    "\n",
    "\n",
    "        print(y[\"diagnostic\"].value_counts())\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "        # 2 StratÃ©gies DB: APOE en one hot encoded et seulement le nombre d'allele 4\n",
    "\n",
    "        # Strategies 1: Nbr allele 4\n",
    "        print(\"strategies 1\")\n",
    "        X_copy = X.copy()\n",
    "        X_copy[\"APOE_nbr_allelle_4\"] = X_copy.apply(lambda x: str(x['APOE']).count(\"4\"), axis=1)\n",
    "        X_copy.drop(columns=[\"APOE\"], inplace=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Normalise\n",
    "        col_2_scale = [x for x in X_copy.columns if \"neuro_comp\" in x]\n",
    "        not_scaled = []\n",
    "        for col in X_copy.columns:\n",
    "            if len(X_copy[col].unique()) != 2 and col != \"APOE_nbr_allelle_4\":\n",
    "                col_2_scale.append(col)\n",
    "            elif (1 not in X_copy[col].unique() or 0 not in X_copy[col].unique()) and col != \"APOE_nbr_allelle_4\":\n",
    "                col_2_scale.append(col)\n",
    "            else:\n",
    "                not_scaled.append(col)\n",
    "        print(\"not scale for strat 2: \"+str(not_scaled))\n",
    "        X_copy[col_2_scale] = scaler.fit_transform(X_copy[col_2_scale], y[\"diagnostic\"])\n",
    "\n",
    "        #feature selection\n",
    "        fs = SelectKBest(score_func=f_classif, k=target_nbr)\n",
    "        fs.fit(X_copy, y[\"diagnostic\"])\n",
    "\n",
    "        if feat_selec == \"with_fs\":\n",
    "            X_new = fs.transform(X_copy)\n",
    "            X_copy = pd.DataFrame(X_new, columns=fs.get_feature_names_out())\n",
    "\n",
    "        print(title_irm+\"_APOE_nbr_allelle_4\")\n",
    "        display(X_copy)\n",
    "        new_dbs[title_irm+\"_APOE_nbr_allelle_4\"] = [X_copy.copy(), y.copy(), fs, grouping]\n",
    "\n",
    "        # Strategies 2: one hot encoded\n",
    "        print(\"strategies 2\")\n",
    "        X_copy = X.copy()\n",
    "        X_copy = pd.get_dummies(X_copy, columns=[\"APOE\"])\n",
    "        \n",
    "        \n",
    "        # normalise\n",
    "        col_2_scale = [x for x in X_copy.columns if \"neuro_comp\" in x]\n",
    "        not_scaled = []\n",
    "        for col in X_copy.columns:\n",
    "            if len(X_copy[col].unique()) != 2:\n",
    "                col_2_scale.append(col)\n",
    "            elif 1 not in X_copy[col].unique() or 0 not in X_copy[col].unique():\n",
    "                col_2_scale.append(col)\n",
    "            else:\n",
    "                not_scaled.append(col)\n",
    "        print(\"not scale for strat 2: \"+str(not_scaled))\n",
    "        X_copy[col_2_scale] = scaler.fit_transform(X_copy[col_2_scale], y[\"diagnostic\"])\n",
    "        \n",
    "        #Feature selection\n",
    "        fs = SelectKBest(score_func=f_classif, k=target_nbr)\n",
    "        fs.fit(X_copy, y[\"diagnostic\"])\n",
    "\n",
    "        if feat_selec == \"with_fs\":\n",
    "            X_new = fs.transform(X_copy)\n",
    "            X_copy = pd.DataFrame(X_new, columns=fs.get_feature_names_out())\n",
    "\n",
    "\n",
    "        print(title_irm+\"_APOE_one_hot\")\n",
    "        display(X_copy)\n",
    "        new_dbs[title_irm+\"_APOE_one_hot\"] = [X_copy.copy(), y.copy(), fs, grouping]\n",
    "    print(new_dbs.keys())\n",
    "\n",
    "\n",
    "if feat_selec == \"with_fs\":\n",
    "    name=[]\n",
    "    column_dfs = []\n",
    "    for i in new_dbs:\n",
    "        name.append(i)\n",
    "        column_dfs.append(set(new_dbs[i][0].columns))\n",
    "        \n",
    "    # CrÃ©ation d'un dictionnaire pour stocker les ensembles en utilisant leurs Ã©lÃ©ments concatÃ©nÃ©s comme clÃ©s\n",
    "    sets_dict = {}\n",
    "\n",
    "    # Parcours des listes\n",
    "    for nom, ensemble in zip(name, column_dfs):\n",
    "        # Convertir l'ensemble en tuple triÃ© pour l'utiliser comme clÃ© dans le dictionnaire\n",
    "        ensemble_triÃ© = tuple(sorted(ensemble))\n",
    "        # VÃ©rifier si l'ensemble existe dÃ©jÃ  dans le dictionnaire\n",
    "        if ensemble_triÃ© in sets_dict:\n",
    "            # Fusionner les noms\n",
    "            sets_dict[ensemble_triÃ©] += \"-\"+nom\n",
    "        else:\n",
    "            # Ajouter le nom et l'ensemble dans le dictionnaire\n",
    "            sets_dict[ensemble_triÃ©] = nom\n",
    "\n",
    "    # Extraire les noms uniques et les ensembles correspondants\n",
    "    nouveaux_noms = []\n",
    "    ensembles_uniques = []\n",
    "    for ensemble, nom in sets_dict.items():\n",
    "        nouveaux_noms.append(nom)\n",
    "        ensembles_uniques.append(set(ensemble))\n",
    "    print(nouveaux_noms)  # Sortie : [\"ACD\", \"B\"]\n",
    "    new_new_dbs = {}\n",
    "    for name in nouveaux_noms:\n",
    "        new_new_dbs[\"_\".join(name.split(\"-\"))] = new_dbs[name.split(\"-\")[0]].copy()\n",
    "    # ba exvivo garde le amy 3 pos, \n",
    "    print(new_new_dbs.keys())\n",
    "    new_dbs = new_new_dbs.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44231c1",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36231ab",
   "metadata": {},
   "source": [
    "### GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac71dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics \n",
    "rf_run = True\n",
    "\n",
    "scores_options = {\"auc_roc\": roc_auc_score, \"f1score\": f1_score}\n",
    "scoring = \"auc_roc\"\n",
    "binary = False\n",
    "\n",
    "custom_scorer = make_scorer(multiclass_score, greater_is_better=True, targets=[1], score=scores_options[scoring], \n",
    "                            binary=binary)\n",
    "\n",
    "today = datetime.now().strftime(\"%d-%m-%Y_%Hh%Mm%Ss\")\n",
    "path=\"./backup_models/preclinical/RF/\"+feat_selec+\"/\"+scoring+\"/\"+today+\"/\"\n",
    "if feat_selec == \"with_fs\":\n",
    "    path=\"./backup_models/preclinical/RF/\"+feat_selec+\"/\"+str(target_nbr)+\"/\"+scoring+\"/\"+today+\"/\"\n",
    "    \n",
    "if rf_run:\n",
    "    #varier nbr of trees ? et plus de depth? \n",
    "    strat_ml = {\n",
    "        \"depth\":{\n",
    "            \"10\":{\n",
    "                'n_estimators': [10, 50, 100], 'criterion': ['gini', 'entropy'],'max_depth': [10], 'max_features': ['sqrt', 'log2', None, 0.5,0.8]\n",
    "            },\n",
    "            \"100\":{\n",
    "                'n_estimators': [10, 50, 100], 'criterion': ['gini', 'entropy'],'max_depth': [100], 'max_features': ['sqrt', 'log2', None, 0.5,0.8]\n",
    "            },\n",
    "            \"1000\":{\n",
    "                'n_estimators': [10, 50, 100], 'criterion': ['gini', 'entropy'],'max_depth': [1000], 'max_features': ['sqrt', 'log2', None, 0.5,0.8]\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    os.makedirs(path+\"gridsearch_models/\", exist_ok=True)\n",
    "    os.makedirs(path+\"test_train_set/\", exist_ok=True)\n",
    "    os.makedirs(path+\"dataset/\", exist_ok=True)\n",
    "    os.makedirs(path+\"roc_curve/\", exist_ok=True)\n",
    "    os.makedirs(path+\"cm/\", exist_ok=True)\n",
    "    os.makedirs(path+\"report/\", exist_ok=True)\n",
    "    \n",
    "    conf_mat = []\n",
    "    top5_combinaison = []\n",
    "    exclude = {}\n",
    "    \n",
    "    for stratml in strat_ml:\n",
    "        strat_df_best_score = pd.DataFrame(columns=strat_ml[stratml].keys(), index=new_dbs.keys())\n",
    "        strat_df_score_test = pd.DataFrame(columns=strat_ml[stratml].keys(), index=new_dbs.keys())\n",
    "        strat_df_score_cross_val = pd.DataFrame(columns=strat_ml[stratml].keys(), index=new_dbs.keys())\n",
    "        strat_df_score_cross_val_std = pd.DataFrame(columns=strat_ml[stratml].keys(), index=new_dbs.keys())\n",
    "        for k in strat_ml[stratml]:\n",
    "            for options in new_dbs:\n",
    "                if len(top5_combinaison) == 0 or stratml+\"_\"+options in top5_combinaison:\n",
    "                    name_file = stratml+\"_\"+k+\"_\"+options+\"_score_\"+scoring\n",
    "                    print(name_file)\n",
    "\n",
    "                    X = new_dbs[options][0]\n",
    "                    y = new_dbs[options][1]\n",
    "                    group_list = new_dbs[options][-1]\n",
    "                    groupkfold = StratifiedGroupKFold(n_splits=5)\n",
    "                    if options+\".csv\" not in os.listdir(path+\"dataset\"):\n",
    "                        pd.concat([group_list, X, y], axis=1).to_csv(path + \"dataset/\" + options + \".csv\")\n",
    "                    \n",
    "                    \n",
    "                    best_params, best_score, score_on_test, model = grid_searches(X, y, RandomForestClassifier, \n",
    "                                                                               custom_scorer, strat_ml[stratml][k], \n",
    "                                                                               path, name_file, v= 3 , cv_grid=groupkfold, \n",
    "                                                                               cv_out_grid=groupkfold, group=group_list)\n",
    "                    \n",
    "                    \n",
    "                    ####### cross val + CM  + roc curve + scores  + STD\n",
    "                    \n",
    "                    \n",
    "                    y = y[\"diagnostic\"]\n",
    "\n",
    "                    splitting = groupkfold.split(X, y, group_list)\n",
    "\n",
    "                    conf_matrix_list_of_arrays = []\n",
    "                    scores = []\n",
    "\n",
    "                    tprs = []\n",
    "                    aucs = []\n",
    "                    mean_fpr = np.linspace(0, 1, 100)\n",
    "                    fig, ax = plt.subplots()\n",
    "                    df_scores = []\n",
    "                    for indice, (train_index, test_index) in enumerate(splitting):\n",
    "\n",
    "                        X_train, X_test, y_train, y_test = X.iloc[train_index], X.iloc[test_index], y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "                        model.fit(X_train, y_train)\n",
    "                        # df_probs = pd.DataFrame(clf.predict_proba(X_test), columns=[i for i in clf.classes_])\n",
    "                        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "                        scores.append(roc_auc_score(y_test, y_pred_proba, multi_class=\"ovr\", average=\"weighted\"))\n",
    "\n",
    "                        # Calculate ROC curve\n",
    "                        fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "                        roc_auc = auc(fpr, tpr)\n",
    "                        y_pred = (y_pred_proba > thresholds[np.argmax(tpr - fpr)]).astype('float')\n",
    "                        conf_matrix = confusion_matrix(y_test, y_pred, labels=[0,1])\n",
    "\n",
    "                        conf_matrix_list_of_arrays.append(conf_matrix)\n",
    "\n",
    "                        # Tracer les courbes ROC pour chaque pli\n",
    "                        plt.plot(fpr, tpr, lw=2, alpha=0.3,\n",
    "                                 label='ROC fold {0} (area = {1:0.2f})'\n",
    "                                       ''.format(indice, roc_auc))\n",
    "\n",
    "\n",
    "                        # Interpoler les tpr Ã  la moyenne des fpr\n",
    "                        tprs.append(np.interp(mean_fpr, fpr, tpr))\n",
    "                        tprs[-1][0] = 0.0\n",
    "                        \n",
    "                        \n",
    "                        report = classification_report(y_test, y_pred, labels=[0,1], output_dict=True)\n",
    "                        df_for_scoring = pd.DataFrame(report).transpose()\n",
    "\n",
    "                        new_line = [0]*(len(df_for_scoring.index)-2)\n",
    "                        score_auc = roc_auc_score(y_test, y_pred_proba, average=\"macro\", multi_class='ovr', labels=[0,1])\n",
    "                        new_line.append(score_auc)\n",
    "                        score_auc = roc_auc_score(y_test, y_pred_proba, average=\"weighted\", multi_class='ovr', labels=[0,1])\n",
    "                        new_line.append(score_auc)\n",
    "                        df_for_scoring[\"auc\"] = new_line\n",
    "\n",
    "                        df_scores.append(df_for_scoring)\n",
    "\n",
    "                    # Tracer la moyenne des courbes ROC\n",
    "                    std_tpr = np.std(tprs, axis=0)\n",
    "                    mean_tpr = np.mean(tprs, axis=0)\n",
    "                    mean_tpr[-1] = 1.0\n",
    "                    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "                    std_auc = np.std([auc(fpr, tpr) for fpr, tpr in zip([mean_fpr]*len(tprs), tprs)])\n",
    "                    plt.plot(mean_fpr, mean_tpr, linestyle='-', linewidth=2,\n",
    "                                 label='Mean ROC {0} (area = {1:0.2f} Â± {2:0.2f})'.format(type(model).__name__, mean_auc, std_auc))\n",
    "\n",
    "\n",
    "                    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "                    plt.xlim([0.0, 1.0])\n",
    "                    plt.ylim([0.0, 1.05])\n",
    "                    plt.xlabel('False Positive Rate')\n",
    "                    plt.ylabel('True Positive Rate')\n",
    "                    plt.title('Receiver Operating Characteristic')\n",
    "                    plt.legend(loc=\"lower right\")\n",
    "                    if name_file is not None:\n",
    "                        plt.savefig(path + \"roc_curve/\" + name_file + \"_roc_curve.png\")\n",
    "                    plt.show()\n",
    "                    plt.clf()\n",
    "\n",
    "                    sum_conf_mat = np.sum(conf_matrix_list_of_arrays, axis=0)\n",
    "                    \n",
    "                    plt.title(\"confusion matrix from RF\")\n",
    "                    # Calcul de la somme des Ã©lÃ©ments de chaque ligne\n",
    "                    row_sums = np.sum(sum_conf_mat, axis=1)\n",
    "\n",
    "                    # Mise Ã  l'Ã©chelle en divisant chaque Ã©lÃ©ment par la somme de sa ligne\n",
    "                    scaled_arr = sum_conf_mat / row_sums[:, np.newaxis]\n",
    "                    # cm = ConfusionMatrixDisplay(sum_conf_mat / np.sum(sum_conf_mat), display_labels=classes).plot()\n",
    "                    ConfusionMatrixDisplay(scaled_arr, display_labels=[0,1]).plot()\n",
    "\n",
    "                    plt.savefig(path + \"cm/\" + name_file + \"_cm.png\")\n",
    "                    plt.show()\n",
    "                    plt.clf()\n",
    "                    \n",
    "                    # Calculer la moyenne de chaque Ã©lÃ©ment\n",
    "                    mean_df = pd.concat(df_scores).groupby(level=0).mean().round(3)\n",
    "\n",
    "                    # Calculer l'Ã©cart type de chaque Ã©lÃ©ment\n",
    "                    std_df = pd.concat(df_scores).groupby(level=0).std().round(3)\n",
    "                    mean_df.to_csv(path + \"report/\" + name_file + \"_mean.csv\")\n",
    "                    std_df.to_csv(path + \"report/\" + name_file + \"_std.csv\")\n",
    "                    display(mean_df)\n",
    "                    print(\"coming from\")\n",
    "                    for lesdf in df_scores:\n",
    "                        display(lesdf)\n",
    "                    ################ fin cross val\n",
    "                    \n",
    "                    \n",
    "                    print(best_params)\n",
    "                    strat_df_best_score.at[options, k] = best_score\n",
    "                    strat_df_score_test.at[options, k] = score_on_test\n",
    "                    strat_df_score_cross_val.at[options, k] = sum(scores)/len(scores)\n",
    "                    strat_df_score_cross_val_std.at[options, k] = statistics.pstdev(scores)\n",
    "\n",
    "            \n",
    "        #for i in conf_mat:\n",
    "            #print(i[0])\n",
    "            #ConfusionMatrixDisplay(i[1], display_labels=i[2]).plot()\n",
    "\n",
    "        strat_df_best_score.dropna(inplace=True)\n",
    "        strat_df_score_test.dropna(inplace=True)\n",
    "        strat_df_score_cross_val.dropna(inplace=True)\n",
    "        strat_df_score_cross_val_std.dropna(inplace=True)\n",
    "\n",
    "        for i in strat_df_best_score.columns:\n",
    "            strat_df_best_score[i] = strat_df_best_score[i].astype(float)\n",
    "            strat_df_score_test[i] = strat_df_score_test[i].astype(float)\n",
    "            strat_df_score_cross_val[i] = strat_df_score_cross_val[i].astype(float)\n",
    "            strat_df_score_cross_val_std[i] = strat_df_score_cross_val_std[i].astype(float)\n",
    "\n",
    "        \n",
    "        display(strat_df_best_score)\n",
    "        display(strat_df_score_test)\n",
    "        display(strat_df_score_cross_val)\n",
    "        display(strat_df_score_cross_val_std)\n",
    "    \n",
    "        strat_df_best_score.to_csv(path+stratml+\"_best_scores_in_grid.csv\")\n",
    "        strat_df_score_test.to_csv(path+stratml+\"_score_on_test.csv\")\n",
    "        strat_df_score_cross_val.to_csv(path+stratml+\"_score_on_cross_val.csv\")\n",
    "        strat_df_score_cross_val_std.to_csv(path+stratml+\"_score_on_cross_val_std.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38ea6f0",
   "metadata": {},
   "source": [
    "### Plot scores & Learning Curve\n",
    "\n",
    "ATTENTION: si on ajoute des strategies ML, il faudra stocker dans une liste et itÃ©rer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a5c04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    path=\"./backup_models/preclinical/RF/\"+feat_selec+\"/\"+scoring\n",
    "    if feat_selec == \"with_fs\":\n",
    "        path=\"./backup_models/preclinical/RF/\"+feat_selec+\"/\"+str(target_nbr)+\"/\"+scoring\n",
    "\n",
    "    plot_scores(path, plots=[\"score_on_cross_val\"], v=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83793a45",
   "metadata": {},
   "source": [
    "### Learning Curve & XAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04064c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    path=\"./backup_models/preclinical/RF/\"+feat_selec+\"/\"+scoring\n",
    "    if feat_selec == \"with_fs\":\n",
    "        path=\"./backup_models/preclinical/RF/\"+feat_selec+\"/\"+str(target_nbr)+\"/\"+scoring\n",
    "        \n",
    "    files = os.listdir(path)\n",
    "    dir_grid = max([datetime.strptime(f, \"%d-%m-%Y_%Hh%Mm%Ss\") for f in files if os.path.isdir(path+'/'+f)]).strftime(\"%d-%m-%Y_%Hh%Mm%Ss\")\n",
    "    for scores in os.listdir(path+\"/\"+dir_grid):\n",
    "        if \"score_on_cross_val\" in scores:\n",
    "            strat_df_score_cross_val = pd.read_csv(path+\"/\"+dir_grid+\"/\"+scores, index_col=0)\n",
    "            break\n",
    "    \n",
    "    max_value = strat_df_score_cross_val.values.max()\n",
    "\n",
    "    max_positions = list(zip(*np.where(strat_df_score_cross_val == max_value)))\n",
    "\n",
    "    # Create the associations\n",
    "    targets = [f\"{strat_df_score_cross_val.columns[col]}_{strat_df_score_cross_val.index[row]}\" for row, col in max_positions]\n",
    "    #targets = ['10_irm2_APOE_nbr_allelle_4_irm2_APOE_one_hot', '100_irm2_APOE_nbr_allelle_4_irm2_APOE_one_hot', '1000_irm2_APOE_nbr_allelle_4_irm2_APOE_one_hot']\n",
    "    print(\"targets: \"+str(targets))\n",
    "    print(new_dbs.keys())\n",
    "    print(path)\n",
    "    groupkfold = StratifiedGroupKFold(n_splits=5)\n",
    "    #plot_learning_curves(new_dbs, path, targets, 5, 3, v=True, scoring=f1_score, binary=False)\n",
    "    #XAI_analyses(new_dbs, path, targets, custom_scorer, v=True, nsplits_exp=5, shap=False)\n",
    "    plot_confusion_matrices(new_dbs, path, targets, groupkfold,scores_options[scoring], binary, [0,1], [1], summat=False)\n",
    "    plot_roc_curve(new_dbs, path, targets, groupkfold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3072c7",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18709b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_run = True\n",
    "\n",
    "scores_options = {\"auc_roc\": roc_auc_score, \"f1score\": f1_score}\n",
    "scoring = \"auc_roc\"\n",
    "binary = False\n",
    "proba_svc = True\n",
    "if scoring == \"auc_roc\":\n",
    "    proba_svc = True\n",
    "\n",
    "custom_scorer = make_scorer(multiclass_score, greater_is_better=True, targets=[1], score=scores_options[scoring], \n",
    "                            binary=binary)\n",
    "\n",
    "today = datetime.now().strftime(\"%d-%m-%Y_%Hh%Mm%Ss\")\n",
    "path=\"./backup_models/preclinical/SVM/\"+feat_selec+\"/\"+scoring+\"/\"+today+\"/\"\n",
    "if feat_selec == \"with_fs\":\n",
    "    path=\"./backup_models/preclinical/SVM/\"+feat_selec+\"/\"+str(target_nbr)+\"/\"+scoring+\"/\"+today+\"/\"\n",
    "    \n",
    "if svm_run:\n",
    "    # ATTENTION: Ã  changer quand stratÃ©gie change\"\n",
    "    \n",
    "    os.makedirs(path+\"gridsearch_models/\", exist_ok=True)\n",
    "    os.makedirs(path+\"test_train_set/\", exist_ok=True)\n",
    "    os.makedirs(path+\"dataset/\", exist_ok=True)\n",
    "    os.makedirs(path+\"roc_curve/\", exist_ok=True)\n",
    "    os.makedirs(path+\"cm/\", exist_ok=True)\n",
    "    os.makedirs(path+\"report/\", exist_ok=True)\n",
    "    \n",
    "    kernels = {\"linear\": {'C': [1, 10, 100], 'kernel': ['linear']}, \n",
    "               \"rbf\": {'C': [1, 10, 100], 'gamma': ['scale', 0.001, 0.0001], 'kernel': ['rbf']}, \n",
    "               \"poly\": {'C': [1, 10, 100], 'gamma': ['scale', 0.001, 0.0001], 'kernel': ['poly'], 'coef0': [0.0, 0.1, 0.5, 1.0],'degree': [3, 5, 7, 9]}, \n",
    "               \"sigmoid\": {'C': [1, 10, 100], 'gamma': ['scale', 0.001, 0.0001], 'kernel': ['sigmoid'],'coef0': [0.0, 0.1, 0.5, 1.0]}}\n",
    "\n",
    "\n",
    "    strat_df_best_score = pd.DataFrame(columns=kernels.keys(), index=new_dbs.keys())\n",
    "    strat_df_score_test = pd.DataFrame(columns=kernels.keys(), index=new_dbs.keys())\n",
    "    strat_df_score_cross_val = pd.DataFrame(columns=kernels.keys(), index=new_dbs.keys())\n",
    "    strat_df_score_cross_val_std = pd.DataFrame(columns=kernels.keys(), index=new_dbs.keys())\n",
    "    top5_combinaison = []\n",
    "    \n",
    "    for k in kernels:\n",
    "        for options in new_dbs:\n",
    "            print(options)\n",
    "            print(\"kernel: \"+k)\n",
    "            if len(top5_combinaison) == 0 or options in top5_combinaison:\n",
    "                name_file = \"kernel_\"+k+\"_\"+options+\"_score_\"+scoring\n",
    "\n",
    "                X = new_dbs[options][0].copy()\n",
    "                y = new_dbs[options][1].copy()\n",
    "                group_list = new_dbs[options][-1]\n",
    "                groupkfold = StratifiedGroupKFold(n_splits=5)\n",
    "                if options+\".csv\" not in os.listdir(path+\"dataset\"):\n",
    "                    pd.concat([group_list, X, y], axis=1).to_csv(path + \"dataset/\" + options + \".csv\")\n",
    "                \n",
    "                best_params, best_score, score_on_test, model = grid_searches(X, y, SVC, \n",
    "                                                                           custom_scorer, kernels[k], \n",
    "                                                                           path, name_file, cv_grid=groupkfold,\n",
    "                                                                           cv_out_grid=groupkfold, group=group_list, basic_par={\"probability\": True})\n",
    "                print(best_params)\n",
    "                best_params[\"probability\"] = proba_svc\n",
    "                \n",
    "                    \n",
    "                ####### cross val + CM  + roc curve + scores  + STD\n",
    "\n",
    "\n",
    "                y = y[\"diagnostic\"]\n",
    "\n",
    "                splitting = groupkfold.split(X, y, group_list)\n",
    "\n",
    "                conf_matrix_list_of_arrays = []\n",
    "                scores = []\n",
    "\n",
    "                tprs = []\n",
    "                aucs = []\n",
    "                mean_fpr = np.linspace(0, 1, 100)\n",
    "                fig, ax = plt.subplots()\n",
    "                df_scores = []\n",
    "                for indice, (train_index, test_index) in enumerate(splitting):\n",
    "\n",
    "                    X_train, X_test, y_train, y_test = X.iloc[train_index], X.iloc[test_index], y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "                    model.fit(X_train, y_train)\n",
    "                    # df_probs = pd.DataFrame(clf.predict_proba(X_test), columns=[i for i in clf.classes_])\n",
    "\n",
    "                    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "                    scores.append(roc_auc_score(y_test, y_pred_proba, multi_class=\"ovr\", average=\"weighted\"))\n",
    "\n",
    "                    # Calculate ROC curve\n",
    "                    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "                    roc_auc = auc(fpr, tpr)\n",
    "                    y_pred = (y_pred_proba > thresholds[np.argmax(tpr - fpr)]).astype('float')\n",
    "                    conf_matrix = confusion_matrix(y_test, y_pred, labels=[0,1])\n",
    "\n",
    "                    conf_matrix_list_of_arrays.append(conf_matrix)\n",
    "\n",
    "                    # Tracer les courbes ROC pour chaque pli\n",
    "                    plt.plot(fpr, tpr, lw=2, alpha=0.3,\n",
    "                             label='ROC fold {0} (area = {1:0.2f})'\n",
    "                                   ''.format(indice, roc_auc))\n",
    "\n",
    "\n",
    "                    # Interpoler les tpr Ã  la moyenne des fpr\n",
    "                    tprs.append(np.interp(mean_fpr, fpr, tpr))\n",
    "                    tprs[-1][0] = 0.0\n",
    "                    \n",
    "                    \n",
    "                    report = classification_report(y_test, y_pred, labels=[0,1], output_dict=True)\n",
    "                    df_for_scoring = pd.DataFrame(report).transpose()\n",
    "\n",
    "                    new_line = [0]*(len(df_for_scoring.index)-2)\n",
    "                    score_auc = roc_auc_score(y_test, y_pred_proba, average=\"macro\", multi_class='ovr', labels=[0,1])\n",
    "                    new_line.append(score_auc)\n",
    "                    score_auc = roc_auc_score(y_test, y_pred_proba, average=\"weighted\", multi_class='ovr', labels=[0,1])\n",
    "                    new_line.append(score_auc)\n",
    "                    df_for_scoring[\"auc\"] = new_line\n",
    "\n",
    "                    df_scores.append(df_for_scoring)\n",
    "\n",
    "                # Tracer la moyenne des courbes ROC\n",
    "                std_tpr = np.std(tprs, axis=0)\n",
    "                mean_tpr = np.mean(tprs, axis=0)\n",
    "                mean_tpr[-1] = 1.0\n",
    "                mean_auc = auc(mean_fpr, mean_tpr)\n",
    "                std_auc = np.std([auc(fpr, tpr) for fpr, tpr in zip([mean_fpr]*len(tprs), tprs)])\n",
    "                plt.plot(mean_fpr, mean_tpr, linestyle='-', linewidth=2,\n",
    "                             label='Mean ROC {0} (area = {1:0.2f} Â± {2:0.2f})'.format(type(model).__name__, mean_auc, std_auc))\n",
    "\n",
    "\n",
    "                plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "                plt.xlim([0.0, 1.0])\n",
    "                plt.ylim([0.0, 1.05])\n",
    "                plt.xlabel('False Positive Rate')\n",
    "                plt.ylabel('True Positive Rate')\n",
    "                plt.title('Receiver Operating Characteristic')\n",
    "                plt.legend(loc=\"lower right\")\n",
    "                if name_file is not None:\n",
    "                    plt.savefig(path + \"roc_curve/\" + name_file + \"_roc_curve.png\")\n",
    "                plt.show()\n",
    "                plt.clf()\n",
    "\n",
    "                sum_conf_mat = np.sum(conf_matrix_list_of_arrays, axis=0)\n",
    "\n",
    "                plt.title(\"confusion matrix from RF\")\n",
    "                # Calcul de la somme des Ã©lÃ©ments de chaque ligne\n",
    "                row_sums = np.sum(sum_conf_mat, axis=1)\n",
    "\n",
    "                # Mise Ã  l'Ã©chelle en divisant chaque Ã©lÃ©ment par la somme de sa ligne\n",
    "                scaled_arr = sum_conf_mat / row_sums[:, np.newaxis]\n",
    "                # cm = ConfusionMatrixDisplay(sum_conf_mat / np.sum(sum_conf_mat), display_labels=classes).plot()\n",
    "                ConfusionMatrixDisplay(scaled_arr, display_labels=[0,1]).plot()\n",
    "\n",
    "                plt.savefig(path + \"cm/\" + name_file + \"_cm.png\")\n",
    "                plt.show()\n",
    "                plt.clf()\n",
    "                \n",
    "                \n",
    "                # Calculer la moyenne de chaque Ã©lÃ©ment\n",
    "                mean_df = pd.concat(df_scores).groupby(level=0).mean().round(3)\n",
    "\n",
    "                # Calculer l'Ã©cart type de chaque Ã©lÃ©ment\n",
    "                std_df = pd.concat(df_scores).groupby(level=0).std().round(3)\n",
    "                mean_df.to_csv(path + \"report/\" + name_file + \"_mean.csv\")\n",
    "                std_df.to_csv(path + \"report/\" + name_file + \"_std.csv\")\n",
    "                ################ fin cross val\n",
    "                print(best_params)\n",
    "                strat_df_best_score.at[options, k] = best_score\n",
    "                strat_df_score_test.at[options, k] = score_on_test\n",
    "                strat_df_score_cross_val.at[options, k] = sum(scores)/len(scores)\n",
    "                strat_df_score_cross_val_std.at[options, k] = statistics.pstdev(scores)\n",
    "                \n",
    "                \n",
    "        \n",
    "    strat_df_best_score.dropna(inplace=True)\n",
    "    strat_df_score_test.dropna(inplace=True)\n",
    "    strat_df_score_cross_val.dropna(inplace=True)\n",
    "    strat_df_score_cross_val_std.dropna(inplace=True)\n",
    "\n",
    "    for i in strat_df_best_score.columns:\n",
    "        strat_df_best_score[i] = strat_df_best_score[i].astype(float)\n",
    "        strat_df_score_test[i] = strat_df_score_test[i].astype(float)\n",
    "        strat_df_score_cross_val[i] = strat_df_score_cross_val[i].astype(float)\n",
    "        strat_df_score_cross_val_std[i] = strat_df_score_cross_val[i].astype(float)\n",
    "\n",
    "\n",
    "    display(strat_df_best_score)\n",
    "    display(strat_df_score_test)\n",
    "    display(strat_df_score_cross_val)\n",
    "    display(strat_df_score_cross_val_std)\n",
    "\n",
    "    strat_df_best_score.to_csv(path+\"best_scores_in_grid.csv\")\n",
    "    strat_df_score_test.to_csv(path+\"score_on_test.csv\")\n",
    "    strat_df_score_cross_val.to_csv(path+\"score_on_cross_val.csv\")\n",
    "    strat_df_score_cross_val_std.to_csv(path+\"score_on_cross_val_std.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780a6b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4705fe4",
   "metadata": {},
   "source": [
    "### Pot Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da762ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    path=\"./backup_models/preclinical/SVM/\"+feat_selec+\"/\"+scoring\n",
    "    if feat_selec == \"with_fs\":\n",
    "        path=\"./backup_models/preclinical/SVM/\"+feat_selec+\"/\"+str(target_nbr)+\"/\"+scoring\n",
    "\n",
    "    plot_scores(path, plots=[\"score_on_cross_val\"], v=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d5f9b9",
   "metadata": {},
   "source": [
    "### Learning Curve & XAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ffb984",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    path=\"./backup_models/preclinical/SVM/\"+feat_selec+\"/\"+scoring\n",
    "    if feat_selec == \"with_fs\":\n",
    "        path=\"./backup_models/preclinical/SVM/\"+feat_selec+\"/\"+str(target_nbr)+\"/\"+scoring\n",
    "        \n",
    "    \n",
    "    dir_grid = max([datetime.strptime(f, \"%d-%m-%Y_%Hh%Mm%Ss\") for f in files if os.path.isdir(path+'/'+f)]).strftime(\"%d-%m-%Y_%Hh%Mm%Ss\")\n",
    "    for scores in os.listdir(path+\"/\"+dir_grid):\n",
    "        if \"score_on_cross_val\" in scores:\n",
    "            strat_df_score_cross_val = pd.read_csv(path+\"/\"+dir_grid+\"/\"+scores, index_col=0)\n",
    "            break\n",
    "    \n",
    "    max_value = strat_df_score_cross_val.max(axis=1).max()\n",
    "    targets = strat_df_score_cross_val[strat_df_score_cross_val.eq(max_value).any(axis=1)].index.tolist()\n",
    "    print(\"targets: \"+str(targets))\n",
    "    \n",
    "    groupkfold = StratifiedGroupKFold(n_splits=5)\n",
    "    #plot_learning_curves(new_dbs, path, targets, 5, 3, v=True, scoring=f1_score, binary=False)\n",
    "    XAI_analyses(new_dbs, path, targets, custom_scorer, v=True, nsplits_exp=5, shap=False)\n",
    "    plot_confusion_matrices(new_dbs, path, targets, groupkfold,scores_options[scoring], binary, [0,1], [1], summat=False)\n",
    "    plot_roc_curve(new_dbs, path, targets, groupkfold)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32892be2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
